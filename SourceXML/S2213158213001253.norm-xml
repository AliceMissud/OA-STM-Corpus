<?xml version="1.0" encoding="UTF-8"?><!-- Normalized for easier text mining. --><xocs:doc xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xs="http://www.w3.org/2001/XMLSchema" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.elsevier.com/xml/ja/dtd" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xsi:schemaLocation="http://www.elsevier.com/xml/xocs/dtd http://schema.elsevier.com/dtds/document/fulltext/xcr/xocs-article.xsd"><xocs:meta><xocs:content-family>serial</xocs:content-family><xocs:content-type>JL</xocs:content-type><xocs:cid>282794</xocs:cid><xocs:srctitle>NeuroImage: Clinical</xocs:srctitle><xocs:normalized-srctitle>NEUROIMAGECLINICAL</xocs:normalized-srctitle><xocs:orig-load-date>2013-10-11</xocs:orig-load-date><xocs:ew-transaction-id>2013-12-10T16:47:41</xocs:ew-transaction-id><xocs:eid>1-s2.0-S2213158213001253</xocs:eid><xocs:pii-formatted>S2213-1582(13)00125-3</xocs:pii-formatted><xocs:pii-unformatted>S2213158213001253</xocs:pii-unformatted><xocs:doi>10.1016/j.nicl.2013.09.008</xocs:doi><xocs:item-stage>S300</xocs:item-stage><xocs:item-version-number>S300.1</xocs:item-version-number><xocs:item-weight>FULL-TEXT</xocs:item-weight><xocs:hub-eid>1-s2.0-S2213158213X00026</xocs:hub-eid><xocs:timestamp>2014-02-03T15:49:17.288084-05:00</xocs:timestamp><xocs:issns><xocs:issn-primary-formatted>2213-1582</xocs:issn-primary-formatted><xocs:issn-primary-unformatted>22131582</xocs:issn-primary-unformatted></xocs:issns><xocs:sponsored-access-type>UNLIMITED</xocs:sponsored-access-type><xocs:funding-body-id>NONE</xocs:funding-body-id><xocs:crossmark>false</xocs:crossmark><xocs:vol-first>3</xocs:vol-first><xocs:suppl>C</xocs:suppl><xocs:vol-iss-suppl-text>Volume 3</xocs:vol-iss-suppl-text><xocs:sort-order>45</xocs:sort-order><xocs:first-fp>416</xocs:first-fp><xocs:last-lp>428</xocs:last-lp><xocs:pages><xocs:first-page>416</xocs:first-page><xocs:last-page>428</xocs:last-page></xocs:pages><xocs:cover-date-orig><xocs:start-date>2013</xocs:start-date></xocs:cover-date-orig><xocs:cover-date-text>2013</xocs:cover-date-text><xocs:cover-date-start>2013-01-01</xocs:cover-date-start><xocs:cover-date-end>2013-12-31</xocs:cover-date-end><xocs:cover-date-year>2013</xocs:cover-date-year><xocs:hub-sec><xocs:hub-sec-title>Regular Articles</xocs:hub-sec-title></xocs:hub-sec><xocs:document-type>article</xocs:document-type><xocs:document-subtype>fla</xocs:document-subtype><xocs:copyright-line>Copyright Â© 2013 The Authors. Published by Elsevier Inc. All rights reserved.</xocs:copyright-line><xocs:normalized-article-title>COMBINEDANALYSISSMRIFMRIIMAGINGDATAPROVIDESACCURATEDISEASEMARKERSFORHEARINGIMPAIRMENT</xocs:normalized-article-title><xocs:normalized-first-auth-surname>TAN</xocs:normalized-first-auth-surname><xocs:normalized-first-auth-initial>L</xocs:normalized-first-auth-initial><xocs:item-toc><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>1</xocs:item-toc-label><xocs:item-toc-section-title>Introduction</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2</xocs:item-toc-label><xocs:item-toc-section-title>Materials and methods</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.1</xocs:item-toc-label><xocs:item-toc-section-title>Data acquisition and preprocessing</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.1.1</xocs:item-toc-label><xocs:item-toc-section-title>Participants</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.1.2</xocs:item-toc-label><xocs:item-toc-section-title>MRI/fMRI acquisition</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.1.3</xocs:item-toc-label><xocs:item-toc-section-title>Data analysis - preprocessing</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.2</xocs:item-toc-label><xocs:item-toc-section-title>Feature extraction and model learning based on structural MR images</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.2.1</xocs:item-toc-label><xocs:item-toc-section-title>Obtaining 2D slices from 3D brain images</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.2.2</xocs:item-toc-label><xocs:item-toc-section-title>Extracting SIFT features</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.2.3</xocs:item-toc-label><xocs:item-toc-section-title>Feature evaluation</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.2.4</xocs:item-toc-label><xocs:item-toc-section-title>Training SVM classifiers</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.2.5</xocs:item-toc-label><xocs:item-toc-section-title>Predicting new subjects</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.3</xocs:item-toc-label><xocs:item-toc-section-title>Feature extraction and model learning based on functional MR images</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.3.1</xocs:item-toc-label><xocs:item-toc-section-title>Feature generation from contrast maps</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.3.2</xocs:item-toc-label><xocs:item-toc-section-title>Sedation method</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.3.3</xocs:item-toc-label><xocs:item-toc-section-title>Feature selection and model learning</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.3.4</xocs:item-toc-label><xocs:item-toc-section-title>Predicting new subjects</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.3.5</xocs:item-toc-label><xocs:item-toc-section-title>Important features</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.4</xocs:item-toc-label><xocs:item-toc-section-title>Integrated model</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.5</xocs:item-toc-label><xocs:item-toc-section-title>Validation of the classifier</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3</xocs:item-toc-label><xocs:item-toc-section-title>Results</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.1</xocs:item-toc-label><xocs:item-toc-section-title>Classifier performance</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.2</xocs:item-toc-label><xocs:item-toc-section-title>Feature selection in sMRI analysis</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.3</xocs:item-toc-label><xocs:item-toc-section-title>Stability of feature selection in fMRI analysis</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.4</xocs:item-toc-label><xocs:item-toc-section-title>Discriminative brain regions</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4</xocs:item-toc-label><xocs:item-toc-section-title>Discussion</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>5</xocs:item-toc-label><xocs:item-toc-section-title>Conclusion</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:acknowledgment"><xocs:item-toc-section-title>Acknowledgment</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:bibliography"><xocs:item-toc-section-title>References</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc><xocs:references><xocs:ref-info refid="rf0005"><xocs:ref-normalized-surname>ALTAYE</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>721</xocs:ref-first-fp><xocs:ref-last-lp>730</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0010"><xocs:ref-normalized-surname>BACHMANN</xocs:ref-normalized-surname><xocs:ref-pub-year>1998</xocs:ref-pub-year><xocs:ref-first-fp>155</xocs:ref-first-fp><xocs:ref-last-lp>165</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0015"><xocs:ref-normalized-surname>BILECEN</xocs:ref-normalized-surname><xocs:ref-pub-year>2000</xocs:ref-pub-year><xocs:ref-first-fp>765</xocs:ref-first-fp><xocs:ref-last-lp>767</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0205"><xocs:ref-normalized-surname>CHANG</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>21</xocs:ref-first-fp><xocs:ref-last-lp>27</xocs:ref-last-lp><xocs:ref-normalized-initial>C</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0210"><xocs:ref-normalized-surname>CHEN</xocs:ref-normalized-surname><xocs:ref-pub-year>2013</xocs:ref-pub-year><xocs:ref-first-fp>22</xocs:ref-first-fp><xocs:ref-last-lp>31</xocs:ref-last-lp><xocs:ref-normalized-initial>Y</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0020"><xocs:ref-normalized-surname>CUINGNET</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>766</xocs:ref-first-fp><xocs:ref-last-lp>781</xocs:ref-last-lp><xocs:ref-normalized-initial>R</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0025"><xocs:ref-normalized-surname>CUNNINGHAM</xocs:ref-normalized-surname><xocs:ref-pub-year>2003</xocs:ref-pub-year><xocs:ref-first-fp>436</xocs:ref-first-fp><xocs:ref-last-lp>440</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0030"><xocs:ref-normalized-surname>DECHTER</xocs:ref-normalized-surname><xocs:ref-pub-year>1985</xocs:ref-pub-year><xocs:ref-first-fp>505</xocs:ref-first-fp><xocs:ref-last-lp>536</xocs:ref-last-lp><xocs:ref-normalized-initial>R</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0215"><xocs:ref-normalized-surname>DIFRANCESCO</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>17THANNUALMEETINGORGANIZATIONFORHUMANBRAINMAPPINGOHBMQUEBECCANADA</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>DISTINCTBOLDACTIVATIONCONNECTIVITYPROFILESININFANTSSEDATEDNEMBUTALPROPOFOLUNDERLANGUAGESTIMULATION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="rf0220"><xocs:ref-normalized-surname>DIFRANCESCO</xocs:ref-normalized-surname><xocs:ref-pub-year>2013</xocs:ref-pub-year><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>ISMRM21STSCIENTIFICMEETINGSALTLAKECITYUT</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>BOLDFMRIININFANTSUNDERSEDATIONCOMPARINGIMPACTPENTOBARBITALPROPOFOLAUDITORYLANGUAGEACTIVATIONCONNECTIVITY</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="rf0225"><xocs:ref-normalized-surname>DIFRANCESCO</xocs:ref-normalized-surname><xocs:ref-pub-year>2013</xocs:ref-pub-year><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0230"><xocs:ref-normalized-surname>DYKSTRA</xocs:ref-normalized-surname><xocs:ref-pub-year>1994</xocs:ref-pub-year><xocs:ref-normalized-initial>C</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PHDTHESISCANADASIMONFRASERUNIVERSITY</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>3DCONTIGUOUSVOLUMEANALYSISFORFUNCTIONALIMAGING</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="rf0035"><xocs:ref-normalized-surname>FAN</xocs:ref-normalized-surname><xocs:ref-pub-year>2007</xocs:ref-pub-year><xocs:ref-first-fp>1189</xocs:ref-first-fp><xocs:ref-last-lp>1199</xocs:ref-last-lp><xocs:ref-normalized-initial>Y</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0040"><xocs:ref-normalized-surname>FAN</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>277</xocs:ref-first-fp><xocs:ref-last-lp>285</xocs:ref-last-lp><xocs:ref-normalized-initial>Y</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0235"><xocs:ref-normalized-surname>HALL</xocs:ref-normalized-surname><xocs:ref-pub-year>1999</xocs:ref-pub-year><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>CORRELATIONBASEDFEATURESUBSETSELECTIONFORMACHINELEARNING</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="rf0240"><xocs:ref-normalized-surname>HOROWITZKRAUS</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-normalized-initial>T</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>EXECUTIVEFUNCTIONINPRESCHOOLAGECHILDRENINTEGRATINGMEASUREMENTNEURODEVELOPMENTTRANSLATIONALRESEARCH</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>IMAGINGEXECUTIVEFUNCTIONSINTYPICALLYATYPICALLYDEVELOPEDCHILDREN</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="rf0050"><xocs:ref-normalized-surname>JOHNSON</xocs:ref-normalized-surname><xocs:ref-pub-year>1967</xocs:ref-pub-year><xocs:ref-first-fp>241</xocs:ref-first-fp><xocs:ref-last-lp>254</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0055"><xocs:ref-normalized-surname>JONAS</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-first-fp>927</xocs:ref-first-fp><xocs:ref-last-lp>929</xocs:ref-last-lp><xocs:ref-normalized-initial>N</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0060"><xocs:ref-normalized-surname>KALOUSIS</xocs:ref-normalized-surname><xocs:ref-pub-year>2007</xocs:ref-pub-year><xocs:ref-first-fp>95</xocs:ref-first-fp><xocs:ref-last-lp>116</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0065"><xocs:ref-normalized-surname>KARUNANAYAKA</xocs:ref-normalized-surname><xocs:ref-pub-year>2007</xocs:ref-pub-year><xocs:ref-first-fp>349</xocs:ref-first-fp><xocs:ref-last-lp>360</xocs:ref-last-lp><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0070"><xocs:ref-normalized-surname>KEMPER</xocs:ref-normalized-surname><xocs:ref-pub-year>2000</xocs:ref-pub-year><xocs:ref-first-fp>484</xocs:ref-first-fp><xocs:ref-last-lp>488</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0245"><xocs:ref-normalized-surname>KRETSCHMANN</xocs:ref-normalized-surname><xocs:ref-pub-year>1998</xocs:ref-pub-year><xocs:ref-normalized-initial>H</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>NEUROFUNCTIONALSYSTEMS3DRECONSTRUCTIONSCORRELATEDNEUROIMAGING</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="rf0075"><xocs:ref-normalized-surname>LAPOINTE</xocs:ref-normalized-surname><xocs:ref-pub-year>2006</xocs:ref-pub-year><xocs:ref-first-fp>863</xocs:ref-first-fp><xocs:ref-last-lp>868</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0080"><xocs:ref-normalized-surname>LEACH</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>31</xocs:ref-first-fp><xocs:ref-last-lp>49</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0250"><xocs:ref-normalized-surname>LOWE</xocs:ref-normalized-surname><xocs:ref-pub-year>1999</xocs:ref-pub-year><xocs:ref-first-fp>1150</xocs:ref-first-fp><xocs:ref-last-lp>1157</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSINTERNATIONALCONFERENCECOMPUTERVISION</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>OBJECTRECOGNITIONLOCALINVARIANTFEATURES</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="or0045"/><xocs:ref-info refid="rf0090"><xocs:ref-normalized-surname>NIPARKO</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>1498</xocs:ref-first-fp><xocs:ref-last-lp>1506</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0255"><xocs:ref-normalized-surname>NORTHERN</xocs:ref-normalized-surname><xocs:ref-pub-year>1994</xocs:ref-pub-year><xocs:ref-first-fp>955</xocs:ref-first-fp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0095"><xocs:ref-normalized-surname>ORRU</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-first-fp>1140</xocs:ref-first-fp><xocs:ref-last-lp>1152</xocs:ref-last-lp><xocs:ref-normalized-initial>G</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0100"><xocs:ref-normalized-surname>PATEL</xocs:ref-normalized-surname><xocs:ref-pub-year>2007</xocs:ref-pub-year><xocs:ref-first-fp>677</xocs:ref-first-fp><xocs:ref-last-lp>683</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0105"><xocs:ref-normalized-surname>POKRAJAC</xocs:ref-normalized-surname><xocs:ref-pub-year>2005</xocs:ref-pub-year><xocs:ref-first-fp>261</xocs:ref-first-fp><xocs:ref-last-lp>280</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0110"><xocs:ref-normalized-surname>PROPST</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>22</xocs:ref-first-fp><xocs:ref-last-lp>26</xocs:ref-last-lp><xocs:ref-normalized-initial>E</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0115"><xocs:ref-normalized-surname>SCHEFFLER</xocs:ref-normalized-surname><xocs:ref-pub-year>1998</xocs:ref-pub-year><xocs:ref-first-fp>156</xocs:ref-first-fp><xocs:ref-last-lp>163</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0125"><xocs:ref-normalized-surname>SCHMITHORST</xocs:ref-normalized-surname><xocs:ref-pub-year>2004</xocs:ref-pub-year><xocs:ref-first-fp>399</xocs:ref-first-fp><xocs:ref-last-lp>402</xocs:ref-last-lp><xocs:ref-normalized-initial>V</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0120"><xocs:ref-normalized-surname>SCHMITHORST</xocs:ref-normalized-surname><xocs:ref-pub-year>2001</xocs:ref-pub-year><xocs:ref-first-fp>535</xocs:ref-first-fp><xocs:ref-last-lp>539</xocs:ref-last-lp><xocs:ref-normalized-initial>V</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0130"><xocs:ref-normalized-surname>SCHMITHORST</xocs:ref-normalized-surname><xocs:ref-pub-year>2006</xocs:ref-pub-year><xocs:ref-first-fp>254</xocs:ref-first-fp><xocs:ref-last-lp>266</xocs:ref-last-lp><xocs:ref-normalized-initial>V</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0260"><xocs:ref-normalized-surname>SCHMITHORST</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-normalized-initial>V</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>CCHIPSCINCINNATICHILDRENSHOSPITALIMAGEPROCESSINGSOFTWARE</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="rf0135"><xocs:ref-normalized-surname>SIVIC</xocs:ref-normalized-surname><xocs:ref-pub-year>2009</xocs:ref-pub-year><xocs:ref-first-fp>591</xocs:ref-first-fp><xocs:ref-last-lp>605</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0140"><xocs:ref-normalized-surname>SMITH</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>991</xocs:ref-first-fp><xocs:ref-last-lp>998</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0145"><xocs:ref-normalized-surname>THEVENAZ</xocs:ref-normalized-surname><xocs:ref-pub-year>1998</xocs:ref-pub-year><xocs:ref-first-fp>27</xocs:ref-first-fp><xocs:ref-last-lp>41</xocs:ref-last-lp><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0150"><xocs:ref-normalized-surname>TILLEMA</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>99</xocs:ref-first-fp><xocs:ref-last-lp>111</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0155"><xocs:ref-normalized-surname>TLUSTOS</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>181</xocs:ref-first-fp><xocs:ref-last-lp>189</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0160"><xocs:ref-normalized-surname>TOEWS</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>2318</xocs:ref-first-fp><xocs:ref-last-lp>2327</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0165"><xocs:ref-normalized-surname>TOSUN</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>186</xocs:ref-first-fp><xocs:ref-last-lp>197</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0170"><xocs:ref-normalized-surname>TRIMBLE</xocs:ref-normalized-surname><xocs:ref-pub-year>2007</xocs:ref-pub-year><xocs:ref-first-fp>317</xocs:ref-first-fp><xocs:ref-last-lp>324</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0175"><xocs:ref-normalized-surname>TSCHOPP</xocs:ref-normalized-surname><xocs:ref-pub-year>2000</xocs:ref-pub-year><xocs:ref-first-fp>753</xocs:ref-first-fp><xocs:ref-last-lp>757</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0265"><xocs:ref-normalized-surname>VEDALDI</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSINTERNATIONALCONFERENCEMULTIMEDIA</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>VLFEATOPENPORTABLELIBRARYCOMPUTERVISIONALGORITHMS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="rf0185"><xocs:ref-normalized-surname>WANG</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-first-fp>931</xocs:ref-first-fp><xocs:ref-last-lp>940</xocs:ref-last-lp><xocs:ref-normalized-initial>L</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0190"><xocs:ref-normalized-surname>WEISSMAN</xocs:ref-normalized-surname><xocs:ref-pub-year>2005</xocs:ref-pub-year><xocs:ref-first-fp>229</xocs:ref-first-fp><xocs:ref-last-lp>237</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0195"><xocs:ref-normalized-surname>WILKE</xocs:ref-normalized-surname><xocs:ref-pub-year>2003</xocs:ref-pub-year><xocs:ref-first-fp>42</xocs:ref-first-fp><xocs:ref-last-lp>44</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="rf0200"><xocs:ref-normalized-surname>WORSLEY</xocs:ref-normalized-surname><xocs:ref-pub-year>2002</xocs:ref-pub-year><xocs:ref-first-fp>1</xocs:ref-first-fp><xocs:ref-last-lp>15</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial></xocs:ref-info></xocs:references><xocs:attachment-metadata-doc><xocs:attachment-set-type>item</xocs:attachment-set-type><xocs:pii-formatted>S2213-1582(13)00125-3</xocs:pii-formatted><xocs:pii-unformatted>S2213158213001253</xocs:pii-unformatted><xocs:eid>1-s2.0-S2213158213001253</xocs:eid><xocs:doi>10.1016/j.nicl.2013.09.008</xocs:doi><xocs:cid>282794</xocs:cid><xocs:timestamp>2013-12-11T09:36:28.058866-05:00</xocs:timestamp><xocs:path>/282794/1-s2.0-S2213158213X00026/1-s2.0-S2213158213001253/</xocs:path><xocs:cover-date-start>2013-01-01</xocs:cover-date-start><xocs:cover-date-end>2013-12-31</xocs:cover-date-end><xocs:sponsored-access-type>UNLIMITED</xocs:sponsored-access-type><xocs:funding-body-id>NONE</xocs:funding-body-id><xocs:attachments><xocs:web-pdf><xocs:attachment-eid>1-s2.0-S2213158213001253-main.pdf</xocs:attachment-eid><xocs:filename>main.pdf</xocs:filename><xocs:extension>pdf</xocs:extension><xocs:pdf-optimized>true</xocs:pdf-optimized><xocs:filesize>1585277</xocs:filesize><xocs:web-pdf-purpose>MAIN</xocs:web-pdf-purpose><xocs:web-pdf-page-count>13</xocs:web-pdf-page-count><xocs:web-pdf-images><xocs:web-pdf-image><xocs:attachment-eid>1-s2.0-S2213158213001253-main_1.png</xocs:attachment-eid><xocs:filename>main_1.png</xocs:filename><xocs:extension>png</xocs:extension><xocs:filesize>146192</xocs:filesize><xocs:pixel-height>849</xocs:pixel-height><xocs:pixel-width>656</xocs:pixel-width><xocs:attachment-type>IMAGE-WEB-PDF</xocs:attachment-type><xocs:pdf-page-num>1</xocs:pdf-page-num></xocs:web-pdf-image></xocs:web-pdf-images></xocs:web-pdf><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si9.gif</xocs:attachment-eid><xocs:file-basename>si9</xocs:file-basename><xocs:filename>si9.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1315</xocs:filesize><xocs:pixel-height>27</xocs:pixel-height><xocs:pixel-width>68</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si8.gif</xocs:attachment-eid><xocs:file-basename>si8</xocs:file-basename><xocs:filename>si8.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>2672</xocs:filesize><xocs:pixel-height>30</xocs:pixel-height><xocs:pixel-width>171</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si7.gif</xocs:attachment-eid><xocs:file-basename>si7</xocs:file-basename><xocs:filename>si7.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1805</xocs:filesize><xocs:pixel-height>44</xocs:pixel-height><xocs:pixel-width>113</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si6.gif</xocs:attachment-eid><xocs:file-basename>si6</xocs:file-basename><xocs:filename>si6.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>2739</xocs:filesize><xocs:pixel-height>44</xocs:pixel-height><xocs:pixel-width>196</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si5.gif</xocs:attachment-eid><xocs:file-basename>si5</xocs:file-basename><xocs:filename>si5.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>2861</xocs:filesize><xocs:pixel-height>22</xocs:pixel-height><xocs:pixel-width>329</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si4.gif</xocs:attachment-eid><xocs:file-basename>si4</xocs:file-basename><xocs:filename>si4.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1420</xocs:filesize><xocs:pixel-height>23</xocs:pixel-height><xocs:pixel-width>103</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si3.gif</xocs:attachment-eid><xocs:file-basename>si3</xocs:file-basename><xocs:filename>si3.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>2284</xocs:filesize><xocs:pixel-height>23</xocs:pixel-height><xocs:pixel-width>200</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si25.gif</xocs:attachment-eid><xocs:file-basename>si25</xocs:file-basename><xocs:filename>si25.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1629</xocs:filesize><xocs:pixel-height>36</xocs:pixel-height><xocs:pixel-width>187</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si24.gif</xocs:attachment-eid><xocs:file-basename>si24</xocs:file-basename><xocs:filename>si24.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1869</xocs:filesize><xocs:pixel-height>48</xocs:pixel-height><xocs:pixel-width>107</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si23.gif</xocs:attachment-eid><xocs:file-basename>si23</xocs:file-basename><xocs:filename>si23.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>2557</xocs:filesize><xocs:pixel-height>30</xocs:pixel-height><xocs:pixel-width>171</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si22.gif</xocs:attachment-eid><xocs:file-basename>si22</xocs:file-basename><xocs:filename>si22.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>3111</xocs:filesize><xocs:pixel-height>13</xocs:pixel-height><xocs:pixel-width>330</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si21.gif</xocs:attachment-eid><xocs:file-basename>si21</xocs:file-basename><xocs:filename>si21.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1282</xocs:filesize><xocs:pixel-height>37</xocs:pixel-height><xocs:pixel-width>89</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si20.gif</xocs:attachment-eid><xocs:file-basename>si20</xocs:file-basename><xocs:filename>si20.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>2494</xocs:filesize><xocs:pixel-height>30</xocs:pixel-height><xocs:pixel-width>212</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si2.gif</xocs:attachment-eid><xocs:file-basename>si2</xocs:file-basename><xocs:filename>si2.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1498</xocs:filesize><xocs:pixel-height>30</xocs:pixel-height><xocs:pixel-width>87</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si19.gif</xocs:attachment-eid><xocs:file-basename>si19</xocs:file-basename><xocs:filename>si19.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>911</xocs:filesize><xocs:pixel-height>14</xocs:pixel-height><xocs:pixel-width>13</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si18.gif</xocs:attachment-eid><xocs:file-basename>si18</xocs:file-basename><xocs:filename>si18.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>877</xocs:filesize><xocs:pixel-height>14</xocs:pixel-height><xocs:pixel-width>14</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si17.gif</xocs:attachment-eid><xocs:file-basename>si17</xocs:file-basename><xocs:filename>si17.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1336</xocs:filesize><xocs:pixel-height>40</xocs:pixel-height><xocs:pixel-width>118</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si16.gif</xocs:attachment-eid><xocs:file-basename>si16</xocs:file-basename><xocs:filename>si16.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>875</xocs:filesize><xocs:pixel-height>9</xocs:pixel-height><xocs:pixel-width>9</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si15.gif</xocs:attachment-eid><xocs:file-basename>si15</xocs:file-basename><xocs:filename>si15.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>3315</xocs:filesize><xocs:pixel-height>23</xocs:pixel-height><xocs:pixel-width>296</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si14.gif</xocs:attachment-eid><xocs:file-basename>si14</xocs:file-basename><xocs:filename>si14.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>875</xocs:filesize><xocs:pixel-height>9</xocs:pixel-height><xocs:pixel-width>9</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si13.gif</xocs:attachment-eid><xocs:file-basename>si13</xocs:file-basename><xocs:filename>si13.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1383</xocs:filesize><xocs:pixel-height>44</xocs:pixel-height><xocs:pixel-width>64</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si12.gif</xocs:attachment-eid><xocs:file-basename>si12</xocs:file-basename><xocs:filename>si12.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>2959</xocs:filesize><xocs:pixel-height>44</xocs:pixel-height><xocs:pixel-width>210</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si11.gif</xocs:attachment-eid><xocs:file-basename>si11</xocs:file-basename><xocs:filename>si11.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1432</xocs:filesize><xocs:pixel-height>45</xocs:pixel-height><xocs:pixel-width>112</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si10.gif</xocs:attachment-eid><xocs:file-basename>si10</xocs:file-basename><xocs:filename>si10.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>908</xocs:filesize><xocs:pixel-height>16</xocs:pixel-height><xocs:pixel-width>10</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-si1.gif</xocs:attachment-eid><xocs:file-basename>si1</xocs:file-basename><xocs:filename>si1.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1562</xocs:filesize><xocs:pixel-height>39</xocs:pixel-height><xocs:pixel-width>103</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr7.jpg</xocs:attachment-eid><xocs:file-basename>gr7</xocs:file-basename><xocs:filename>gr7.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>178450</xocs:filesize><xocs:pixel-height>1025</xocs:pixel-height><xocs:pixel-width>734</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr6.jpg</xocs:attachment-eid><xocs:file-basename>gr6</xocs:file-basename><xocs:filename>gr6.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>66826</xocs:filesize><xocs:pixel-height>381</xocs:pixel-height><xocs:pixel-width>781</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr5.jpg</xocs:attachment-eid><xocs:file-basename>gr5</xocs:file-basename><xocs:filename>gr5.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>37463</xocs:filesize><xocs:pixel-height>497</xocs:pixel-height><xocs:pixel-width>646</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr4.jpg</xocs:attachment-eid><xocs:file-basename>gr4</xocs:file-basename><xocs:filename>gr4.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>64377</xocs:filesize><xocs:pixel-height>482</xocs:pixel-height><xocs:pixel-width>714</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr3.jpg</xocs:attachment-eid><xocs:file-basename>gr3</xocs:file-basename><xocs:filename>gr3.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>73236</xocs:filesize><xocs:pixel-height>411</xocs:pixel-height><xocs:pixel-width>809</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr2.jpg</xocs:attachment-eid><xocs:file-basename>gr2</xocs:file-basename><xocs:filename>gr2.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>49090</xocs:filesize><xocs:pixel-height>216</xocs:pixel-height><xocs:pixel-width>535</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr1.jpg</xocs:attachment-eid><xocs:file-basename>gr1</xocs:file-basename><xocs:filename>gr1.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>22543</xocs:filesize><xocs:pixel-height>225</xocs:pixel-height><xocs:pixel-width>459</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr7.sml</xocs:attachment-eid><xocs:file-basename>gr7</xocs:file-basename><xocs:filename>gr7.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>9624</xocs:filesize><xocs:pixel-height>163</xocs:pixel-height><xocs:pixel-width>117</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr6.sml</xocs:attachment-eid><xocs:file-basename>gr6</xocs:file-basename><xocs:filename>gr6.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>4512</xocs:filesize><xocs:pixel-height>107</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr5.sml</xocs:attachment-eid><xocs:file-basename>gr5</xocs:file-basename><xocs:filename>gr5.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>3732</xocs:filesize><xocs:pixel-height>164</xocs:pixel-height><xocs:pixel-width>213</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr4.sml</xocs:attachment-eid><xocs:file-basename>gr4</xocs:file-basename><xocs:filename>gr4.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>4382</xocs:filesize><xocs:pixel-height>148</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr3.sml</xocs:attachment-eid><xocs:file-basename>gr3</xocs:file-basename><xocs:filename>gr3.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>6935</xocs:filesize><xocs:pixel-height>111</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr2.sml</xocs:attachment-eid><xocs:file-basename>gr2</xocs:file-basename><xocs:filename>gr2.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>9999</xocs:filesize><xocs:pixel-height>88</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-gr1.sml</xocs:attachment-eid><xocs:file-basename>gr1</xocs:file-basename><xocs:filename>gr1.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>4364</xocs:filesize><xocs:pixel-height>107</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S2213158213001253-mmc1.pdf</xocs:attachment-eid><xocs:file-basename>mmc1</xocs:file-basename><xocs:filename>mmc1.pdf</xocs:filename><xocs:extension>pdf</xocs:extension><xocs:filesize>1599763</xocs:filesize><xocs:attachment-type>APPLICATION</xocs:attachment-type></xocs:attachment></xocs:attachments></xocs:attachment-metadata-doc><xocs:refkeys><xocs:refkey3>TANX2013X416</xocs:refkey3><xocs:refkey4lp>TANX2013X416X428</xocs:refkey4lp><xocs:refkey4ai>TANX2013X416XL</xocs:refkey4ai><xocs:refkey5>TANX2013X416X428XL</xocs:refkey5></xocs:refkeys><xocs:open-access><xocs:oa-article-status is-open-access="1" is-open-archive="0">Full</xocs:oa-article-status><xocs:oa-access-effective-date>2013-09-25T14:20:31Z</xocs:oa-access-effective-date><xocs:oa-sponsor><xocs:oa-sponsor-type>Author</xocs:oa-sponsor-type></xocs:oa-sponsor><xocs:oa-user-license>http://creativecommons.org/licenses/by/3.0/</xocs:oa-user-license></xocs:open-access></xocs:meta><xocs:serial-item><article xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd" version="5.2" docsubtype="fla" xml:lang="en"><item-info><jid>YNICL</jid><aid>165</aid><ce:pii>S2213-1582(13)00125-3</ce:pii><ce:doi>10.1016/j.nicl.2013.09.008</ce:doi><ce:copyright type="other" year="2013">The Authors</ce:copyright><ce:copyright-line>Â© 2013. Published by The Author. All rights reserved.</ce:copyright-line></item-info><ce:floats><ce:figure id="f0005"><ce:label>Fig. 1</ce:label><ce:caption id="ca0005"><ce:simple-para id="sp0015" view="all">Timing diagram of a single block of the HUSH fMRI data acquisition and auditory stimulation paradigm. MRI scans and accompanying gradient noise occurred in the intervals labeled "scan". During other intervals of the sequence, the scanner remained silent. Sound level inside the scanner was 104<ce:hsp sp="0.25"/>dB during the acquisitions and ~<ce:hsp sp="0.10"/>60<ce:hsp sp="0.25"/>dB during the silent intervals. This block was repeated 18 times for a total scan duration of approximately 11<ce:hsp sp="0.25"/>min.</ce:simple-para></ce:caption><ce:link locator="gr1" id="lk0005"/></ce:figure><ce:figure id="f0010"><ce:label>Fig. 2</ce:label><ce:caption id="ca0010"><ce:simple-para id="sp0020" view="all">SIFT features. (a) SIFT features extracted from a 2D slice of a brain MR image. Every circle represents a feature. The center of a circle represents the location of the feature. The radius of the circle represents the scale of the feature. There is an appearance matrix associated with every SIFT feature. (b) An appearance matrix consists of a 4<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>4 squares. Every square contains an orientation histogram to represent the gradient orientations of the pixels within this square (<ce:cross-ref refid="bb0130" id="cf0005">Lowe, 1999</ce:cross-ref>). The length of the short lines within the squares represents the number of pixels with the corresponding gradient orientation.</ce:simple-para></ce:caption><ce:link locator="gr2" id="lk0010"/></ce:figure><ce:figure id="f0015"><ce:label>Fig. 3</ce:label><ce:caption id="ca0015"><ce:simple-para id="sp0025" view="all">Vectorization of contrast maps. The first row of images is the original contrast maps, and the second row is the extracted ROIs. For visualization purposes, we show only one slice of a 3D brain image. For each type of contrast, we delineated ROIs from all 39 subjects to construct our dictionary. The dictionary size of the contrast speech vs. silence, tones vs. silence and speech vs. tones was 483, 494 and 497, respectively. These three dictionaries were applied to the corresponding contrast maps for each subject. As a result, each subject was represented with a 1474-D vector.</ce:simple-para></ce:caption><ce:link locator="gr3" id="lk0015"/></ce:figure><ce:figure id="f0020"><ce:label>Fig. 4</ce:label><ce:caption id="ca0020"><ce:simple-para id="sp0030" view="all">Framework for the two-layer classifier.</ce:simple-para></ce:caption><ce:link locator="gr4" id="lk0020"/></ce:figure><ce:figure id="f0025"><ce:label>Fig. 5</ce:label><ce:caption id="ca0025"><ce:simple-para id="sp0035" view="all">ROCs of the three classifiers.</ce:simple-para></ce:caption><ce:link locator="gr5" id="lk0025"/></ce:figure><ce:figure id="f0030"><ce:label>Fig. 6</ce:label><ce:caption id="ca0030"><ce:simple-para id="sp0040" view="all">Distribution of sMRI-fMRI scores. Each panel is one-fold of cross-validation. The horizontal axis is the output of the sMRI classifier and the vertical axis is the output of the fMRI classifier. Blue dots are HI training samples, red dots are NH training samples, the black star is the testing sample. The true label of the testing sample is HI for all the 8 folds of cross validation. Figures for all the 39 folds of cross-validation can be found in Fig. S1.</ce:simple-para></ce:caption><ce:link locator="gr6" id="lk0030"/></ce:figure><ce:figure id="f0035"><ce:label>Fig. 7</ce:label><ce:caption id="ca0035"><ce:simple-para id="sp0045" view="all">Important brain regions identified by the fMRI classifier as differentiating the hearing impaired group (HI) from the normal hearing control group (NH). The brain regions are visualized with the xjview toolbox (<ce:inter-ref xlink:href="http://www.alivelearn.net/xjview" id="ir0010" xlink:type="simple">http://www.alivelearn.net/xjview</ce:inter-ref>). Images are displayed in neurological orientation.</ce:simple-para></ce:caption><ce:link locator="gr7" id="lk0035"/></ce:figure><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" id="t0005" frame="topbot" colsep="0" rowsep="0"><ce:label>Table 1</ce:label><ce:caption id="ca0040"><ce:simple-para id="sp0050" view="all">Classification performance of the three classifiers.</ce:simple-para></ce:caption><tgroup cols="6"><colspec colname="col1"/><colspec colname="col2"/><colspec colname="col3"/><colspec colname="col4"/><colspec colname="col5"/><colspec colname="col6"/><thead><row valign="top" rowsep="1"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">Specificity</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Sensitivity</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">Accuracy</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">AUC</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col6" align="left">EER</entry></row></thead><tbody><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">sMRI</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="char" char=".">0.83</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="char" char=".">0.62</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="char" char=".">0.72</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="char" char=".">0.78</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col6" align="char" char=".">0.78</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">fMRI</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="char" char=".">0.76</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="char" char=".">0.72</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="char" char=".">0.74</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="char" char=".">0.83</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col6" align="char" char=".">0.76</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">sMRI<hsp sp="0.25"/>+<hsp sp="0.25"/>fMRI</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="char" char=".">0.86</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="char" char=".">0.89</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="char" char=".">0.87</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="char" char=".">0.90</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col6" align="char" char=".">0.89</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" id="t0010" frame="topbot" colsep="0" rowsep="0"><ce:label>Table 2</ce:label><ce:caption id="ca0045"><ce:simple-para id="sp0055" view="all">Classification performance with different likelihood thresholds for feature selection from sMRI data.</ce:simple-para></ce:caption><tgroup cols="6"><colspec colname="col1"/><colspec colname="col2"/><colspec colname="col3"/><colspec colname="col4"/><colspec colname="col5"/><colspec colname="col6"/><tbody><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">Likelihood threshold</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="char" char=".">0.7</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="char" char=".">0.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="char" char=".">0.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="char" char=".">1.0</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col6" align="char" char=".">1.1</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">Accuracy</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="char" char=".">0.72</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="char" char=".">0.72</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="char" char=".">0.72</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="char" char=".">0.72</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col6" align="char" char=".">0.72</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">AUC</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="char" char=".">0.69</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="char" char=".">0.71</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="char" char=".">0.78</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="char" char=".">0.70</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col6" align="char" char=".">0.74</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">EER</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="char" char=".">0.69</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="char" char=".">0.69</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="char" char=".">0.77</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="char" char=".">0.77</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col6" align="char" char=".">0.77</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" id="t0015" frame="topbot" colsep="0" rowsep="0"><ce:label>Table 3</ce:label><ce:caption id="ca0050"><ce:simple-para id="sp0060" view="all">Characteristics of the top functional ROIs. STG is short for singular temporal gyrus, MTG for middle temporal gyrus.</ce:simple-para></ce:caption><tgroup cols="5"><colspec colname="col1"/><colspec colname="col2"/><colspec colname="col3"/><colspec colname="col4"/><colspec colname="col5"/><thead><row valign="top" rowsep="1"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">Feature</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">Number of voxels</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Contrasts</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">BA areas</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Anatomical labels</entry></row></thead><tbody><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">A1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">16</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Speech vs. silence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">Red Nuc.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Red Nuc.</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">A2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">10</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Speech vs. silence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">Thal., Sub Thal. Nuc</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Thal., Sub Thal. Nuc.</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">B</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">19</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Speech vs. tones</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">22</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">STG</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">C1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">19</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Tones vs. silence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">33</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Pregenual Cing. G.</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">C2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">15</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Tones vs. silence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">33,24</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Pregenual Cing. G.,<br/>Vent. Ant. Cing. G.</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">D</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">14</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Tones vs. silence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">Auditory nuclei</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Pontine Auditory Nuclei</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">E1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">12</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Speech vs. tones</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">32</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Cing. G.</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">E2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">11</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Speech vs. tones</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">32,10</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Cing. G., Prefrontal Cortex</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">F1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">60</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Tones vs. silence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">22,40,39,13</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">STG, Super Marg. G, Angu. G., Ins.</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">F2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">40</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Tones vs. silence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">40,39,13</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Super Marg. G, Angu. G., Ins.</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">G</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">11</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Speech vs. silence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">24,32</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Vent. Ant. Cing. G.,Cing. G.</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">H</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">115</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Speech vs. silence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">19,22,39,18,21,40</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Mid Occ. G., STG, Angu. G., Lingual G, MTG, Super Marg. G</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">I1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">24</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Speech vs. silence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">22,40,39</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">STG, Super Marg. G, Angu. G.</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">I2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">30</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Speech vs. silence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">40,39</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Super Marg. G, Angu. G.</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">J1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">253</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Tones vs. silence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">Thal., Red Nuc.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Thal., Red Nuc.</entry></row><row valign="top"><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col1" align="left">J2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col2" align="left">142</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col3" align="left">Tones vs. silence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col4" align="left">Thal., Mamillary Body</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" colname="col5" align="left">Thal., Mamillary Body</entry></row></tbody></tgroup></ce:table></ce:floats><head><ce:article-footnote><ce:label>â</ce:label><ce:note-para id="np0010" view="all">This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</ce:note-para></ce:article-footnote><ce:title id="ti0005">Combined analysis of sMRI and fMRI imaging data provides accurate disease markers for hearing impairment</ce:title><ce:author-group id="ag0005"><ce:author id="au0005"><ce:given-name>Lirong</ce:given-name><ce:surname>Tan</ce:surname><ce:cross-ref refid="af0005" id="cf0010"><ce:sup loc="post">a</ce:sup></ce:cross-ref><ce:cross-ref refid="af0010" id="cf0015"><ce:sup loc="post">b</ce:sup></ce:cross-ref></ce:author><ce:author id="au0010"><ce:given-name>Ye</ce:given-name><ce:surname>Chen</ce:surname><ce:cross-ref refid="af0005" id="cf0020"><ce:sup loc="post">a</ce:sup></ce:cross-ref><ce:cross-ref refid="af0015" id="cf0025"><ce:sup loc="post">c</ce:sup></ce:cross-ref></ce:author><ce:author id="au0015"><ce:given-name>Thomas C.</ce:given-name><ce:surname>Maloney</ce:surname><ce:cross-ref refid="af0020" id="cf0030"><ce:sup loc="post">d</ce:sup></ce:cross-ref></ce:author><ce:author id="au0020"><ce:indexed-name>Care</ce:indexed-name><ce:given-name>Marguerite M.</ce:given-name><ce:surname>CarÃ©</ce:surname><ce:cross-ref refid="af0025" id="cf0035"><ce:sup loc="post">e</ce:sup></ce:cross-ref></ce:author><ce:author id="au0025"><ce:given-name>Scott K.</ce:given-name><ce:surname>Holland</ce:surname><ce:cross-ref refid="af0020" id="cf0040"><ce:sup loc="post">d</ce:sup></ce:cross-ref><ce:cross-ref refid="af0025" id="cf0045"><ce:sup loc="post">e</ce:sup></ce:cross-ref></ce:author><ce:author id="au0030"><ce:given-name>Long J.</ce:given-name><ce:surname>Lu</ce:surname><ce:cross-ref refid="af0005" id="cf0050"><ce:sup loc="post">a</ce:sup></ce:cross-ref><ce:cross-ref refid="af0010" id="cf0055"><ce:sup loc="post">b</ce:sup></ce:cross-ref><ce:cross-ref refid="af0030" id="cf0060"><ce:sup loc="post">f</ce:sup></ce:cross-ref><ce:cross-ref refid="cr0005" id="cf0065"><ce:sup loc="post">â</ce:sup></ce:cross-ref><ce:e-address id="em0005" type="email">long.lu@cchmc.org</ce:e-address><ce:e-address type="url" id="em0010">http://dragon.cchmc.org</ce:e-address></ce:author><ce:affiliation id="af0005"><ce:label>a</ce:label><ce:textfn id="tn0005">Division of Biomedical Informatics, Cincinnati Children's Hospital Research Foundation, 3333 Burnet Avenue, Cincinnati, OH 45229-3026, USA</ce:textfn><sa:affiliation><sa:organization>Division of Biomedical Informatics</sa:organization><sa:organization>Cincinnati Children's Hospital Research Foundation</sa:organization><sa:address-line>3333 Burnet Avenue</sa:address-line><sa:city>Cincinnati</sa:city><sa:state>OH</sa:state><sa:postal-code>45229-3026</sa:postal-code><sa:country>USA</sa:country></sa:affiliation></ce:affiliation><ce:affiliation id="af0010"><ce:label>b</ce:label><ce:textfn id="tn0010">School of Computing Sciences and Informatics, University of Cincinnati, 810 Old Chemistry, Cincinnati, OH 45221-0008, USA</ce:textfn><sa:affiliation><sa:organization>School of Computing Sciences and Informatics</sa:organization><sa:organization>University of Cincinnati</sa:organization><sa:address-line>810 Old Chemistry</sa:address-line><sa:city>Cincinnati</sa:city><sa:state>OH</sa:state><sa:postal-code>45221-0008</sa:postal-code><sa:country>USA</sa:country></sa:affiliation></ce:affiliation><ce:affiliation id="af0015"><ce:label>c</ce:label><ce:textfn id="tn0015">School of Electronics and Computing Systems, University of Cincinnati, 497 Rhodes Hall, Cincinnati, OH 45221, USA</ce:textfn><sa:affiliation><sa:organization>School of Electronics and Computing Systems</sa:organization><sa:organization>University of Cincinnati</sa:organization><sa:address-line>497 Rhodes Hall</sa:address-line><sa:city>Cincinnati</sa:city><sa:state>OH</sa:state><sa:postal-code>45221</sa:postal-code><sa:country>USA</sa:country></sa:affiliation></ce:affiliation><ce:affiliation id="af0020"><ce:label>d</ce:label><ce:textfn id="tn0020">Pediatric Neuroimaging Research Consortium, Cincinnati Children's Hospital Medical Center, Cincinnati, OH 45221, USA</ce:textfn><sa:affiliation><sa:organization>Pediatric Neuroimaging Research Consortium</sa:organization><sa:organization>Cincinnati Children's Hospital Medical Center</sa:organization><sa:city>Cincinnati</sa:city><sa:state>OH</sa:state><sa:postal-code>45221</sa:postal-code><sa:country>USA</sa:country></sa:affiliation></ce:affiliation><ce:affiliation id="af0025"><ce:label>e</ce:label><ce:textfn id="tn0025">Department of Pediatric Radiology, Cincinnati Children's Hospital Medical Center, Cincinnati, OH 45221, USA</ce:textfn><sa:affiliation><sa:organization>Department of Pediatric Radiology</sa:organization><sa:organization>Cincinnati Children's Hospital Medical Center</sa:organization><sa:city>Cincinnati</sa:city><sa:state>OH</sa:state><sa:postal-code>45221</sa:postal-code><sa:country>USA</sa:country></sa:affiliation></ce:affiliation><ce:affiliation id="af0030"><ce:label>f</ce:label><ce:textfn id="tn0030">Department of Environmental Health, College of Medicine, University of Cincinnati, 231 Albert Sabin Way, Cincinnati, OH 45267-0524, USA</ce:textfn><sa:affiliation><sa:organization>Department of Environmental Health</sa:organization><sa:organization>College of Medicine</sa:organization><sa:organization>University of Cincinnati</sa:organization><sa:address-line>231 Albert Sabin Way</sa:address-line><sa:city>Cincinnati</sa:city><sa:state>OH</sa:state><sa:postal-code>45267-0524</sa:postal-code><sa:country>USA</sa:country></sa:affiliation></ce:affiliation><ce:correspondence id="cr0005"><ce:label>â</ce:label><ce:text id="tx0005">Corresponding author at: Division of Biomedical Informatics, MLC 7024, Cincinnati Children's Hospital Research Foundation, 3333 Burnet Avenue, Cincinnati, OH 45229, USA. Tel.: +<ce:hsp sp="0.10"/>1 513 636 8720; fax: +<ce:hsp sp="0.10"/>1 513 636 2056.</ce:text></ce:correspondence></ce:author-group><ce:date-received day="5" month="6" year="2013"/><ce:date-revised day="23" month="9" year="2013"/><ce:date-accepted day="23" month="9" year="2013"/><ce:abstract id="ab0005" view="all" class="author"><ce:section-title id="st0005">Abstract</ce:section-title><ce:abstract-sec id="as0005" view="all"><ce:simple-para id="sp0005" view="all">In this research, we developed a robust two-layer classifier that can accurately classify normal hearing (NH) from hearing impaired (HI) infants with congenital sensori-neural hearing loss (SNHL) based on their Magnetic Resonance (MR) images. Unlike traditional methods that examine the intensity of each single voxel, we extracted high-level features to characterize the structural MR images (sMRI) and functional MR images (fMRI). The Scale Invariant Feature Transform (SIFT) algorithm was employed to detect and describe the local features in sMRI. For fMRI, we constructed contrast maps and detected the most activated/de-activated regions in each individual. Based on those salient regions occurring across individuals, the bag-of-words strategy was introduced to vectorize the contrast maps. We then used a two-layer model to integrate these two types of features together. With the leave-one-out cross-validation approach, this integrated model achieved an AUC score of 0.90. Additionally, our algorithm highlighted several important brain regions that differentiated between NH and HI children. Some of these regions, e.g. planum temporale and angular gyrus, were well known auditory and visual language association regions. Others, e.g. the anterior cingulate cortex (ACC), were not necessarily expected to play a role in differentiating HI from NH children and provided a new understanding of brain function and of the disorder itself. These important brain regions provided clues about neuroimaging markers that may be relevant to the future use of functional neuroimaging to guide predictions about speech and language outcomes in HI infants who receive a cochlear implant. This type of prognostic information could be extremely useful and is currently not available to clinicians by any other means.</ce:simple-para></ce:abstract-sec></ce:abstract><ce:abstract class="author-highlights" id="ab0010" view="all"><ce:section-title id="st0010">Highlights</ce:section-title><ce:abstract-sec id="as0010" view="all"><ce:simple-para id="sp0010" view="all"><ce:list id="l0005"><ce:list-item id="u0005"><ce:label>â¢</ce:label><ce:para id="p0480" view="all">We probe brain structural and functional changes in hearing impaired (HI) infants.</ce:para></ce:list-item><ce:list-item id="u0010"><ce:label>â¢</ce:label><ce:para id="p0485" view="all">We build a robust two-layer classifier that integrates sMRI and fMRI data.</ce:para></ce:list-item><ce:list-item id="u0015"><ce:label>â¢</ce:label><ce:para id="p0490" view="all">This integrated model accurately separates HI from normal infants (AUC 0.9).</ce:para></ce:list-item><ce:list-item id="u0020"><ce:label>â¢</ce:label><ce:para id="p0495" view="all">Our method detects important brain regions different between HI and normal infants.</ce:para></ce:list-item><ce:list-item id="u0025"><ce:label>â¢</ce:label><ce:para id="p0500" view="all">Our method can include diverse types of data and be applied to other diseases.</ce:para></ce:list-item></ce:list></ce:simple-para></ce:abstract-sec></ce:abstract><ce:keywords id="ks0005" view="all" class="keyword"><ce:section-title id="st0015">Keywords</ce:section-title><ce:keyword id="kw0035"><ce:text id="tx0010">Brain</ce:text></ce:keyword><ce:keyword id="kw0040"><ce:text id="tx0015">Hearing loss</ce:text></ce:keyword><ce:keyword id="kw0045"><ce:text id="tx0020">MRI images</ce:text></ce:keyword><ce:keyword id="kw0050"><ce:text id="tx0025">sMRI</ce:text></ce:keyword><ce:keyword id="kw0055"><ce:text id="tx0030">fMRI</ce:text></ce:keyword><ce:keyword id="kw0060"><ce:text id="tx0035">SVM</ce:text></ce:keyword></ce:keywords></head><body view="all"><ce:sections><ce:section id="s0005" view="all"><ce:label>1</ce:label><ce:section-title id="st0020">Introduction</ce:section-title><ce:para id="p0005" view="all">It has been estimated that approximately 1 to 6 infants per 1000 are born with severe to profound congenital sensori-neural hearing loss (SNHL) (<ce:cross-refs refid="bb0010 bb0035 bb0105 bb0140" id="cf0070">Bachmann and Arvedson, 1998; Cunningham and Cox, 2003; Kemper and Downs, 2000; Northern, 1994</ce:cross-refs>). Those children receive little or no benefit from hearing aids and face challenges in developing language abilities due to their inability to detect acoustic-phonetic signals, which are essential for hearing-dependent learning. Cochlear implantation (CI) is a surgical procedure that inserts an electronic device into the cochlea for direct stimulation of the auditory nerve and has been demonstrated to be effective in restoring hearing in patients suffering from SNHL. Statistical data from the National Institute on Deafness and Other Communication Disorders (NIDCD) indicate that approximately 28,400 children in the United States have received a cochlear implant as of December 2010. While many congenitally deaf CI recipients achieve a high degree of accuracy in speech perception and develop near-normal language skills, about 30% of the recipients do not derive any benefit from the CI (<ce:cross-ref refid="bb0135" id="cf0075">Niparko et al., 2010</ce:cross-ref>). A deeper understanding of hearing loss and better characterization of the brain regions affected by hearing loss will help reduce the high variance in CI outcomes and result in a more effective treatment of children with hearing loss.</ce:para><ce:para id="p0010" view="all">In recent years, Magnetic Resonance (MR) images have been used to study neurological disorders and brain development in children, such as reading and attention problems, traumatic brain injury, hearing impairment, perinatal stroke and other conditions (<ce:cross-refs refid="bb0080 bb0120 bb0195 bb0205 bb0210" id="cf0080">Horowitz-Kraus and Holland, 2012; Leach and Holland, 2010; Smith et al., 2011; Tillema et al., 2008; Tlustos et al., 2011</ce:cross-refs>). Brain MRI scans have revealed significant differences between Hearing Impaired (HI) and Normal Hearing (NH) children. Jonas et al. reviewed a total number of 162 patients' structural MRI scans, and detected 51 abnormalities in 49 patients. Those abnormalities included white matter changes, structural or anatomical abnormalities, neoplasms, gray matter changes, vasculitis and neuro-metabolic changes (<ce:cross-ref refid="bb0090" id="cf0085">Jonas et al., 2012</ce:cross-ref>). Similar studies have showed consistent results (<ce:cross-refs refid="bb0115 bb0195 bb0225" id="cf0090">Lapointe et al., 2006; Smith et al., 2011; Trimble et al., 2007</ce:cross-refs>). Furthermore, functional MRI studies have demonstrated that the activation pattern of HI is different from that of NH during certain scanning tasks (<ce:cross-refs refid="bb0015 bb0150 bb0160 bb0165 bb0230" id="cf0095">Bilecen et al., 2000; Patel et al., 2007; Propst et al., 2010; Scheffler et al., 1998; Tschopp et al., 2000</ce:cross-refs>). For example, Propst and colleagues studied the activation pattern of HI with narrowband noise and speech-in-noise tasks (<ce:cross-ref refid="bb0160" id="cf0100">Propst et al., 2010</ce:cross-ref>). In the narrowband noise task, they found that HI children had weaker activation in the auditory areas when compared to NH children. Meanwhile, NH also activated auditory association areas and attention networks, which were not detected in HI children. In the speech-in-noise task, HI children activated the secondary auditory processing areas only in the left hemisphere, rather than bilaterally as is typical of NH. Recently, we have tried to use the activation in the primary auditory cortex (A1) to predict CI outcomes. A strong correlation (linear regression coefficient, R<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>0.88) was detected between the improvement in post-CI hearing threshold and the amount of activation in the A1 region before CI (<ce:cross-ref refid="bb0150" id="cf0105">Patel et al., 2007</ce:cross-ref>). Despite these recent advances, it remains unclear whether these structural and functional abnormalities are sufficient to distinguish HI from NH individuals.</ce:para><ce:para id="p0015" view="all">In this study, we set out to investigate whether we can accurately classify HI from NH individuals based on MR images alone by utilizing machine learning techniques. We have trained three classifiers, one based on structural MR (sMRI) images, another based on functional MR (fMRI) images, and a third that integrates sMRI and fMRI images. While traditional methods utilize voxel-based morphometric (VBM) features, in which each single voxel serves as an independent feature, we extracted high-level features to characterize the 3D images. Specifically, we employed the Scale Invariant Feature Transform (SIFT) algorithm to detect and describe local features in sMRI and extracted region-level features to represent the functional contrast maps. Based upon the extracted features, SVM classifiers were trained to separate HI from NH.</ce:para><ce:para id="p0020" view="all">The SIFT algorithm was first proposed by Lowe for object recognition (<ce:cross-ref refid="bb0130" id="cf0110">Lowe, 1999</ce:cross-ref>). Since then, it has been widely used in the computer vision field. Basically, the SIFT algorithm detects blob-like image components and calculates a vector to describe each of these components. Each vector becomes a SIFT feature. The set of SIFT features extracted from an image contains important characteristics of this image and can be used for subsequent analysis, e.g. object recognition, gesture recognition etc. In this study, we employed the SIFT algorithm to extract SIFT features from brain structural MR images, and devised an approach for the automatic classification of NH vs. HI based on the SIFT features.</ce:para><ce:para id="p0025" view="all">There are three levels of significance for this study. First of all, we convincingly demonstrate that hearing loss can be accurately diagnosed based on MR images alone. Secondly, brain regions identified by the classifiers enable us to better understand hearing loss, and may serve as valuable indicators for the CI outcome and facilitate follow-up treatment post-CI (<ce:cross-ref refid="bb0090" id="cf0115">Jonas et al., 2012</ce:cross-ref>). Finally, our algorithm can be easily extended to assist in diagnosing other disorders affecting children's brains, e.g., speech sound disorders of childhood, leading to a path for improving child health.</ce:para><ce:para id="p0030" view="all">The organization of this article is as follows. In <ce:cross-ref refid="s0010" id="cf0120">Materials and methods</ce:cross-ref>, we describe in sequence the data sources and the preprocessing procedures, the methods of analyzing sMRI and fMRI images, the integrative model that combines these two methods, and the validation of our classifiers. In <ce:cross-ref refid="s0015" id="cf0125">Results</ce:cross-ref>, we compare the classification performance of the sMRI classifier, the fMRI classifier and the combined classifier, and assess the stability of feature selection as well as the discriminatory power of features. Finally, in <ce:cross-ref refid="s0020" id="cf0130">Discussion</ce:cross-ref>, we summarize the present work, highlight the significance of our approach, and discuss the limitations and envisioned future improvements. We also examine the predictive brain regions our classifiers identified and discuss their relevance in the context of hearing loss.</ce:para></ce:section><ce:section id="s0010" view="all"><ce:label>2</ce:label><ce:section-title id="st0025">Materials and methods</ce:section-title><ce:section id="s0030" view="all"><ce:label>2.1</ce:label><ce:section-title id="st0055">Data acquisition and preprocessing</ce:section-title><ce:section id="s0035" view="all"><ce:label>2.1.1</ce:label><ce:section-title id="st0060">Participants</ce:section-title><ce:para id="p0045" view="all">Thirty-nine infants and toddlers participated in a clinically indicated MRI brain study under sedation. This study was conducted with approval from the Cincinnati Children's Hospital Medical Center Institutional Review Board (IRB). Eighteen of the participants had SNHL (10 females, average age<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>14<ce:hsp sp="0.25"/>months, range<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>8-24<ce:hsp sp="0.25"/>months). All hearing impaired participants were referred by the Division of Otolaryngology for MRI as part of the cochlear implant staging process and consented to participate in our adjoining fMRI protocol. They had documented bilateral severe to profound hearing loss with average hearing thresholds in the range of 90<ce:hsp sp="0.25"/>dB or greater. Nine of these subjects had no measureable hearing response in either ear at the maximum level of our audiometry equipment, at 120<ce:hsp sp="0.25"/>dB and can be considered deaf. The remaining 21 participants were normal hearing controls (15 females, average age<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>12<ce:hsp sp="0.25"/>months, range<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>8-17<ce:hsp sp="0.25"/>months). These children received clinical MRI scans with sedation for non-hearing related indications. They were recruited for the control group if they met the inclusion criteria: gestational age of at least 36<ce:hsp sp="0.25"/>weeks, normal otoacoustic emissions hearing, and normal neuroanatomy determined by the neuroradiologist. Informed consent of parent or guardian was obtained prior to the study protocol, and the parent agreed to additional hearing tests at a separate visit. The child's reason for referral for brain MRI was not related to hearing. Exclusions included head circumference &lt;<ce:hsp sp="0.10"/>5 percentile or &gt;<ce:hsp sp="0.10"/>95 percentile, orthodontic or metallic implants that interfere with the MRI, abnormal brain pathology in the central auditory pathways. Examples of indications for scanning in this group were, "odd body positioning-rule out chiari malformation", "recent onset irritable behavior-rule out brain tumor". All participants were screened for hearing loss using otoacoustic emission (OAE) prior to the MRI scan. Failed OAE at the time of scan was also an exclusion criterion for the normal control group. All of these brain scans of both hearing impaired group and control group were reviewed by a pediatric neuroradiologist and assessed as having no anatomical findings of significance. One of the challenges of research in pediatric neuroimaging is that it is unethical to expose children to more than minimal risk for the purposes of research. This principle is dictated by our conscience as well as by the IRB at most institutions. Consequently, one of the fine points in the design of the present study is that we were required to select our control population among infants who were referred for an MRI scan with sedation because of a clinical indication. With the precautions described above and other procedures we took to insure normal auditory function and brain anatomy, this is perhaps the best control group that could be obtained for this age group in an ethical fashion. However, it is important to note that the controls were not randomly sampled from the general population.</ce:para></ce:section><ce:section id="s0040" view="all"><ce:label>2.1.2</ce:label><ce:section-title id="st0065">MRI/fMRI acquisition</ce:section-title><ce:para id="p0055" view="all">Anatomical images for this study were acquired using a 3.0<ce:hsp sp="0.25"/>Tesla Siemens Trio MRI scanner in the clinical Department of Radiology. Isotropic images of the brain were acquired using an inversion recovery prepared rapid gradient-echo 3D method (MP-RAGE) covering the entire brain at a spatial resolution of 1<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>1<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>1<ce:hsp sp="0.25"/>mm in an axial orientation. 3D MP-RAGE acquisition parameters were as follows: TI/TR/TE<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>1100/1900/4.1<ce:hsp sp="0.25"/>ms, FOV<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>25.6<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>20.8<ce:hsp sp="0.25"/>cm, matrix<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>256<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>208, scan time<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>3<ce:hsp sp="0.25"/>min and 50<ce:hsp sp="0.25"/>s. These high resolution 3D-T1 weighted images were used for co-registration of fMRI scans which were also acquired during this scheduled MRI.</ce:para><ce:para id="p0060" view="all">Functional MRI scans were performed using a silent background fMRI acquisition technique that allowed auditory stimuli to be presented during a silent gradient interval of the scan, followed by an acquisition interval that captured the peak BOLD response of relevant brain regions (<ce:cross-ref refid="bb0175" id="cf0135">Schmithorst and Holland, 2004</ce:cross-ref>). Using the scanner described above we acquired BOLD fMRI scans in an axial plane (4<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>4<ce:hsp sp="0.25"/>mm resolution), using the manufacturer's standard gradient echo, EPI sequence covering the same FOV as the 3D T1 images (see paragraph above), with the following parameters: TR/TE<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>2000/23<ce:hsp sp="0.25"/>msec, flip angle<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>90Â°, matrix<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>64<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>64 and 25 axial slices with thickness<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>5<ce:hsp sp="0.25"/>mm. In the present study, all stimulus and control intervals were of equal duration (5<ce:hsp sp="0.25"/>s) in a three-phase auditory paradigm consisting of speech, silence, and narrow band noise tones interleaved with acquisition periods of 6<ce:hsp sp="0.25"/>s during which 3 image volumes were obtained covering the whole brain. A timing diagram for the fMRI data acquisition and stimulation paradigm is shown in <ce:cross-ref refid="f0005" id="cf0140">Fig. 1</ce:cross-ref><ce:float-anchor refid="f0005"/>.</ce:para><ce:para id="p0065" view="all">The speech stimulus consisted of sentences read in a female voice. Altogether 36 sentences were read in 18 segments of 5<ce:hsp sp="0.25"/>s duration and comprising 2 sentences each. This condition was followed by a 6<ce:hsp sp="0.25"/>s data acquisition and then a 5<ce:hsp sp="0.25"/>s interval of silence as a control condition. After another 6<ce:hsp sp="0.25"/>s control interval acquisition, a second auditory control condition was played. This condition consisted of Narrow Band Noise (NBN) tones patterned after standard audiology evaluations for detection of hearing thresholds. Five NBN tones of 1<ce:hsp sp="0.25"/>s duration with center frequencies of 250, 500, 1000, 2000 and 4000<ce:hsp sp="0.25"/>Hz and bandwidth of 50% were played in random order during this control condition, for a total of 5<ce:hsp sp="0.25"/>s during a silent interval of the scanner. An additional interval of 1<ce:hsp sp="0.25"/>s of silence followed each acquisition to provide an acoustic demarcation prior to the stimulus onset of each stimulus condition. This resulted in the fMRI acquisition time of approximately 11<ce:hsp sp="0.25"/>min. See <ce:cross-ref refid="f0005" id="cf0145">Fig. 1</ce:cross-ref> for a detailed schematic of the task and timing. Auditory stimuli were administered through calibrated MR compatible headphones at a sound level of 10-15<ce:hsp sp="0.25"/>dB greater than the individual participant's Pure Tone Average (PTA) hearing threshold. Each hearing impaired participant in the study had a recent audiogram, which was used to determine the sound level for fMRI. Our MR compatible audio system was modified to allow for an output through the headphones measuring up to 130<ce:hsp sp="0.25"/>dB.</ce:para></ce:section><ce:section id="s0045" view="all"><ce:label>2.1.3</ce:label><ce:section-title id="st0070">Data analysis - preprocessing</ce:section-title><ce:para id="p0075" view="all">fMRI data were initially analyzed on a voxel-by-voxel basis to identify the activated brain regions using a standard pre-processing pipeline implemented in the Cincinnati Children's Hospital Image Processing Software (CCHIPS) (<ce:cross-ref refid="bb0180" id="cf0150">Schmithorst et al., 2010</ce:cross-ref>) written in IDL computer language. In this paper, we use voxel for 3-dimensional images and pixel for 2-dimensional images.</ce:para><ce:para id="p0080" view="all">Since the subjects were sedated, we assumed that the anatomical image was naturally aligned with the functional images for each individual. Therefore, alignments between anatomical images and functional images were not needed in preprocessing. In this case, it does not matter if we apply the normalization transformation before or after contrast determination. To generate both normalized contrast maps used in the current study as well as contrast maps in native space for other uses, we first generated contrast maps in each individual's native space and then normalized the contrast maps to standard space. The raw EPI images were simultaneously corrected for Nyquist ghosting and geometrical distortion (due to B0 field inhomogeneity) (<ce:cross-ref refid="bb0170" id="cf0155">Schmithorst et al., 2001</ce:cross-ref>). EPI functional MR time-series images were corrected on a voxel-by-voxel basis for drift using a quadratic baseline correction. Motion artifacts were corrected using a pyramid iterative co-registration algorithm (<ce:cross-ref refid="bb0200" id="cf0160">Thevenaz et al., 1998</ce:cross-ref>). During this stage, infant brain images were transformed to the AC-PC plane. Finally, the individual image volumes (1,2,3) in the event-related fMRI acquisition were separated and submitted to a final pre-processing step using the General Linear Model (<ce:cross-ref refid="bb0255" id="cf0165">Worsley et al., 2002</ce:cross-ref>) to construct individual Z-maps for each volume and contrast condition (speech vs. silence, speech vs. tones and tones vs. silence). Z-maps showing activation for each condition for each participant were then computed by averaging the Z-maps from the individual volumes for each contrast condition (<ce:cross-refs refid="bb0150 bb0175" id="cf0170">Patel et al., 2007; Schmithorst and Holland, 2004</ce:cross-refs>). These Z-maps, in each individual's native space were used by the radiologists and neurotologists for clinical interpretation of findings. The neuroradiologist reviewed both functional and anatomical MRI scans for each participant and completed a standardized report indicating whether brain abnormalities or brain activities were detected in primary auditory areas, language areas or other brain regions. After that, we performed spatial normalization using SPM8 with a T1 template constructed from a control group of age matched subjects selected specifically for this infant cohort (<ce:cross-ref refid="bb0005" id="cf0175">Altaye et al., 2008</ce:cross-ref>). The normalized anatomical images and functional Z-maps were then submitted to the next stages of analysis.</ce:para></ce:section></ce:section><ce:section id="s0050" view="all"><ce:label>2.2</ce:label><ce:section-title id="st0075">Feature extraction and model learning based on structural MR images</ce:section-title><ce:para id="p0090" view="all">For sMRI images, we used SIFT features to represent the brain images and developed an algorithm to analyze the SIFT features. We have previously applied this method to Alzheimer's disease, Parkinson's disease and bipolar disease, and it has demonstrated promising classification performance (<ce:cross-ref refid="bb0025" id="cf0180">Chen et al., 2013</ce:cross-ref>).</ce:para><ce:section id="s0055" view="all"><ce:label>2.2.1</ce:label><ce:section-title id="st0080">Obtaining 2D slices from 3D brain images</ce:section-title><ce:para id="p0100" view="all">Due to the high density of SIFT features in the brain images and the pair-wise comparison among SIFT features required in a later step, analyzing the 3D brain image as a whole is computationally infeasible. Thus, the spatially normalized 3D brain (157<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>189<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>136) was divided into 560 20<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>20<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>20 cubes. Since the dimensions of brain image were not divisible by 20, the cubes at the end of dimensions only contained the remaining volume of the brain image and therefore had a size smaller than 20<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>20<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>20. The number 20 was determined based on our experience from the application of this algorithm to several other diseases. The cube size mainly affects the computation speed and accuracy of the likelihood scores as described in the <ce:cross-ref refid="s0065" id="cf0185">Feature evaluation</ce:cross-ref> section below. A larger size leads to a much longer computation time, while a smaller size decreases the accuracy of likelihood scores and subsequently leads to lower classification accuracy. According to our experimental results, the cube size 20<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>20<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>20 provides a good balance between speed and accuracy. Every cube was sliced along three different orientations to obtain 3 sets of 20 2D brain images. We analyzed every cube and every set of 2D brain images individually. The analysis results were combined together in the last step.</ce:para></ce:section><ce:section id="s0060" view="all"><ce:label>2.2.2</ce:label><ce:section-title id="st0085">Extracting SIFT features</ce:section-title><ce:para id="p0110" view="all">The SIFT algorithm for analyzing 2D images was implemented in several stable software packages (<ce:cross-refs refid="bb0125 bb0235" id="cf0455">Lowe; Vedaldi and Fulkerson, 2010</ce:cross-refs>). In this study, we used the SIFT algorithm provided in a publicly available computer vision software package vlFeat (<ce:cross-ref refid="bb0235" id="cf0195">Vedaldi and Fulkerson, 2010</ce:cross-ref>). The SIFT features are described by center locations, scales, orientations and appearance matrices. An example of SIFT features is shown in <ce:cross-ref refid="f0010" id="cf0200">Fig. 2</ce:cross-ref><ce:float-anchor refid="f0010"/>. The SIFT features are shown as circles in <ce:cross-ref refid="f0010" id="cf0205">Fig. 2</ce:cross-ref>(a). Each circle represents a SIFT feature. The center and radius of the circle represent the center location and the scale of the SIFT feature. The existence of a SIFT feature suggests that there is a blob-like image component at the center location of the SIFT feature and the scale of the feature represents the radius of the blob-like component. The image intensity distribution around the blob-like component is further characterized by an orientation and an appearance matrix. The orientation, as shown by the line starting from the center of the circle, represents the general direction of change in image intensity. The appearance matrix represents the detailed change in image intensity. An example of an appearance matrix is shown in <ce:cross-ref refid="f0010" id="cf0210">Fig. 2</ce:cross-ref>(b). The square centered at the center location of a SIFT feature is divided into 16 subsquares. There are 8 lines starting from the center of each subsquare along 8 different directions. The length of a line represents the number of pixels which have a gradient direction the same as the line, and some of the lines may have a length of zero. For example, many of the pixels in the lower left corner subsquare, as shown in <ce:cross-ref refid="f0010" id="cf0215">Fig. 2</ce:cross-ref>(b), have a gradient direction pointing to the lower side of the image; therefore the length of the line starting from the center of this subsquare and pointing to the lower side is long. The center location, scale, direction and appearance matrix of a SIFT feature can be organized as a vector of 133 numbers: the center location includes 3 numbers representing its coordinates in the 3D volume of the brain image; the scale and orientation is represented as one number respectively; the appearance matrix is represented by 128 numbers, 8 numbers for each of the 16 subsquares. This vector form is used in the computation; while the isomorphic graph representation, as shown in <ce:cross-ref refid="f0010" id="cf0220">Fig. 2</ce:cross-ref>, is used as a user friendly way of representing the SIFT features.</ce:para></ce:section><ce:section id="s0065" view="all"><ce:label>2.2.3</ce:label><ce:section-title id="st0090">Feature evaluation</ce:section-title><ce:para id="p0120" view="all">The extracted SIFT features were identified as one of the three feature types, namely patient feature, healthy feature and noise feature. The features were evaluated based on their frequencies of occurrence in patient brains and healthy brains.</ce:para><ce:para id="p0420" view="all">There were two steps to evaluate the features, and each SIFT feature was evaluated separately. The first step was to find all the other features that were similar to the feature that was being analyzed. The similarity between two features was measured by four criteria: the distance between the center locations Î<ce:italic><ce:inf loc="post">x</ce:inf></ce:italic>(<ce:italic>i</ce:italic>, <ce:italic>j</ce:italic>), the scale difference Î<ce:italic><ce:inf loc="post">Ï</ce:inf></ce:italic>(<ce:italic>i</ce:italic>, <ce:italic>j</ce:italic>), the orientation difference Î<ce:italic><ce:inf loc="post">o</ce:inf></ce:italic>(<ce:italic>i</ce:italic>, <ce:italic>j</ce:italic>) and the difference between their appearance matrix Î<ce:italic><ce:inf loc="post">a</ce:inf></ce:italic>(<ce:italic>i</ce:italic>, <ce:italic>j</ce:italic>). They were defined as follows:<ce:display><ce:formula id="fo0005"><ce:label>(1)</ce:label><mml:math altimg="si1.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Î</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mfenced open="(" close=")" separators=","><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mfenced open="â" close="â"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:math></ce:formula></ce:display><ce:display><ce:formula id="fo0010"><ce:label>(2)</ce:label><mml:math altimg="si2.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Î</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mfenced open="(" close=")" separators=","><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="|" close="|"><mml:mrow><mml:mi mathvariant="italic">ln</mml:mi><mml:mfrac><mml:msub><mml:mi>Ï</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>Ï</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:math></ce:formula></ce:display><ce:display><ce:formula id="fo0015"><ce:label>(3)</ce:label><mml:math altimg="si3.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Î</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mfenced open="(" close=")" separators=","><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mo>min</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mfenced open="|" close="|"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>Ï</mml:mi><mml:mo>-</mml:mo><mml:mfenced open="|" close="|"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math></ce:formula></ce:display><ce:display><ce:formula id="fo0020"><ce:label>(4)</ce:label><mml:math altimg="si4.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Î</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mfenced open="(" close=")" separators=","><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mfenced open="â" close="â"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></ce:formula></ce:display>where <ce:italic>x<ce:inf loc="post">i</ce:inf></ce:italic> was the center location of feature <ce:italic>i</ce:italic>, <ce:italic>Ï</ce:italic><ce:inf loc="post"><ce:italic>i</ce:italic></ce:inf> was the scale of feature <ce:italic>i</ce:italic>, <ce:italic>o<ce:inf loc="post">i</ce:inf></ce:italic> was the orientation angle of feature <ce:italic>i</ce:italic> and <ce:italic>a<ce:inf loc="post">i</ce:inf></ce:italic> was the appearance matrix of feature <ce:italic>i</ce:italic>. If all the four differences were less than their corresponding threshold, two features were considered to be similar. All the features that were similar to feature <ce:italic>i</ce:italic> constituted the similar feature set for feature <ce:italic>i</ce:italic>:<ce:display><ce:formula id="fo0025"><ce:label>(5)</ce:label><mml:math altimg="si5.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>Î</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mfenced open="(" close=")" separators=","><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mfenced><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>Ïµ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="true">â§</mml:mo><mml:msub><mml:mi>Î</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mfenced open="(" close=")" separators=","><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mfenced><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>Ïµ</mml:mi><mml:mi>Ï</mml:mi></mml:msub><mml:mo stretchy="true">â§</mml:mo><mml:msub><mml:mi>Î</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mfenced open="(" close=")" separators=","><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mfenced><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>Ïµ</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true">â§</mml:mo><mml:msub><mml:mi>Î</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mfenced open="(" close=")" separators=","><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mfenced><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>Ïµ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mspace width="0.25em"/></mml:mrow></mml:mfenced></mml:mrow></mml:math></ce:formula></ce:display>where <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>x</ce:italic></ce:inf>, <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>Ï</ce:italic></ce:inf>, <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>o</ce:italic></ce:inf> and <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>a</ce:italic></ce:inf> were similarity thresholds for center locations, scales, orientations and appearance matrix, respectively. According to <ce:cross-ref refid="bb0215" id="cf0225">Toews et al. (2010)</ce:cross-ref>, the thresholds <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>x</ce:italic></ce:inf> and <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>Ï</ce:italic></ce:inf> were set to 0.5 and 2/3 respectively. The thresholds <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>o</ce:italic></ce:inf> and <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>a</ce:italic></ce:inf> were set to <ce:italic>Ï</ce:italic>/2 and 0.45 respectively based on a grid search (<ce:cross-ref refid="bb0020" id="cf0230">Chang and Lin, 2011</ce:cross-ref>). Grid search is an efficient way to find the best parameter combinations, when there are multiple parameters in a model and the parameters are continuous variables. First, we discretized the continuous parameters. Parameter <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>o</ce:italic></ce:inf> was discretized into three discrete values [<ce:italic>Ï</ce:italic>/4, 2<ce:italic>Ï</ce:italic>/4, 3<ce:italic>Ï</ce:italic>/4], and parameter <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>a</ce:italic></ce:inf> was discretized into five discrete values [0.3, 0.35, 0.4, 0.45, 0.5]. Then all the combinations of these discrete values, 15 combinations in total, were tried and the parameter combination with the highest classification accuracy was chosen as the best parameter setting.</ce:para><ce:para id="p0425" view="all">The second step for feature evaluation was to assign likelihood scores to the SIFT features. The likelihood score was defined as follows:<ce:display><ce:formula id="fo0030"><ce:label>(6)</ce:label><mml:math altimg="si6.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close=""><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mo>ln</mml:mo><mml:mfrac><mml:mrow><mml:mfenced open="|" close="|"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mstyle displaystyle="true"><mml:mo stretchy="true">â©</mml:mo></mml:mstyle><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:mfenced><mml:mo stretchy="true">/</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mfenced open="|" close="|"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mstyle displaystyle="true"><mml:mo stretchy="true">â©</mml:mo></mml:mstyle><mml:mi mathvariant="normal">C</mml:mi></mml:mrow></mml:mfenced><mml:mo stretchy="true">/</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mfenced open="|" close="|"><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>â¥</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mspace width="0.25em"/><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></ce:formula></ce:display>where <ce:italic>S<ce:inf loc="post">i</ce:inf></ce:italic> was the similar feature set for SIFT feature <ce:italic>i</ce:italic>, P was the patient feature set which included all the SIFT features extracted from all patient brains in the training set, C was the healthy feature set including all the SIFT features from all healthy brains in the training set, <ce:italic>N<ce:inf loc="post">P</ce:inf></ce:italic> and <ce:italic>N<ce:inf loc="post">C</ce:inf></ce:italic> was the number of patient brains and the number of healthy control brains in the training set, respectively.</ce:para><ce:para id="p0430" view="all">A SIFT feature was identified as a patient feature if <ce:italic>L<ce:inf loc="post">i</ce:inf></ce:italic> was larger than a threshold <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>l</ce:italic></ce:inf>; it was a healthy feature if <ce:italic>L<ce:inf loc="post">i</ce:inf></ce:italic> was smaller than -<ce:hsp sp="0.10"/><ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>l</ce:italic></ce:inf>; it was a noise feature otherwise. Formally, the class labels of the features were determined as follows:<ce:display><ce:formula id="fo0035"><ce:label>(7)</ce:label><mml:math altimg="si7.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close=""><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>Ïµ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mfenced open="|" close="|"><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>â¤</mml:mo><mml:msub><mml:mi>Ïµ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mi>Ïµ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></ce:formula></ce:display>where <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>l</ce:italic></ce:inf> was the threshold for likelihood scores. We used grid search to determine the best parameter setting. For the threshold, the value from 0.1 to 1.2 with a step size of 0.1 was searched. After the grid search, <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>l</ce:italic></ce:inf> was set to be 0.9.</ce:para><ce:para id="p0160" view="all">According to the above feature evaluation process, we need to find the similar feature set for every feature (Eq. <ce:cross-ref refid="fo0025" id="cf0235">(5)</ce:cross-ref>), which requires comparing this feature with all other features. For more than 10<ce:sup loc="post">5</ce:sup> features in 39 brains, it would require 10<ce:sup loc="post">10</ce:sup> pair-wise distance calculations, which is a very slow process. Upon those observations, we divided the whole brain volume into small cubes. For the evaluation of a feature, we only calculated its distance to the other features in the same cube. In this way, the computation time is significantly reduced, but the classification accuracy may be adversely affected. For example, a feature close to cube boundaries may have some of its similar features (Eq. <ce:cross-ref refid="fo0025" id="cf0240">(5)</ce:cross-ref>) in adjacent cubes. Ignoring those similar features in adjacent cubes could lead to an inaccurate likelihood score (Eq. <ce:cross-ref refid="fo0030" id="cf0245">(6)</ce:cross-ref>) for this feature. This issue is especially serious when the number of training samples is limited as in our project. On the other hand, a larger cube size would have fewer features close to cube boundaries, and would result in more accurate likelihood scores and hence higher classification accuracy. According to our previous experience from the application of this algorithm to the classification of several other diseases, such as Parkinson's disease, Alzheimer's disease and bipolar disorder, 20<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>20<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>20 was considered to be an appropriate cube size. This cube size 20<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>20<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>20, determined based on adult-sized brains in our previous studies, was used directly for the infant brains in the present study, since our infant brains were normalized using the infant template and the infant template was enlarged to the size very close to that of adult brains (<ce:cross-ref refid="bb0005" id="cf0250">Altaye et al., 2008</ce:cross-ref>).</ce:para></ce:section><ce:section id="s0070" view="all"><ce:label>2.2.4</ce:label><ce:section-title id="st0095">Training SVM classifiers</ce:section-title><ce:para id="p0170" view="all">We trained a linear SVM for every set of 2D slices in every cube to classify the set of SIFT features extracted from this set of 2D slices across subjects into 3 categories. For a new SIFT feature from a brain image whose class-label is unknown, the corresponding SVM is expected to be able to predict the class label of this new SIFT feature without finding its similar feature set in the huge amount of SIFT features extracted from the brain images used for training.</ce:para></ce:section><ce:section id="s0135" view="all"><ce:label>2.2.5</ce:label><ce:section-title id="st0160">Predicting new subjects</ce:section-title><ce:para id="p0435" view="all">To predict a new subject to be NH or HI, the subject's sMRI scan was first normalized to the standard space using SPM8 with the infant T1 template (<ce:cross-ref refid="bb0005" id="cf0255">Altaye et al., 2008</ce:cross-ref>). The normalized brain was divided into cubes and sliced along three orientations as described above. SIFT features were extracted and then classified using the SVM that was trained for the same cube and same slice orientation. After all the SIFT features were classified, we counted the number of features of the three types. The total number of noise features was not used in the final decision process. The new subject was classified according to the following equation:<ce:display><ce:formula id="fo0040"><ce:label>(8)</ce:label><mml:math altimg="si8.gif" overflow="scroll"><mml:mrow><mml:mi>Class</mml:mi><mml:mspace width="0.12em"/><mml:mi>label</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close=""><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mi>HI</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>,</mml:mo><mml:mi mathvariant="italic">if</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mi>C</mml:mi><mml:mi mathvariant="italic">sum</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>Ïµ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mi>NH</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>,</mml:mo><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></ce:formula></ce:display>where <mml:math altimg="si9.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi mathvariant="italic">sum</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo stretchy="true">â</mml:mo><mml:mi>i</mml:mi></mml:munder></mml:mstyle><mml:msub><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math>, <mml:math altimg="si10.gif" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math> is the predicted class label of the <ce:italic>i</ce:italic>-th SIFT feature as shown in Eq. <ce:cross-ref refid="fo0035" id="cf0260">(7)</ce:cross-ref>, <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>s</ce:italic></ce:inf> is a threshold for the final classification of sMRI and its value is determined based on the method described in section <ce:cross-ref refid="s0110" id="cf0265">Validation of the classifier</ce:cross-ref>.</ce:para></ce:section></ce:section><ce:section id="s0075" view="all"><ce:label>2.3</ce:label><ce:section-title id="st0100">Feature extraction and model learning based on functional MR images</ce:section-title><ce:para id="p0195" view="all">For fMRI images, we constructed contrast maps using the General Linear Model (GLM) (<ce:cross-ref refid="bb0255" id="cf0270">Worsley et al., 2002</ce:cross-ref>) as described in the <ce:cross-ref refid="s0030" id="cf0275">Data acquisition and preprocessing</ce:cross-ref> section. Contrast values were estimated from the difference in image intensity for each voxel between two conditions. A positive contrast value indicated that brain activation was higher in the first condition when compared to the second condition, while a negative contrast value suggested a lower activation in the first condition. We generated region-level features and proposed a novel approach to vectorize the contrast maps utilizing the "bag-of-words" strategy (<ce:cross-ref refid="bb0190" id="cf0280">Sivic and Zisserman, 2009</ce:cross-ref>).</ce:para><ce:section id="s0080" view="all"><ce:label>2.3.1</ce:label><ce:section-title id="st0105">Feature generation from contrast maps</ce:section-title><ce:para id="p0205" view="all">Normalized Z-maps were thresholded to select voxels with extreme contrast values for subsequent analysis. Among the selected voxels, we connected the voxels which were adjacent to each other in a 3D neighborhood, in which each voxel had 26 neighbors if it was not on the border. As a result, the selected voxels were merged into a set of disjoint regions, each of which was defined as a region of interest (ROI) (<ce:cross-refs refid="bb0060 bb0155" id="cf0285">Dykstra, 1994; Pokrajac et al., 2005</ce:cross-refs>). To prevent mixing positive voxels and negative voxels in a single ROI, which could negate the signal, we considered these two categories of voxels separately. Positive voxels were ranked decreasingly whereas negative voxels were ranked increasingly according to their activation magnitudes. Only the top 5% of each category were selected. The cutoff of 5% was chosen because it outperformed other cutoffs, 1% and 10%, with respect to the classification performance. In this way, a number of ROIs were delineated to characterize the pattern of a contrast map. Due to individual differences and random noise, however, the set of ROIs delineated from different subjects varied significantly. To address this problem, we delineated a set of ROIs based on each subject, and applied all ROIs derived from all subjects to each single subject to form a long vector for each subject, with each dimension representing the mean contrast value over all voxels within the corresponding ROI. Finally, we concatenated the vectors from the three contrast maps, and obtained a 1474-dimension vector for each subject. In other words, each significantly activated/deactivated region was treated as a word, and all words occurring across all subjects constituted the dictionary. The frequency of each word was measured by the mean contrast value. An intuitive view of the contrast map vectorization process is shown in <ce:cross-ref refid="f0015" id="cf0290">Fig. 3</ce:cross-ref><ce:float-anchor refid="f0015"/>.</ce:para><ce:para id="p0440" view="all">Since we performed ROI detection on each contrast map and then concatenated all the ROIs together, ROIs that were consistent among subjects were detected more than once. To merge those similar ROIs into one single feature, we performed a hierarchical clustering with average linkage (<ce:cross-ref refid="bb0085" id="cf0295">Johnson, 1967</ce:cross-ref>). The original space was represented as:<ce:display><ce:formula id="fo0045"><ce:label>(9)</ce:label><mml:math altimg="si11.gif" overflow="scroll"><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>S</mml:mi><mml:mfenced open="(" close=")" separators=","><mml:mn>1, 1</mml:mn></mml:mfenced></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mo>â¯</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>S</mml:mi><mml:mfenced open="(" close=")" separators=","><mml:mn>1, 1474</mml:mn></mml:mfenced></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mo>â®</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mo>â±</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mo>â®</mml:mo></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>S</mml:mi><mml:mfenced open="(" close=")" separators=","><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:mfenced></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mo>â¯</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>S</mml:mi><mml:mfenced open="(" close=")" separators=","><mml:mi>N</mml:mi><mml:mn>1474</mml:mn></mml:mfenced></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:math></ce:formula></ce:display>where each row represents a training sample and each column represents a ROI, <ce:italic>S</ce:italic><ce:inf loc="post">(<ce:italic>i</ce:italic>,<ce:italic>j</ce:italic>)</ce:inf> is the mean contrast value of ROI <ce:italic>j</ce:italic> for subject <ce:italic>i</ce:italic>, <ce:italic>N</ce:italic> is the total number of subjects. The distance between two ROIs was calculated as the Euclidean distance:<ce:display><ce:formula id="fo0050"><ce:label>(10)</ce:label><mml:math altimg="si12.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">dist</mml:mi><mml:mfenced open="(" close=")" separators=","><mml:msub><mml:mi>ROI</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>ROI</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">â</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mfenced open="(" close=")" separators=","><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:mfenced></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mfenced open="(" close=")" separators=","><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mfenced></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></ce:formula></ce:display></ce:para><ce:para id="p0220" view="all">We cut the hierarchical tree with the inconsistency coefficient of 0.01, and calculated the mean value of the ROIs that were clustered together as the value of the joint feature. The cutoff of 0.01 was easily determined since the cluster results did not change in the cutoff range from 0.01 to 0.7. After hierarchical clustering, the dimensionality was reduced to 969.</ce:para></ce:section><ce:section id="s0085" view="all"><ce:label>2.3.2</ce:label><ce:section-title id="st0110">Sedation method</ce:section-title><ce:para id="p0445" view="all">Subjects were sedated with three different sedation methods during the MRI scanning. Different sedation methods were expected to affect the activation pattern differently (<ce:cross-ref refid="bb0050" id="cf0300">DiFrancesco et al., in press</ce:cross-ref>). Therefore, we added sedation method as an additional feature, which was represented as a 3D binary vector<ce:display><ce:formula id="fo0055"><ce:label>(11)</ce:label><mml:math altimg="si13.gif" overflow="scroll"><mml:mrow><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:math></ce:formula></ce:display></ce:para><ce:para id="p0450" view="all">As shown in the matrix defined in Eq. <ce:cross-ref refid="fo0055" id="cf0305">(11)</ce:cross-ref>, each row of the matrix represented one of the three sedation methods. In this way, we represented each subject as a 972-dimension feature vector, including 969 features from the contrast maps after hierarchical clustering and 3 binary features from sedation method. Therefore, our dataset was represented as <mml:math altimg="si14.gif" overflow="scroll"><mml:mi mathvariant="script">D</mml:mi></mml:math> defined in Eq. <ce:cross-ref refid="fo0060" id="cf0310">(12)</ce:cross-ref>:<ce:display><ce:formula id="fo0060"><ce:label>(12)</ce:label><mml:math altimg="si15.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:mfenced open="(" close=")" separators=","><mml:msup><mml:mi>x</mml:mi><mml:mfenced open="(" close=")"><mml:mn>1</mml:mn></mml:mfenced></mml:msup><mml:msup><mml:mi>y</mml:mi><mml:mfenced open="(" close=")"><mml:mn>1</mml:mn></mml:mfenced></mml:msup></mml:mfenced><mml:mo>,</mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:mfenced open="(" close=")" separators=","><mml:msup><mml:mi>x</mml:mi><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup><mml:msup><mml:mi>y</mml:mi><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup></mml:mfenced><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:mfenced open="(" close=")" separators=","><mml:msup><mml:mi>x</mml:mi><mml:mfenced open="(" close=")"><mml:mn>39</mml:mn></mml:mfenced></mml:msup><mml:msup><mml:mi>y</mml:mi><mml:mfenced open="(" close=")"><mml:mn>39</mml:mn></mml:mfenced></mml:msup></mml:mfenced><mml:mo stretchy="true">|</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msup><mml:mo>â</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mn>972</mml:mn></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:math></ce:formula></ce:display>where <ce:italic>x</ce:italic><ce:sup loc="post">(<ce:italic>i</ce:italic>)</ce:sup> and <ce:italic>y</ce:italic><ce:sup loc="post">(<ce:italic>i</ce:italic>)</ce:sup> was the feature vector and group label (NH or HI) for the <ce:italic>i</ce:italic>-th subject, respectively. This dataset <mml:math altimg="si16.gif" overflow="scroll"><mml:mi mathvariant="script">D</mml:mi></mml:math> was used for subsequent feature selection and model learning.</ce:para></ce:section><ce:section id="s0090" view="all"><ce:label>2.3.3</ce:label><ce:section-title id="st0115">Feature selection and model learning</ce:section-title><ce:para id="p0455" view="all">The WEKA software package was utilized to select a subset of features that were highly correlated with class labels and uncorrelated with each other (<ce:cross-ref refid="bb0075" id="cf0315">Hall, 1999</ce:cross-ref>). The merit of a subset of features was measured as:<ce:display><ce:formula id="fo0065"><ce:label>(13)</ce:label><mml:math altimg="si17.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>k</mml:mi><mml:mover accent="true"><mml:msub><mml:mi>r</mml:mi><mml:mi mathvariant="italic">cf</mml:mi></mml:msub><mml:mo stretchy="true">Â¯</mml:mo></mml:mover></mml:mrow><mml:msqrt><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mover accent="true"><mml:msub><mml:mi>r</mml:mi><mml:mi mathvariant="italic">ff</mml:mi></mml:msub><mml:mo stretchy="true">Â¯</mml:mo></mml:mover></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></ce:formula></ce:display>where <mml:math altimg="si18.gif" overflow="scroll"><mml:mover accent="true"><mml:msub><mml:mi>r</mml:mi><mml:mi mathvariant="italic">cf</mml:mi></mml:msub><mml:mo stretchy="true">Â¯</mml:mo></mml:mover></mml:math> was the mean correlation between class label and selected features, <mml:math altimg="si19.gif" overflow="scroll"><mml:mover accent="true"><mml:msub><mml:mi>r</mml:mi><mml:mi mathvariant="italic">ff</mml:mi></mml:msub><mml:mo stretchy="true">Â¯</mml:mo></mml:mover></mml:math> was the mean correlation between two features, <ce:italic>k</ce:italic> was the number of features in subset <ce:italic>S</ce:italic>. Greedy hill-climbing augmented with a backtracking facility was applied to search through the space of feature subsets (<ce:cross-ref refid="bb0040" id="cf0320">Dechter and Pearl, 1985</ce:cross-ref>). For explanation purposes, we can imagine that there was a rooted tree, which had included all possible feature subsets. In this tree, each node was a feature subset, which was represented as a 972 dimensional binary vector, with 1(0) indicating that the corresponding feature was (not) selected. Each node had 972 successors/children, each of which was generated by flipping one of the 972 dimensions of the current node. Our goal was to step through this tree to find a node with relatively high <ce:italic>M<ce:inf loc="post">s</ce:inf></ce:italic>. In practice, the whole tree would not be constructed because it was unlimited. Only the successors were generated whenever needed. The search started from the root, which was the empty set of features in our project, and repeatedly chose the successor with the highest <ce:italic>M<ce:inf loc="post">s</ce:inf></ce:italic> at each node. The search terminated when 5 consecutive non-improving steps occurred. With the selected subset of features, we trained a linear SVM classifier (<ce:cross-ref refid="bb0020" id="cf0325">Chang and Lin, 2011</ce:cross-ref>).</ce:para></ce:section><ce:section id="s0095" view="all"><ce:label>2.3.4</ce:label><ce:section-title id="st0120">Predicting new subjects</ce:section-title><ce:para id="p0265" view="all">Given a new subject, we first normalized the contrast maps to the infant template space (<ce:cross-ref refid="bb0005" id="cf0330">Altaye et al., 2008</ce:cross-ref>), so that the given contrast maps were registered with the training contrast maps. A 972-D feature vector was then constructed with procedures described above, which was subsequently filtered based on the feature selection results obtained from the training set. Finally, the formatted feature vector was fed to the trained classifier, yielding a decision score (fMRI_score) for the new subject based on the functional MRI data alone. The rule for classification was formulated as:<ce:display><ce:formula id="fo0070"><ce:label>(14)</ce:label><mml:math altimg="si20.gif" overflow="scroll"><mml:mrow><mml:mi>Class</mml:mi><mml:mspace width="0.12em"/><mml:mi>label</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close=""><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mi>HI</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>,</mml:mo><mml:mi>if</mml:mi><mml:mspace width="0.12em"/><mml:mi>fMRI</mml:mi><mml:mo>_</mml:mo><mml:mi>score</mml:mi><mml:mo>â¥</mml:mo><mml:msub><mml:mi>Ïµ</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mi>NH</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>,</mml:mo><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:math></ce:formula></ce:display></ce:para></ce:section><ce:section id="s0100" view="all"><ce:label>2.3.5</ce:label><ce:section-title id="st0125">Important features</ce:section-title><ce:para id="p0460" view="all">The importance of a feature was measured as follows:<ce:display><ce:formula id="fo0075"><ce:label>(15)</ce:label><mml:math altimg="si21.gif" overflow="scroll"><mml:mrow><mml:mi>I</mml:mi><mml:mfenced open="(" close=")"><mml:mi>f</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="|" close="|"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>Ï</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mi mathvariant="italic">if</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></ce:formula></ce:display>where || was the absolute value function, <ce:italic>N</ce:italic> was the total number of folds of cross-validation as described in the following part, <ce:italic>w<ce:inf loc="post">if</ce:inf></ce:italic> was the SVM weight for feature <ce:italic>f</ce:italic> during <ce:italic>i</ce:italic>-th fold of cross-validation, <ce:italic>Ï<ce:inf loc="post">i</ce:inf></ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>1 if the feature <ce:italic>f</ce:italic> was selected in the <ce:italic>i</ce:italic>-th fold of cross-validation. Otherwise, <ce:italic>Ï<ce:inf loc="post">i</ce:inf></ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>0. For the ROIs that were merged into a joint feature through the hierarchical clustering, the importance of such an ROI was equal to the importance of the feature, to which this ROI belonged.</ce:para></ce:section></ce:section><ce:section id="s0105" view="all"><ce:label>2.4</ce:label><ce:section-title id="st0130">Integrated model</ce:section-title><ce:para id="p0290" view="all">To combine the sMRI and fMRI data, we designed a two-layer classification model (<ce:cross-ref refid="f0020" id="cf0335">Fig. 4</ce:cross-ref><ce:float-anchor refid="f0020"/>). Given a training set, we trained two classifiers, namely sMRI classifier and fMRI classifier. Then we applied these two classifiers to the training set. As a result, we obtained two predicted scores for each training sample. Thus, the original feature space was transformed into a new two-dimensional feature space through these two classifiers. Finally, we trained a linear SVM classifier (with parameter C<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>1) in the new feature space to combine the two scores together.</ce:para><ce:para id="p0465" view="all">When predicting new subjects, we first obtained the two predicted scores from the sMRI classifier and fMRI classifier, then fed these two predicted scores into the second layer classifier to yield the final decision score <ce:italic>y</ce:italic>. The decision rule was defined as follows:<ce:display><ce:formula id="fo0080"><ce:label>(16)</ce:label><mml:math altimg="si22.gif" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi mathvariant="italic">sum</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>fMRI</mml:mi><mml:mo>_</mml:mo><mml:mi>score</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi mathvariant="italic">sum</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>â</mml:mo><mml:mi>fMRI</mml:mi><mml:mo>_</mml:mo><mml:mi>score</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">bias</mml:mi></mml:mrow></mml:math></ce:formula></ce:display><ce:display><ce:formula id="fo0085"><ce:label>(17)</ce:label><mml:math altimg="si23.gif" overflow="scroll"><mml:mrow><mml:mi>Class</mml:mi><mml:mspace width="0.12em"/><mml:mi>label</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close=""><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mi>HI</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>,</mml:mo><mml:mi>if</mml:mi><mml:mspace width="0.25em"/><mml:mi>y</mml:mi><mml:mo>â¥</mml:mo><mml:msub><mml:mi>Ïµ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mi>NH</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mo>,</mml:mo><mml:mi>otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></ce:formula></ce:display>where <ce:italic>w</ce:italic><ce:inf loc="post">1</ce:inf>, <ce:italic>w</ce:italic><ce:inf loc="post">2</ce:inf> and <ce:italic>bias</ce:italic> were the parameters in the SVM model, which were learnt from the training.</ce:para></ce:section><ce:section id="s0110" view="all"><ce:label>2.5</ce:label><ce:section-title id="st0135">Validation of the classifier</ce:section-title><ce:para id="p0310" view="all">Leave-one-out cross-validation (LOOCV) was employed to validate the three classifiers as follows. The total number of subjects was denoted as <ce:italic>N</ce:italic>. We performed <ce:italic>N</ce:italic> experiments, each of which was called one fold of cross-validation. In the <ce:italic>n</ce:italic>-th (n<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>1,â¦,N) fold of cross-validation, the <ce:italic>n</ce:italic>-th subject was used for testing; while the others were used for training. Threshold <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>s</ce:italic></ce:inf> was determined so that the false positive rate and false negative rate for the training brains were equal, while <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>f</ce:italic></ce:inf> and <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>i</ce:italic></ce:inf> were set to be 0. These thresholds were applied to the test images to assign them to be either NH or HI. The classification accuracy for all the <ce:italic>N</ce:italic> subjects was reported as accuracy. Equal error rate (EER) accuracies were also determined based purely on the predicted scores of the testing brain images, e.g. the threshold <ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>s</ce:italic></ce:inf>/<ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>f</ce:italic></ce:inf>/<ce:italic>Ïµ</ce:italic><ce:inf loc="post"><ce:italic>i</ce:italic></ce:inf> were chosen so that the false positive rate was equal to false negative rate for the testing brains. In addition, area under curve (AUC) was also calculated to evaluate the performance of classifiers.</ce:para></ce:section></ce:section><ce:section id="s0015" view="all"><ce:label>3</ce:label><ce:section-title id="st0030">Results</ce:section-title><ce:section id="s0115" view="all"><ce:label>3.1</ce:label><ce:section-title id="st0140">Classifier performance</ce:section-title><ce:para id="p0320" view="extended">Performances of the three classifiers are shown in <ce:cross-ref refid="t0005" id="cf0340">Table 1</ce:cross-ref><ce:float-anchor refid="t0005"/>, and receiver operating curves (ROCs) are plotted in <ce:cross-ref refid="f0025" id="cf0345">Fig. 5</ce:cross-ref><ce:float-anchor refid="f0025"/>. While the sMRI classifier and fMRI classifier performed well individually, their combination achieved a significant improvement in performance. The combined classifier yielded AUC and EER as high as 0.90 and 0.89, respectively. From the ROC, we can see that the sMRI classifier could not predict some of the positive subjects (HI) correctly even when the decision threshold was set to be very low, because the classifier did not reach 100% true positive rate even when the false positive rate approached 100%. However, the ROC for fMRI was in an opposite situation. The ROC did not reach 0% false positive rate even when the true positive rate approached 0%, suggesting that the fMRI classifier had difficulty in classifying some of the negative subjects (NH) correctly. As sMRI and fMRI classifiers were vulnerable to different types of errors, it was possible to combine them to overcome their individual limitations. To illustrate the reason why the combination can be successful, we plotted sMRI-fMRI scores in <ce:cross-ref refid="f0030" id="cf0350">Figs. 6</ce:cross-ref><ce:float-anchor refid="f0030"/> and <ce:cross-ref refid="ec0005" id="cf0355">S1</ce:cross-ref>. Simply speaking, the fMRI classifier draws a horizontal line to separate the two groups of subjects based on the fMRI data, while the sMRI classifier draws a vertical line to separate the two groups based on the sMRI data. Obviously, the two groups could not be perfectly separated by either a horizontal or a vertical line in <ce:cross-ref refid="f0030" id="cf0360">Figs. 6</ce:cross-ref> and <ce:cross-ref refid="ec0005" id="cf0365">S1</ce:cross-ref>. However, by combining the fMRI and sMRI classifiers, the two groups of subjects were separable with a diagonal line as shown in the figures.</ce:para><ce:para id="p0322" view="compact-standard">Performances of the three classifiers are shown in <ce:cross-ref refid="t0005" id="cf0342">Table 1</ce:cross-ref>, and receiver operating curves (ROCs) are plotted in <ce:cross-ref refid="f0025" id="cf0347">Fig. 5</ce:cross-ref>. While the sMRI classifier and fMRI classifier performed well individually, their combination achieved a significant improvement in performance. The combined classifier yielded AUC and EER as high as 0.90 and 0.89, respectively. From the ROC, we can see that the sMRI classifier could not predict some of the positive subjects (HI) correctly even when the decision threshold was set to be very low, because the classifier did not reach 100% true positive rate even when the false positive rate approached 100%. However, the ROC for fMRI was in an opposite situation. The ROC did not reach 0% false positive rate even when the true positive rate approached 0%, suggesting that the fMRI classifier had difficulty in classifying some of the negative subjects (NH) correctly. As sMRI and fMRI classifiers were vulnerable to different types of errors, it was possible to combine them to overcome their individual limitations. To illustrate the reason why the combination can be successful, we plotted sMRI-fMRI scores in <ce:cross-ref refid="f0030" id="cf0352">Fig. 6</ce:cross-ref> and S1. Simply speaking, the fMRI classifier draws a horizontal line to separate the two groups of subjects based on the fMRI data, while the sMRI classifier draws a vertical line to separate the two groups based on the sMRI data. Obviously, the two groups could not be perfectly separated by either a horizontal or a vertical line in <ce:cross-ref refid="f0030" id="cf0362">Fig. 6</ce:cross-ref> and S1. However, by combining the fMRI and sMRI classifiers, the two groups of subjects were separable with a diagonal line as shown in the figures.</ce:para></ce:section><ce:section id="s0120" view="all"><ce:label>3.2</ce:label><ce:section-title id="st0145">Feature selection in sMRI analysis</ce:section-title><ce:para id="p0330" view="all">In the analysis of sMRI data, image features were selected based on their likelihood scores. The total number of image features in a brain image ranged from 35,000 to 52,000. Most of these image features were noise features. The total number of selected features, i.e., healthy and patient features, ranged from 300 to 1400 for different brains with a likelihood threshold of 0.9. Different choices of likelihood threshold for the sMRI feature selection resulted in different numbers of selected features and therefore different classification accuracies. <ce:cross-ref refid="t0010" id="cf0370">Table 2</ce:cross-ref><ce:float-anchor refid="t0010"/> shows the relation between classification accuracy and the likelihood threshold. The classification accuracy did not change for likelihood threshold ranging from 0.7 to 1.1. The AUC changed within a range of 0.09 with a peak where the likelihood threshold equaled 0.9. The EER accuracy varied within a range of 0.08. All three classification performance measures were stable with different likelihood thresholds.</ce:para></ce:section><ce:section id="s0125" view="all"><ce:label>3.3</ce:label><ce:section-title id="st0150">Stability of feature selection in fMRI analysis</ce:section-title><ce:para id="p0470" view="all">We have analyzed the stability of feature selection in the analysis of fMRI data. There were in total 972 features as the input for feature selection. Only 6.2% of the features (with a total number of 60) were selected at least once. For each fold of cross-validation, there were usually about 20 features selected for the training, generally 30% of which were consistently present in all folds of cross-validation. We calculated a stability index as follows (<ce:cross-ref refid="bb0095" id="cf0375">Kalousis et al., 2007</ce:cross-ref>):<ce:display><ce:formula id="fo0090"><ce:label>(18)</ce:label><mml:math altimg="si24.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">Sim</mml:mi><mml:mfenced open="(" close=")" separators=","><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mfenced open="|" close="|"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mstyle displaystyle="true"><mml:mo stretchy="true">â©</mml:mo></mml:mstyle><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mfenced open="|" close="|"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mstyle displaystyle="true"><mml:mo stretchy="true">âª</mml:mo></mml:mstyle><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mfrac></mml:mrow></mml:math></ce:formula></ce:display><ce:display><ce:formula id="fo0095"><ce:label>(19)</ce:label><mml:math altimg="si25.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">index</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>c</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>c</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">â</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mi>c</mml:mi></mml:munderover></mml:mstyle><mml:mi mathvariant="italic">Sim</mml:mi><mml:mfenced open="(" close=")" separators=","><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math></ce:formula></ce:display>where <ce:italic>c</ce:italic> was the total number of rounds of feature selection, <ce:italic>s<ce:inf loc="post">i</ce:inf></ce:italic> and <ce:italic>s<ce:inf loc="post">j</ce:inf></ce:italic> were two sets of features selected during two runs, |<ce:italic>s</ce:italic><ce:inf loc="post"><ce:italic>i</ce:italic></ce:inf>â©<ce:italic>s</ce:italic><ce:inf loc="post"><ce:italic>j</ce:italic></ce:inf>| was the cardinality of the intersection between <ce:italic>s<ce:inf loc="post">i</ce:inf></ce:italic> and <ce:italic>s<ce:inf loc="post">j</ce:inf></ce:italic>, and |<ce:italic>s</ce:italic><ce:inf loc="post"><ce:italic>i</ce:italic></ce:inf>âª<ce:italic>s</ce:italic><ce:inf loc="post"><ce:italic>j</ce:italic></ce:inf>| was the cardinality of the union of <ce:italic>s<ce:inf loc="post">i</ce:inf></ce:italic> and <ce:italic>s<ce:inf loc="post">j</ce:inf></ce:italic>. Our feature selection yielded a stability index of 66.2%, which indicated that 66.2% of the selected features, on average, were common between any two runs of feature selection. Since the Euclidean distance was used in the hierarchical clustering, only very similar ROIs were merged. There was still considerable redundancy among features. For example, two ROIs, e.g. one from the contrast speech vs. silence and the other from the contrast tones vs. silence, were significantly correlated with class labels, and meanwhile they were also highly correlated with each other. Due to the large Euclidean distance between them, however, they were not merged during the hierarchical clustering. In feature selection, these two ROIs were treated as different features and selected interchangeably. This caused the calculated stability index to be lower than the actual value. In this regard, 66.2% represented very high stability.</ce:para></ce:section><ce:section id="s0130" view="all"><ce:label>3.4</ce:label><ce:section-title id="st0155">Discriminative brain regions</ce:section-title><ce:para id="p0355" view="all">For sMRI, we measured the importance of a SIFT feature with its likelihood score. In our project, however, the SIFT features usually had a scale of 10<ce:hsp sp="0.25"/>mm or even larger, and correspondingly the side length of the appearance matrices was larger than 40<ce:hsp sp="0.25"/>mm. Due to the large size of the SIFT features, it was more difficult and less useful to interpret the medical implications of such large brain regions.</ce:para><ce:para id="p0360" view="all">With those considerations, we only focused on the highly predictive brain regions identified by the fMRI classifier. <ce:cross-ref refid="f0035" id="cf0380">Fig. 7</ce:cross-ref><ce:float-anchor refid="f0035"/> shows the top 10 functional features extracted from fMRI data that differentiate the HI and NH groups. Features are numbered from A to J in order. ROI A1 and A2 were merged during hierarchical clustering into a joint feature A. Similar procedures were performed for features C, E, F, I and J. We can see that ROIs grouped together during hierarchical clustering are always from the same type of contrast maps (<ce:cross-ref refid="t0015" id="cf0385">Table 3</ce:cross-ref><ce:float-anchor refid="t0015"/>) and encompass adjoining or sometimes overlapping brain regions as designated by Brodmann's Areas in the 4th column of <ce:cross-ref refid="t0015" id="cf0390">Table 3</ce:cross-ref>.</ce:para></ce:section></ce:section><ce:section id="s0020" view="all"><ce:label>4</ce:label><ce:section-title id="st0035">Discussion</ce:section-title><ce:para id="p0365" view="all">In this work, we have built a robust two-layer classifier that can accurately separate HI from NH infants. We realize that hearing in newborns can be accurately tested using the auditory brainstem response (ABR) evaluations or the otoacoustic emission (OAE) measures, it is thus not our intention to develop a tool for computer-aided diagnosis of hearing loss. Rather we provide a proof of principle that it is possible to accurately determine the functional, developmental status of the central auditory system in congenitally hearing impaired children based on MR images alone by utilizing machine learning techniques. Such success has been previously reported in other progressive diseases, such as Alzheimer's disease (<ce:cross-ref refid="bb0030" id="cf0395">Cuingnet et al., 2011</ce:cross-ref>). However, for many progressive diseases, definite diagnosis is often difficult to establish, in which case the LOOCV approach may not be able to estimate the classifier performance accurately. Therefore, our dataset with solid labels corresponding to diagnostic categories of the participants that have NH or HI enables us to make an objective evaluation of our algorithm, and demonstrate conclusively the feasibility of using machine learning in making automated diagnoses or prognoses based on imaging examinations. The approach described here may not be limited to a specific disease; essentially, any disease dataset with sMRI and fMRI brain images can be analyzed with our method provided that sufficient training data is available.</ce:para><ce:para id="p0370" view="all">A major innovation that makes highly accurate predictions possible in our approach is that we extracted high-level features instead of using each single voxel as a feature as in traditional approaches. The SIFT features from sMRI images and region-level features from fMRI images are much less sensitive to registration errors when compared to voxel-features. In addition, utilization of high-level features can considerably reduce the dimensionality of feature space, which not only makes our classification problem easier to handle, but also helps to reduce the problem of over-fitting. At last, our classification model is more interpretable, because our model involves fewer features consisting of continuous regions instead of scattered voxels. These features can then be related more easily to disease etiology, diagnosis and prognosis.</ce:para><ce:para id="p0375" view="all">Another innovation of our approach is that we employed a bag-of-words strategy to analyze the functional contrast maps. This technique can characterize the activation pattern for every individual in spite of the great variability in the activation pattern among individuals. Considering the relatively small sample size, we constructed our feature pool with all available samples, including the one used for testing during the cross-validation. We implemented a variant version of our algorithm, in which we extracted ROIs based only on the training samples, and subsequently applied those ROIs to the testing sample directly. As expected, the variant algorithm performed slightly worse (AUC<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>0.81) than our original algorithm (AUC<ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>0.83). Adding the ROIs from new samples requires us to retrain the classifier every time when new samples are available. As the feature pool becomes larger in the future, the retraining is not necessary.</ce:para><ce:para id="p0380" view="all">Integration of different types of data, e.g. data from multiple modalities, has been demonstrated to be more powerful for classification (<ce:cross-refs refid="bb0065 bb0070 bb0220 bb0240" id="cf0400">Fan et al., 2007, 2008; Tosun et al., 2010; Wang et al., 2012</ce:cross-refs>). However, how to implement such integrations in the best way remains to be explored (<ce:cross-ref refid="bb0145" id="cf0405">Orru et al., 2012</ce:cross-ref>). Traditionally, features from different types of data are concatenated and a single classifier is trained (<ce:cross-refs refid="bb0065 bb0070 bb0220 bb0240" id="cf0410">Fan et al., 2007, 2008; Tosun et al., 2010; Wang et al., 2012</ce:cross-refs>). Specifically, the traditional integration method requires the training set to be organized into matrices, with each row representing a training sample and each column representing a feature. One matrix is constructed for one type of data, and subsequently all the matrices are concatenated into one big matrix, which serves as the input for classifier training. In our project, the fMRI data can be easily organized in this way. For sMRI, however, each training sample has a set of SIFT features, which can be treated as a set of words included in an article. Different articles have different sets of words. Thus, it is not easy to organize the sMRI data into a matrix as described above, and the traditional integration method is not applicable. Under such circumstances, we proposed a two-layer model to integrate the sMRI and fMRI data. Since the traditional approach was not applicable in our project, we did not compare their performances in the present paper. Additionally, our two-layer model is also applicable when features from different modalities can be concatenated. In this case, one classifier is trained for one modality, and a second-layer classifier is subsequently used to integrate the multiple classifiers on the first-layer. This approach is able to combine as many types of data as possible, without worrying about the high dimensionality or overfitting.</ce:para><ce:para id="p0385" view="all">Although computer-aided diagnosis of hearing loss is not needed, our algorithm can potentially advance the study of congenital hearing loss mechanism by identifying discriminative brain regions as disease biomarkers for hearing impairment at various levels in the auditory system. Inspecting the most important features that differentiate children born with hearing impairment from children with normal hearing in this study, we see some features that are in line with hypotheses about under stimulation of auditory function in HI infants; while other observations already begin to add to our knowledge of how congenital deafness affects brain development and function. For example, features B, F, H, and I include known components of the auditory language network which our group and others have previously shown to be engaged by the narrative comprehension task (<ce:cross-refs refid="bb0100 bb0185" id="cf0415">Karunanayaka et al., 2007; Schmithorst et al., 2006</ce:cross-refs>). These features include (B) the planum temporale and primary auditory cortex in the left hemisphere (including Wernicke's area, the classical language recognition module), as well as the angular gyrus and supramarginal gyrus at the temporal parietal junction of the (F) left and (H, I) right hemispheres, known auditory and visual language association regions. Although all participants were bilaterally severely to profoundly hearing impaired, we observe left dominant auditory/language related activity present in components A, B, and F. In addition, components H and I contain right hemisphere auditory/language activity. Functional features such as these are not unexpected in terms of regions of differential cortical activation between HI and NH children listening to natural language as an auditory stimulus and it is reassuring to see these regions highlighted by our algorithm as potential biomarkers corresponding to hearing impairment. Similarly, there is evidence of differential activation in subcortical features corresponding to the auditory brainstem pathways. Features A, D, and J include elements of the reticular auditory pathway of the brainstem which has been identified by electrophysiological studies to have a key role in auditory perception of location of sounds as well as the ability to filter a source of sound in background noise. Roughly these features appear to encompass key elements of the auditory pathway at the level of the pons (D) including the cochlear nucleus, trapezoid body, lateral lemniscus and superior olive on the right, (A) inferior colliculus, medial geniculate on the left and (J) thalamus bilaterally (<ce:cross-ref refid="bb0110" id="cf0420">Kretschmann and Weinrich, 1998</ce:cross-ref>). Although the resolution of the fMRI scans (4<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>4<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>5<ce:hsp sp="0.25"/>mm) is not sufficient to resolve these structures individually, differences in activation in these regions, as indicated by reference to the higher resolution anatomical images, suggest that brain stem auditory nuclei may be involved.</ce:para><ce:para id="p0390" view="all">One feature that is conspicuously absent from those illustrated in <ce:cross-ref refid="f0035" id="cf0425">Fig. 7</ce:cross-ref> is the primary auditory cortex (BA41). We expected that this region would be important in differentiating HI from NH participants and hoped that it could potentially become a biomarker for predicting outcome for hearing and language following cochlear implantation in HI infants as suggested by our earlier work (<ce:cross-ref refid="bb0150" id="cf0430">Patel et al., 2007</ce:cross-ref>). The sedation used in the present study is a likely confounding to primary auditory function and may be partly responsible for the absence of a functional MRI feature in primary auditory cortex that differentiates the groups (<ce:cross-ref refid="bb0050" id="cf0435">DiFrancesco et al., in press</ce:cross-ref>). However, because <ce:cross-ref refid="f0035" id="cf0440">Fig. 7</ce:cross-ref> highlights differences between the groups that optimally separate them, it is possible that brain regions beyond primary auditory cortex that are responsible for recognizing sounds as speech and for extracting and associating content are more differentially stimulated in a scenario where the hearing impaired brain receives a rare auditory input that is above the threshold it can detect. Vibrations, loud noise and other stimuli may occasionally stimulate the auditory cortex in a deaf infant so that it is capable of processing sound and responds during our experiment in the same manner as the NH children who are receiving sound stimulation at the same relative SPL. However, unless the HI infant is participating in a successful hearing aid trial, it is much less likely that they are routinely subjected to an auditory stream of speech that is consistently above their hearing threshold and hence unintelligible. HI infants in this study were all severe to profoundly hearing-impaired and ultimately received a cochlear implant because they did not derive sufficient benefit from an external hearing aid. Though this explanation is speculative, it could explain why features B, C, E, F, G, H, and I seem to be more important in separating the HI and NH groups of infants based on brain activation during fMRI.</ce:para><ce:para id="p0395" view="all">On the other hand, our analysis on the fMRI data in this study also identified a number of areas that are not necessarily expected to play a role in differentiating HI from NH children. In particular, several functional features also appear in various portions of the anterior cingulate cortex (ACC, BA 24,32,33): areas associated with attention management, conflict monitoring, and error detection (<ce:cross-ref refid="bb0245" id="cf0445">Weissman et al., 2005</ce:cross-ref>). These features may be related to responses in the HI group to the novel auditory stimulus. ACC features are present in all three contrasts (C2, E1, E2, and G), suggesting a difference in response to sound input in the HI group who do not typically receive an auditory input at a level above their auditory threshold. Important features are also present in secondary visual cortex (H) (BA18), associative visual cortex (BA19) and other subcortical regions; differentiating the two groups. These features provide clues about additional neuroimaging biomarkers that may be relevant to the future use of functional neuroimaging to guide predictions about speech and language outcomes in HI infants who receive a cochlear implant. This type of prognostic information, currently not available, is obviously of great significance. For example, it helps to calibrate the expectations and avoid subsequent disappointment, save money of the family and avoid anesthetic risks when it is clear that a child will derive no benefit from the procedure.</ce:para><ce:para id="p0400" view="all">In the present study, all infants were sedated for a clinical MRI scan and the fMRI task was appended to the end of the protocol. Further, there were different agents used for the sedation in the population we sampled, including propofol, Nembutal and sevoflurane. These drugs may have a different influence on the BOLD signal we detected. Note that the influence of sedation is to attenuate the auditory and language related brain activity and corresponding BOLD signal relative to what would be detected in awake or even sleeping babies (<ce:cross-refs refid="bb0055 bb0045 bb0050 bb0250" id="cf0450">Difrancesco et al., 2011; DiFrancesco et al., 2013, in press; Wilke et al., 2003</ce:cross-refs>). Therefore, the current approach for automatic classification of NH vs. HI would likely be more effective in a scenario where fMRI data could be recorded from the participants without the influence of sedation. Demonstrating that our approach can accurately classify infants by hearing status even under the confounding influence of sedation encourages optimism for other applications where confounding disease-related conditions may modify the BOLD signal, such as cerebrovascular diseases.</ce:para><ce:para id="p0405" view="all">In the future, we will try image segmentation algorithms to define ROIs instead of thresholding the contrast maps. Other evidence, such as tissue density maps and functional connectivity networks, may be integrated into our model. For example, we can train a classifier based on the tissue density maps and then integrate it into our model with the second-layer classifier. Beyond the MRI data, our model will also permit integration from electrophysiologic imaging modalities such as evoked response potentials (ERP), electroencephalography (EEG), or magnetoencephalography (MEG). These brain scanning techniques directly record brain activities; however they are limited in their spatial resolution by the algorithms that are used to localize sources of brain activity based on recordings at the surface of the skull. Combining MR imaging features with electrophysiologic features recorded directly from brain responses to auditory input could leverage the benefits of each imaging modality to produce much more accurate predictions about patient outcomes. Due to the inherent properties of our two-layer model, integration of other evidences can be easily implemented. With the improved classifier, the method is likely to have applications to many other diseases.</ce:para></ce:section><ce:section id="s0025" view="all"><ce:label>5</ce:label><ce:section-title id="st0040">Conclusion</ce:section-title><ce:para id="p0410" view="all">First, our study demonstrates that HI and NH infants can be differentiated by brain MR images, e.g. different fMRI contrasts in auditory language network and auditory brain stem nuclei. Based upon the discriminative features, a classification model can be built to predict whether an individual has normal hearing or impaired hearing. The discriminative features may also be used as objective biomarkers of hearing loss or used for further disease mechanism studies. Secondly, our two-layer model integrates sMRI and fMRI in an effective way. While our sMRI classifier and fMRI classifier work moderately well individually, the combination of the two classifiers gives birth to a much more powerful classifier, which corroborates the hypothesis that integration of multiple modalities improves classification accuracy. Besides, our integration approach is very flexible, and it can be easily extended to include many diverse types of data. Future work with this machine learning approach to automated image classification may allow us to make predictions about speech and language outcomes in individual children who receive cochlear implants for remediation of congenital hearing impairment.</ce:para><ce:para view="extended" id="p0475">The following are the supplementary data related to this article<ce:display><ce:e-component id="ec0005"><ce:label>Fig. S1</ce:label><ce:caption id="ca0055"><ce:simple-para id="sp0065" view="all">Distribution of sMRI-fMRI scores for all 39 folds of cross validation. Each panel is one-fold of cross-validation. Horizontal axis is the output of the sMRI classifier and vertical axis is the output of the fMRI classifier. Blue dots are HI training samples, red dots are NH training samples, the black star is the testing sample. The true label of the testing sample is HI for fold1 to fold18, and NH for fold19 to fold39.</ce:simple-para></ce:caption><ce:link locator="mmc1"/></ce:e-component></ce:display></ce:para><ce:para view="compact-standard" id="p0505">Supplementary data to this article can be found online at <ce:inter-ref xlink:href="doi:10.1016/j.nicl.2013.09.008" id="ir0015" xlink:type="simple">http://dx.doi.org/10.1016/j.nicl.2013.09.008</ce:inter-ref>.</ce:para></ce:section></ce:sections><ce:acknowledgment id="ac0005" view="all"><ce:section-title id="st0045">Acknowledgment</ce:section-title><ce:para id="p0415" view="all">LT designed and developed the fMRI classifier. YC designed and developed the sMRI classifier. LT and YC developed the integration of the sMRI and fMRI classifiers. TCM preprocessed the sMRI and fMRI data. MMC reviewed the anatomical and functional MRI images. LJL and SKH conceived the project idea and supervised the project. LT, YC, SKH and LJL are involved in writing and preparing the manuscript. The project is funded by the <ce:grant-sponsor id="gts0005" xlink:type="simple" xlink:role="http://www.elsevier.com/xml/linking-roles/grant-sponsor">CCTST Methodology</ce:grant-sponsor> grant as part of an Institutional Clinical and Translational Science Award (<ce:grant-number refid="gts0005">NIH/NCRR 8UL1TR000077-04</ce:grant-number>) and <ce:grant-number refid="gts0005">NIH R01-DC07186</ce:grant-number>.</ce:para></ce:acknowledgment></body><tail view="all"><ce:bibliography id="bi0005" view="all"><ce:section-title id="st0050">References</ce:section-title><ce:bibliography-sec id="bs0005" view="all"><ce:bib-reference id="bb0005"><ce:label>Altaye et al., 2008</ce:label><sb:reference id="rf0005"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Altaye</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Wilke</ce:surname></sb:author><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Gaser</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Infant brain probability templates for MRI segmentation and normalization</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Neuroimage</sb:maintitle></sb:title><sb:volume-nr>43</sb:volume-nr></sb:series><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>721</sb:first-page><sb:last-page>730</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0010"><ce:label>Bachmann and Arvedson, 1998</ce:label><sb:reference id="rf0010"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.R.</ce:given-name><ce:surname>Bachmann</ce:surname></sb:author><sb:author><ce:given-name>J.C.</ce:given-name><ce:surname>Arvedson</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Early identification and intervention for children who are hearing impaired</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Pediatr. Rev.</sb:maintitle></sb:title><sb:volume-nr>19</sb:volume-nr></sb:series><sb:date>1998</sb:date></sb:issue><sb:pages><sb:first-page>155</sb:first-page><sb:last-page>165</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0015"><ce:label>Bilecen et al., 2000</ce:label><sb:reference id="rf0015"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Bilecen</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Seifritz</ce:surname></sb:author><sb:author><ce:given-name>E.W.</ce:given-name><ce:surname>Radu</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Schmid</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Wetzel</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Probst</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Scheffler</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Cortical reorganization after acute unilateral hearing loss traced by fMRI</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Neurology</sb:maintitle></sb:title><sb:volume-nr>54</sb:volume-nr></sb:series><sb:date>2000</sb:date></sb:issue><sb:pages><sb:first-page>765</sb:first-page><sb:last-page>767</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0020"><ce:label>Chang and Lin, 2011</ce:label><sb:reference id="rf0205"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>C.-C.</ce:given-name><ce:surname>Chang</ce:surname></sb:author><sb:author><ce:given-name>C.-J.</ce:given-name><ce:surname>Lin</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>LIBSVM: a library for support vector machines</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>ACM Trans. Intell. Syst. Technol.</sb:maintitle></sb:title><sb:volume-nr>27</sb:volume-nr></sb:series><sb:date>2011</sb:date></sb:issue><sb:pages><sb:first-page>21</sb:first-page><sb:last-page>27</sb:last-page></sb:pages></sb:host><sb:comment>(27)</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bb0025"><ce:label>Chen et al., 2013</ce:label><sb:reference id="rf0210"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Chen</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Storrs</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Tan</ce:surname></sb:author><sb:author><ce:given-name>L.J.</ce:given-name><ce:surname>Mazlack</ce:surname></sb:author><sb:author><ce:given-name>J.H.</ce:given-name><ce:surname>Lee</ce:surname></sb:author><sb:author><ce:given-name>J.L.</ce:given-name><ce:surname>Lu</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Detecting brain structural changes as biomarker from Magnetic Resonance images using a local feature based SVM approach</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J. NeuroSci. Methods</sb:maintitle></sb:title><sb:volume-nr>221</sb:volume-nr></sb:series><sb:date>2013</sb:date></sb:issue><sb:pages><sb:first-page>22</sb:first-page><sb:last-page>31</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0030"><ce:label>Cuingnet et al., 2011</ce:label><sb:reference id="rf0020"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Cuingnet</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Gerardin</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Tessieras</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Auzias</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Lehericy</ce:surname></sb:author><sb:author><ce:given-name>M.O.</ce:given-name><ce:surname>Habert</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Chupin</ce:surname></sb:author><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Benali</ce:surname></sb:author><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Colliot</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Automatic classification of patients with Alzheimer's disease from structural MRI: a comparison of ten methods using the ADNI database</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Neuroimage</sb:maintitle></sb:title><sb:volume-nr>56</sb:volume-nr></sb:series><sb:date>2011</sb:date></sb:issue><sb:pages><sb:first-page>766</sb:first-page><sb:last-page>781</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0035"><ce:label>Cunningham and Cox, 2003</ce:label><sb:reference id="rf0025"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Cunningham</ce:surname></sb:author><sb:author><ce:given-name>E.O.</ce:given-name><ce:surname>Cox</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Hearing assessment in infants and children: recommendations beyond neonatal screening</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Pediatrics</sb:maintitle></sb:title><sb:volume-nr>111</sb:volume-nr></sb:series><sb:date>2003</sb:date></sb:issue><sb:pages><sb:first-page>436</sb:first-page><sb:last-page>440</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0040"><ce:label>Dechter and Pearl, 1985</ce:label><sb:reference id="rf0030"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Dechter</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Pearl</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Generalized best-first search strategies and the optimality of A*</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J. ACM</sb:maintitle></sb:title><sb:volume-nr>32</sb:volume-nr></sb:series><sb:date>1985</sb:date></sb:issue><sb:pages><sb:first-page>505</sb:first-page><sb:last-page>536</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0055"><ce:label>Difrancesco et al., 2011</ce:label><sb:reference id="rf0215"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.W.</ce:given-name><ce:surname>Difrancesco</ce:surname></sb:author><sb:author><ce:given-name>B.J.</ce:given-name><ce:surname>Zappia</ce:surname></sb:author><sb:author><ce:given-name>S.A.</ce:given-name><ce:surname>Robertson</ce:surname></sb:author><sb:author><ce:given-name>D.I.</ce:given-name><ce:surname>Choo</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Distinct BOLD activation and connectivity profiles in infants sedated with nembutal and propofol under language stimulation</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>17th Annual Meeting of the Organization for Human Brain Mapping (OHBM) Quebec, Canada</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0045"><ce:label>DiFrancesco et al., 2013</ce:label><sb:reference id="rf0220"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>DiFrancesco</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Robertson</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Karunananayaka</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>BOLD fMRI in infants under sedation: comparing the impact of pentobarbital and propofol on auditory and language activation and connectivity</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>ISMRM 21st Scientific Meeting Salt Lake City, UT</sb:maintitle></sb:title><sb:date>2013</sb:date></sb:edited-book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0050"><ce:label>DiFrancesco et al., in press</ce:label><sb:reference id="rf0225"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.W.</ce:given-name><ce:surname>DiFrancesco</ce:surname></sb:author><sb:author><ce:given-name>S.A.</ce:given-name><ce:surname>Robertson</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Karunanayaka</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>BOLD fMRI in infants under sedation: comparing the impact of pentobarbital and propofol on auditory and language activation</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J. Magn. Reson. Imaging</sb:maintitle></sb:title></sb:series><sb:date>2013</sb:date></sb:issue></sb:host><sb:comment>(in press)</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bb0060"><ce:label>Dykstra, 1994</ce:label><sb:reference id="rf0230"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>C.J.</ce:given-name><ce:surname>Dykstra</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>3D contiguous volume analysis for functional imaging</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>PhD Thesis Canada: Simon Fraser University</sb:maintitle></sb:title><sb:date>1994</sb:date></sb:edited-book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0065"><ce:label>Fan et al., 2007</ce:label><sb:reference id="rf0035"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Fan</ce:surname></sb:author><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Rao</ce:surname></sb:author><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Hurt</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Giannetta</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Korczykowski</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Shera</ce:surname></sb:author><sb:author><ce:given-name>B.B.</ce:given-name><ce:surname>Avants</ce:surname></sb:author><sb:author><ce:given-name>J.C.</ce:given-name><ce:surname>Gee</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Wang</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Shen</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Multivariate examination of brain abnormality using both structural and functional MRI</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Neuroimage</sb:maintitle></sb:title><sb:volume-nr>36</sb:volume-nr></sb:series><sb:date>2007</sb:date></sb:issue><sb:pages><sb:first-page>1189</sb:first-page><sb:last-page>1199</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0070"><ce:label>Fan et al., 2008</ce:label><sb:reference id="rf0040"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Fan</ce:surname></sb:author><sb:author><ce:given-name>S.M.</ce:given-name><ce:surname>Resnick</ce:surname></sb:author><sb:author><ce:given-name>X.</ce:given-name><ce:surname>Wu</ce:surname></sb:author><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Davatzikos</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Structural and functional biomarkers of prodromal Alzheimer's disease: a high-dimensional pattern classification study</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Neuroimage</sb:maintitle></sb:title><sb:volume-nr>41</sb:volume-nr></sb:series><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>277</sb:first-page><sb:last-page>285</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0075"><ce:label>Hall, 1999</ce:label><sb:reference id="rf0235"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Hall</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Correlation-based Feature Subset Selection for Machine Learning</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1999</sb:date></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0080"><ce:label>Horowitz-Kraus and Holland, 2012</ce:label><sb:reference id="rf0240"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Horowitz-Kraus</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Imaging executive functions in typically and atypically developed children</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:editors><sb:editor><ce:given-name>J.</ce:given-name><ce:surname>Griffen</ce:surname></sb:editor><sb:editor><ce:given-name>L.</ce:given-name><ce:surname>Freund</ce:surname></sb:editor></sb:editors><sb:title><sb:maintitle>Executive Function in Preschool Age Children: Integrating Measurement, Neurodevelopment and Translational Research</sb:maintitle></sb:title><sb:date>2012</sb:date><sb:publisher><sb:name>American Psychological Association Press</sb:name><sb:location>Washington, DC</sb:location></sb:publisher></sb:edited-book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0085"><ce:label>Johnson, 1967</ce:label><sb:reference id="rf0050"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.C.</ce:given-name><ce:surname>Johnson</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Hierarchical clustering schemes</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Psychometrika</sb:maintitle></sb:title><sb:volume-nr>2</sb:volume-nr></sb:series><sb:date>1967</sb:date></sb:issue><sb:pages><sb:first-page>241</sb:first-page><sb:last-page>254</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0090"><ce:label>Jonas et al., 2012</ce:label><sb:reference id="rf0055"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>N.E.</ce:given-name><ce:surname>Jonas</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Ahmed</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Grainger</ce:surname></sb:author><sb:author><ce:given-name>C.G.</ce:given-name><ce:surname>Jephson</ce:surname></sb:author><sb:author><ce:given-name>M.E.</ce:given-name><ce:surname>Wyatt</ce:surname></sb:author><sb:author><ce:given-name>B.E.</ce:given-name><ce:surname>Hartley</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Saunders</ce:surname></sb:author><sb:author><ce:given-name>L.A.</ce:given-name><ce:surname>Cochrane</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>MRI brain abnormalities in cochlear implant candidates: how common and how important are they?</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Int. J. Pediatr. Otorhinolaryngol.</sb:maintitle></sb:title><sb:volume-nr>76</sb:volume-nr></sb:series><sb:date>2012</sb:date></sb:issue><sb:pages><sb:first-page>927</sb:first-page><sb:last-page>929</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0095"><ce:label>Kalousis et al., 2007</ce:label><sb:reference id="rf0060"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Kalousis</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Prados</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Hilario</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Stability of feature selection algorithms: a study on high-dimensional spaces</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Knowl. Inf. Syst.</sb:maintitle></sb:title><sb:volume-nr>12</sb:volume-nr></sb:series><sb:date>2007</sb:date></sb:issue><sb:pages><sb:first-page>95</sb:first-page><sb:last-page>116</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0100"><ce:label>Karunanayaka et al., 2007</ce:label><sb:reference id="rf0065"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>P.R.</ce:given-name><ce:surname>Karunanayaka</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author><sb:author><ce:given-name>V.J.</ce:given-name><ce:surname>Schmithorst</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Solodkin</ce:surname></sb:author><sb:author><ce:given-name>E.E.</ce:given-name><ce:surname>Chen</ce:surname></sb:author><sb:author><ce:given-name>J.P.</ce:given-name><ce:surname>Szaflarski</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Plante</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Age-related connectivity changes in fMRI data from children listening to stories</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Neuroimage</sb:maintitle></sb:title><sb:volume-nr>34</sb:volume-nr></sb:series><sb:date>2007</sb:date></sb:issue><sb:pages><sb:first-page>349</sb:first-page><sb:last-page>360</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0105"><ce:label>Kemper and Downs, 2000</ce:label><sb:reference id="rf0070"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.R.</ce:given-name><ce:surname>Kemper</ce:surname></sb:author><sb:author><ce:given-name>S.M.</ce:given-name><ce:surname>Downs</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A cost-effectiveness analysis of newborn hearing screening strategies</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Arch. Pediatr. Adolesc. Med.</sb:maintitle></sb:title><sb:volume-nr>154</sb:volume-nr></sb:series><sb:date>2000</sb:date></sb:issue><sb:pages><sb:first-page>484</sb:first-page><sb:last-page>488</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0110"><ce:label>Kretschmann and Weinrich, 1998</ce:label><sb:reference id="rf0245"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>H.J.</ce:given-name><ce:surname>Kretschmann</ce:surname></sb:author><sb:author><ce:given-name>W.</ce:given-name><ce:surname>Weinrich</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Neurofunctional Systems: 3D Reconstructions With Correlated Neuroimaging</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1998</sb:date><sb:publisher><sb:name>Thieme</sb:name></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0115"><ce:label>Lapointe et al., 2006</ce:label><sb:reference id="rf0075"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Lapointe</ce:surname></sb:author><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Viamonte</ce:surname></sb:author><sb:author><ce:given-name>M.C.</ce:given-name><ce:surname>Morriss</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Manolidis</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Central nervous system findings by magnetic resonance in children with profound sensorineural hearing loss</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Int. J. Pediatr. Otorhinolaryngol.</sb:maintitle></sb:title><sb:volume-nr>70</sb:volume-nr></sb:series><sb:date>2006</sb:date></sb:issue><sb:pages><sb:first-page>863</sb:first-page><sb:last-page>868</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0120"><ce:label>Leach and Holland, 2010</ce:label><sb:reference id="rf0080"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.L.</ce:given-name><ce:surname>Leach</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Functional MRI in children: clinical and research applications</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Pediatr. Radiol.</sb:maintitle></sb:title><sb:volume-nr>40</sb:volume-nr></sb:series><sb:date>2010</sb:date></sb:issue><sb:pages><sb:first-page>31</sb:first-page><sb:last-page>49</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0130"><ce:label>Lowe, 1999</ce:label><sb:reference id="rf0250"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Lowe</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Object recognition from local-invariant features</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:book-series><sb:series><sb:title><sb:maintitle>Proceedings of the International Conference on Computer Vision</sb:maintitle></sb:title><sb:volume-nr>vol. 2</sb:volume-nr></sb:series></sb:book-series><sb:date>1999</sb:date></sb:edited-book><sb:pages><sb:first-page>1150</sb:first-page><sb:last-page>1157</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0125"><ce:label>Lowe</ce:label><ce:other-ref id="or0045"><ce:textref id="tr0005">Lowe, D., Demo Software: SIFT Keypoint Detector.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bb0135"><ce:label>Niparko et al., 2010</ce:label><sb:reference id="rf0090"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.K.</ce:given-name><ce:surname>Niparko</ce:surname></sb:author><sb:author><ce:given-name>E.A.</ce:given-name><ce:surname>Tobey</ce:surname></sb:author><sb:author><ce:given-name>D.J.</ce:given-name><ce:surname>Thal</ce:surname></sb:author><sb:author><ce:given-name>L.S.</ce:given-name><ce:surname>Eisenberg</ce:surname></sb:author><sb:author><ce:given-name>N.Y.</ce:given-name><ce:surname>Wang</ce:surname></sb:author><sb:author><ce:given-name>A.L.</ce:given-name><ce:surname>Quittner</ce:surname></sb:author><sb:author><ce:given-name>N.E.</ce:given-name><ce:surname>Fink</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Spoken language development in children following cochlear implantation</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>JAMA</sb:maintitle></sb:title><sb:volume-nr>303</sb:volume-nr></sb:series><sb:date>2010</sb:date></sb:issue><sb:pages><sb:first-page>1498</sb:first-page><sb:last-page>1506</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0140"><ce:label>Northern, 1994</ce:label><sb:reference id="rf0255"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.L.</ce:given-name><ce:surname>Northern</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Universal screening for infant hearing impairment</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Pediatrics</sb:maintitle></sb:title><sb:volume-nr>94</sb:volume-nr></sb:series><sb:date>1994</sb:date></sb:issue><sb:pages><sb:first-page>955</sb:first-page></sb:pages></sb:host><sb:comment>(author reply 959-963)</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bb0145"><ce:label>Orru et al., 2012</ce:label><sb:reference id="rf0095"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Orru</ce:surname></sb:author><sb:author><ce:given-name>W.</ce:given-name><ce:surname>Pettersson-Yeo</ce:surname></sb:author><sb:author><ce:given-name>A.F.</ce:given-name><ce:surname>Marquand</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Sartori</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Mechelli</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Using support vector machine to identify imaging biomarkers of neurological and psychiatric disease: a critical review</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Neurosci. Biobehav. Rev.</sb:maintitle></sb:title><sb:volume-nr>36</sb:volume-nr></sb:series><sb:date>2012</sb:date></sb:issue><sb:pages><sb:first-page>1140</sb:first-page><sb:last-page>1152</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0150"><ce:label>Patel et al., 2007</ce:label><sb:reference id="rf0100"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.M.</ce:given-name><ce:surname>Patel</ce:surname></sb:author><sb:author><ce:given-name>L.D.</ce:given-name><ce:surname>Cahill</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Ret</ce:surname></sb:author><sb:author><ce:given-name>V.</ce:given-name><ce:surname>Schmithorst</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Choo</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Holland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Functional magnetic resonance imaging of hearing-impaired children under sedation before cochlear implantation</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Arch. Otolaryngol. Head Neck Surg.</sb:maintitle></sb:title><sb:volume-nr>133</sb:volume-nr></sb:series><sb:date>2007</sb:date></sb:issue><sb:pages><sb:first-page>677</sb:first-page><sb:last-page>683</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0155"><ce:label>Pokrajac et al., 2005</ce:label><sb:reference id="rf0105"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Pokrajac</ce:surname></sb:author><sb:author><ce:given-name>V.</ce:given-name><ce:surname>Megalooikonomou</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Lazarevic</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Kontos</ce:surname></sb:author><sb:author><ce:given-name>Z.</ce:given-name><ce:surname>Obradovic</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Applying spatial distribution analysis techniques to classification of 3D medical images</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Artif. Intell. Med.</sb:maintitle></sb:title><sb:volume-nr>33</sb:volume-nr></sb:series><sb:date>2005</sb:date></sb:issue><sb:pages><sb:first-page>261</sb:first-page><sb:last-page>280</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0160"><ce:label>Propst et al., 2010</ce:label><sb:reference id="rf0110"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>E.J.</ce:given-name><ce:surname>Propst</ce:surname></sb:author><sb:author><ce:given-name>J.H.</ce:given-name><ce:surname>Greinwald</ce:surname></sb:author><sb:author><ce:given-name>V.</ce:given-name><ce:surname>Schmithorst</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Neuroanatomic differences in children with unilateral sensorineural hearing loss detected using functional magnetic resonance imaging</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Arch. Otolaryngol. Head Neck Surg.</sb:maintitle></sb:title><sb:volume-nr>136</sb:volume-nr></sb:series><sb:date>2010</sb:date></sb:issue><sb:pages><sb:first-page>22</sb:first-page><sb:last-page>26</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0165"><ce:label>Scheffler et al., 1998</ce:label><sb:reference id="rf0115"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Scheffler</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Bilecen</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Schmid</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Tschopp</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Seelig</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Auditory cortical responses in hearing subjects and unilateral deaf patients as detected by functional magnetic resonance imaging</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Cereb. Cortex</sb:maintitle></sb:title><sb:volume-nr>8</sb:volume-nr></sb:series><sb:date>1998</sb:date></sb:issue><sb:pages><sb:first-page>156</sb:first-page><sb:last-page>163</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0175"><ce:label>Schmithorst and Holland, 2004</ce:label><sb:reference id="rf0125"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>V.J.</ce:given-name><ce:surname>Schmithorst</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Event-related fMRI technique for auditory processing with hemodynamics unrelated to acoustic gradient noise</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Magn. Reson. Med.</sb:maintitle></sb:title><sb:volume-nr>51</sb:volume-nr></sb:series><sb:date>2004</sb:date></sb:issue><sb:pages><sb:first-page>399</sb:first-page><sb:last-page>402</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0170"><ce:label>Schmithorst et al., 2001</ce:label><sb:reference id="rf0120"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>V.J.</ce:given-name><ce:surname>Schmithorst</ce:surname></sb:author><sb:author><ce:given-name>B.J.</ce:given-name><ce:surname>Dardzinski</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Simultaneous correction of ghost and geometric distortion artifacts in EPI using a multiecho reference scan</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Med. Imaging</sb:maintitle></sb:title><sb:volume-nr>20</sb:volume-nr></sb:series><sb:date>2001</sb:date></sb:issue><sb:pages><sb:first-page>535</sb:first-page><sb:last-page>539</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0185"><ce:label>Schmithorst et al., 2006</ce:label><sb:reference id="rf0130"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>V.J.</ce:given-name><ce:surname>Schmithorst</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Plante</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Cognitive modules utilized for narrative comprehension in children: a functional magnetic resonance imaging study</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Neuroimage</sb:maintitle></sb:title><sb:volume-nr>29</sb:volume-nr></sb:series><sb:date>2006</sb:date></sb:issue><sb:pages><sb:first-page>254</sb:first-page><sb:last-page>266</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0180"><ce:label>Schmithorst et al., 2010</ce:label><sb:reference id="rf0260"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>V.J.</ce:given-name><ce:surname>Schmithorst</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author><sb:author><ce:given-name>B.J.</ce:given-name><ce:surname>Dardzinsk</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>CCHIPS: Cincinnati Children's Hospital Image Processing Software</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>2010</sb:date></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0190"><ce:label>Sivic and Zisserman, 2009</ce:label><sb:reference id="rf0135"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Sivic</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Zisserman</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Efficient visual search of videos cast as text retrieval</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Pattern Anal. Mach. Intell.</sb:maintitle></sb:title><sb:volume-nr>31</sb:volume-nr></sb:series><sb:date>2009</sb:date></sb:issue><sb:pages><sb:first-page>591</sb:first-page><sb:last-page>605</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0195"><ce:label>Smith et al., 2011</ce:label><sb:reference id="rf0140"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.M.</ce:given-name><ce:surname>Smith</ce:surname></sb:author><sb:author><ce:given-name>M.D.</ce:given-name><ce:surname>Mecoli</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Altaye</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Komlos</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Maitra</ce:surname></sb:author><sb:author><ce:given-name>K.P.</ce:given-name><ce:surname>Eaton</ce:surname></sb:author><sb:author><ce:given-name>J.C.</ce:given-name><ce:surname>Egelhoff</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Morphometric differences in the Heschl's gyrus of hearing impaired and normal hearing infants</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Cereb. Cortex</sb:maintitle></sb:title><sb:volume-nr>21</sb:volume-nr></sb:series><sb:date>2011</sb:date></sb:issue><sb:pages><sb:first-page>991</sb:first-page><sb:last-page>998</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0200"><ce:label>Thevenaz et al., 1998</ce:label><sb:reference id="rf0145"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Thevenaz</ce:surname></sb:author><sb:author><ce:given-name>U.E.</ce:given-name><ce:surname>Ruttimann</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Unser</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A pyramid approach to subpixel registration based on intensity</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Image Process.</sb:maintitle></sb:title><sb:volume-nr>7</sb:volume-nr></sb:series><sb:date>1998</sb:date></sb:issue><sb:pages><sb:first-page>27</sb:first-page><sb:last-page>41</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0205"><ce:label>Tillema et al., 2008</ce:label><sb:reference id="rf0150"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.M.</ce:given-name><ce:surname>Tillema</ce:surname></sb:author><sb:author><ce:given-name>A.W.</ce:given-name><ce:surname>Byars</ce:surname></sb:author><sb:author><ce:given-name>L.M.</ce:given-name><ce:surname>Jacola</ce:surname></sb:author><sb:author><ce:given-name>M.B.</ce:given-name><ce:surname>Schapiro</ce:surname></sb:author><sb:author><ce:given-name>V.J.</ce:given-name><ce:surname>Schmithorst</ce:surname></sb:author><sb:author><ce:given-name>J.P.</ce:given-name><ce:surname>Szaflarski</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Cortical reorganization of language functioning following perinatal left MCA stroke</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Brain Lang.</sb:maintitle></sb:title><sb:volume-nr>105</sb:volume-nr></sb:series><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>99</sb:first-page><sb:last-page>111</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0210"><ce:label>Tlustos et al., 2011</ce:label><sb:reference id="rf0155"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.J.</ce:given-name><ce:surname>Tlustos</ce:surname></sb:author><sb:author><ce:given-name>C.Y.</ce:given-name><ce:surname>Chiu</ce:surname></sb:author><sb:author><ce:given-name>N.C.</ce:given-name><ce:surname>Walz</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Bernard</ce:surname></sb:author><sb:author><ce:given-name>S.L.</ce:given-name><ce:surname>Wade</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Neural correlates of interference control in adolescents with traumatic brain injury: functional magnetic resonance imaging study of the counting stroop task</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J. Int. Neuropsychol. Soc.</sb:maintitle></sb:title><sb:volume-nr>17</sb:volume-nr></sb:series><sb:date>2011</sb:date></sb:issue><sb:pages><sb:first-page>181</sb:first-page><sb:last-page>189</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0215"><ce:label>Toews et al., 2010</ce:label><sb:reference id="rf0160"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Toews</ce:surname></sb:author><sb:author><ce:given-name>W.</ce:given-name><ce:surname>Wells</ce:surname><ce:suffix>III</ce:suffix></sb:author><sb:author><ce:given-name>D.L.</ce:given-name><ce:surname>Collins</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Arbel</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Feature-based morphometry: discovering group-related anatomical patterns</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Neuroimage</sb:maintitle></sb:title><sb:volume-nr>49</sb:volume-nr></sb:series><sb:date>2010</sb:date></sb:issue><sb:pages><sb:first-page>2318</sb:first-page><sb:last-page>2327</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0220"><ce:label>Tosun et al., 2010</ce:label><sb:reference id="rf0165"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Tosun</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Mojabi</ce:surname></sb:author><sb:author><ce:given-name>M.W.</ce:given-name><ce:surname>Weiner</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Schuff</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Joint analysis of structural and perfusion MRI for cognitive assessment and classification of Alzheimer's disease and normal aging</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Neuroimage</sb:maintitle></sb:title><sb:volume-nr>52</sb:volume-nr></sb:series><sb:date>2010</sb:date></sb:issue><sb:pages><sb:first-page>186</sb:first-page><sb:last-page>197</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0225"><ce:label>Trimble et al., 2007</ce:label><sb:reference id="rf0170"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Trimble</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Blaser</ce:surname></sb:author><sb:author><ce:given-name>A.L.</ce:given-name><ce:surname>James</ce:surname></sb:author><sb:author><ce:given-name>B.C.</ce:given-name><ce:surname>Papsin</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Computed tomography and/or magnetic resonance imaging before pediatric cochlear implantation? Developing an investigative strategy</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Otol. Neurotol.</sb:maintitle></sb:title><sb:volume-nr>28</sb:volume-nr></sb:series><sb:date>2007</sb:date></sb:issue><sb:pages><sb:first-page>317</sb:first-page><sb:last-page>324</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0230"><ce:label>Tschopp et al., 2000</ce:label><sb:reference id="rf0175"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Tschopp</ce:surname></sb:author><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Schillinger</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Schmid</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Rausch</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Bilecen</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Scheffler</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Detection of central auditory compensation in unilateral deafness with functional magnetic resonance tomography</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Laryngorhinootologie</sb:maintitle></sb:title><sb:volume-nr>79</sb:volume-nr></sb:series><sb:date>2000</sb:date></sb:issue><sb:pages><sb:first-page>753</sb:first-page><sb:last-page>757</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0235"><ce:label>Vedaldi and Fulkerson, 2010</ce:label><sb:reference id="rf0265"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Vedaldi</ce:surname></sb:author><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Fulkerson</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Vlfeat: an open and portable library of computer vision algorithms</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the International Conference on Multimedia</sb:maintitle></sb:title><sb:date>2010</sb:date></sb:edited-book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0240"><ce:label>Wang et al., 2012</ce:label><sb:reference id="rf0185"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Wang</ce:surname></sb:author><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Shen</ce:surname></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Tang</ce:surname></sb:author><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Zang</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Hu</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Combined structural and resting-state functional MRI analysis of sexual dimorphism in the young adult human brain: an MVPA approach</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Neuroimage</sb:maintitle></sb:title><sb:volume-nr>61</sb:volume-nr></sb:series><sb:date>2012</sb:date></sb:issue><sb:pages><sb:first-page>931</sb:first-page><sb:last-page>940</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0245"><ce:label>Weissman et al., 2005</ce:label><sb:reference id="rf0190"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.H.</ce:given-name><ce:surname>Weissman</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Gopalakrishnan</ce:surname></sb:author><sb:author><ce:given-name>C.J.</ce:given-name><ce:surname>Hazlett</ce:surname></sb:author><sb:author><ce:given-name>M.G.</ce:given-name><ce:surname>Woldorff</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Dorsal anterior cingulate cortex resolves conflict from distracting stimuli by boosting attention toward relevant events</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Cereb. Cortex</sb:maintitle></sb:title><sb:volume-nr>15</sb:volume-nr></sb:series><sb:date>2005</sb:date></sb:issue><sb:pages><sb:first-page>229</sb:first-page><sb:last-page>237</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0250"><ce:label>Wilke et al., 2003</ce:label><sb:reference id="rf0195"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Wilke</ce:surname></sb:author><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Holland</ce:surname></sb:author><sb:author><ce:given-name>W.S.J.</ce:given-name><ce:surname>Ball</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Language processing during natural sleep in a 6-year-old boy, as assessed with functional MR imaging</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>AJNR Am. J. Neuroradiol.</sb:maintitle></sb:title><sb:volume-nr>24</sb:volume-nr></sb:series><sb:date>2003</sb:date></sb:issue><sb:pages><sb:first-page>42</sb:first-page><sb:last-page>44</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bb0255"><ce:label>Worsley et al., 2002</ce:label><sb:reference id="rf0200"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.J.</ce:given-name><ce:surname>Worsley</ce:surname></sb:author><sb:author><ce:given-name>C.H.</ce:given-name><ce:surname>Liao</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Aston</ce:surname></sb:author><sb:author><ce:given-name>V.</ce:given-name><ce:surname>Petre</ce:surname></sb:author><sb:author><ce:given-name>G.H.</ce:given-name><ce:surname>Duncan</ce:surname></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Morales</ce:surname></sb:author><sb:author><ce:given-name>A.C.</ce:given-name><ce:surname>Evans</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A general statistical analysis for fMRI data</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Neuroimage</sb:maintitle></sb:title><sb:volume-nr>15</sb:volume-nr></sb:series><sb:date>2002</sb:date></sb:issue><sb:pages><sb:first-page>1</sb:first-page><sb:last-page>15</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference></ce:bibliography-sec></ce:bibliography></tail></article></xocs:serial-item></xocs:doc>
