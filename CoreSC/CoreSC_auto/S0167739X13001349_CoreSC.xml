<?xml version="1.0" ?><PAPER><mode2 hasDoc="yes" name="S0167739X13001349.tmf1" version="elsevier"/>
<TITLE>A memory access model for highly-threaded many-core architectures
</TITLE>
<ABSTRACT>

Abstract
<s sid="1"><CoreSc1 advantage="None" conceptID="Res1" novelty="None" type="Res"/><text>A number of highly-threaded, many-core architectures hide memory-access latency by low-overhead context switching among a large number of threads.</text></s>
<s sid="2"><CoreSc1 advantage="None" conceptID="Res2" novelty="None" type="Res"/><text>The speedup of a program on these machines depends on how well the latency is hidden.</text></s>
<s sid="3"><CoreSc1 advantage="None" conceptID="Res3" novelty="None" type="Res"/><text>If the number of threads were infinite, theoretically, these machines could provide the performance predicted by the PRAM analysis of these programs.</text></s>
<s sid="4"><CoreSc1 advantage="None" conceptID="Res4" novelty="None" type="Res"/><text>However, the number of threads per processor is not infinite, and is constrained by both hardware and algorithmic limits.</text></s>
<s sid="5"><CoreSc1 advantage="None" conceptID="Obj1" novelty="None" type="Obj"/><text>In this paper, we introduce the Threaded Many-core Memory (TMM) model which is meant to capture the important characteristics of these highly-threaded, many-core machines.</text></s>
<s sid="6"><CoreSc1 advantage="None" conceptID="Obj2" novelty="None" type="Obj"/><text>Since we model some important machine parameters of these machines, we expect analysis under this model to provide a more fine-grained and accurate performance prediction than the PRAM analysis.</text></s>
<s sid="7"><CoreSc1 advantage="None" conceptID="Obj3" novelty="None" type="Obj"/><text>We analyze 4 algorithms for the classic all pairs shortest paths problem under this model.</text></s>
<s sid="8"><CoreSc1 advantage="None" conceptID="Res5" novelty="None" type="Res"/><text>We find that even when two algorithms have the same PRAM performance, our model predicts different performance for some settings of machine parameters.</text></s>
<s sid="9"><CoreSc1 advantage="None" conceptID="Con1" novelty="None" type="Con"/><text>For example, for dense graphs, the dynamic programming algorithm and Johnson's algorithm have the same performance in the PRAM model.</text></s>
<s sid="10"><CoreSc1 advantage="None" conceptID="Con2" novelty="None" type="Con"/><text>However, our model predicts different performance for large enough memory-access latency and validates the intuition that the dynamic programming algorithm performs better on these machines.</text></s>
<s sid="11"><CoreSc1 advantage="None" conceptID="Obj4" novelty="None" type="Obj"/><text>We validate several predictions made by our model using empirical measurements on an instantiation of a highly-threaded, many-core machine, namely the NVIDIA GTX 480.</text></s>
Highlights
<s sid="12"><CoreSc1 advantage="None" conceptID="Obj5" novelty="None" type="Obj"/><text>• We design a memory model to analyze algorithms for highly-threaded many-core systems.</text></s>
<s sid="13"><CoreSc1 advantage="None" conceptID="Obj6" novelty="None" type="Obj"/><text>• The model captures significant factors of performance: work, span, and memory accesses.</text></s>
<s sid="14"><CoreSc1 advantage="None" conceptID="Obj7" novelty="None" type="Obj"/><text>• We show the model is better than PRAM by applying both to 4 shortest paths algorithms.</text></s>
<s sid="15"><CoreSc1 advantage="None" conceptID="Obj8" novelty="None" type="Obj"/><text>• Empirical performance is effectively predicted by our model in many circumstances.</text></s>
<s sid="16"><CoreSc1 advantage="None" conceptID="Obj9" novelty="None" type="Obj"/><text>• It is the first formalized asymptotic model helpful for algorithm design on many-cores.</text></s>
</ABSTRACT>
<BODY>

Introduction
<s sid="17"><CoreSc1 advantage="None" conceptID="Met1" novelty="None" type="Met"/><text>Highly-threaded, many-core devices such as GPUs have gained popularity in the last decade; both NVIDIA and AMD manufacture general purpose GPUs that fall in this category.</text></s>
<s sid="18"><CoreSc1 advantage="None" conceptID="Met2" novelty="None" type="Met"/><text>The important distinction between these machines and traditional multi-core machines is that these devices provide a large number of low-overhead hardware threads with low-overhead context switching between them; this fast context-switch mechanism is used to hide the memory access latency of transferring data from slow large (and often global) memory to fast, small (and typically local) memory.</text></s>
<s sid="19"><CoreSc1 advantage="None" conceptID="Met3" novelty="None" type="Met"/><text>Researchers have designed algorithms to solve many interesting problems for these devices, such as GPU sorting or hashing [1-4], linear algebra [5-7], dynamic programming [8,9], graph algorithms [10-13], and many other classic algorithms [14,15].</text></s>
<s sid="20"><CoreSc1 advantage="None" conceptID="Bac1" novelty="None" type="Bac"/><text>These projects generally report impressive gains in performance.</text></s>
<s sid="21"><CoreSc1 advantage="None" conceptID="Bac2" novelty="None" type="Bac"/><text>These devices appear to be here to stay.</text></s>
<s sid="22"><CoreSc1 advantage="None" conceptID="Goa1" novelty="None" type="Goa"/><text>While there is a lot of folk wisdom on how to design good algorithms for these highly-threaded machines, in addition to a significant body of work on performance analysis [16-20], there are no systematic theoretical models to analyze the performance of programs on these machines.</text></s>
<s sid="23"><CoreSc1 advantage="None" conceptID="Goa2" novelty="None" type="Goa"/><text>We are interested in analyzing and characterizing performance of algorithms on these highly-threaded, many-core machines in a more abstract, algorithmic, and systematic manner.</text></s>
<s sid="24"><CoreSc1 advantage="None" conceptID="Met4" novelty="None" type="Met"/><text>Theoretical analysis relies upon models that represent underlying assumptions; if a model does not capture the important aspects of target machines and programs, then the analysis is not predictive of real performance.</text></s>
<s sid="25"><CoreSc1 advantage="None" conceptID="Met5" novelty="None" type="Met"/><text>Over the years, computer scientists have designed various models to capture important aspects of the machines that we use.</text></s>
<s sid="26"><CoreSc1 advantage="None" conceptID="Met6" novelty="None" type="Met"/><text>The most fundamental model that is used to analyze sequential algorithms is the Random Access Machine (RAM) model [21], which we teach undergraduates in their first algorithms class.</text></s>
<s sid="27"><CoreSc1 advantage="None" conceptID="Met7" novelty="None" type="Met"/><text>This model assumes that all operations, including memory accesses, take unit time.</text></s>
<s sid="28"><CoreSc1 advantage="None" conceptID="Mot1" novelty="None" type="Mot"/><text>While this model is a good predictor of performance on computationally intensive programs, it does not properly capture the important characteristics of the memory hierarchy of modern machines.</text></s>
<s sid="29"><CoreSc1 advantage="None" conceptID="Res6" novelty="None" type="Res"/><text>Aggarwal and Vitter proposed the Disk Access Machine (DAM) model [22] which counts the number of memory transfers from slow to fast memory instead of simply counting the number of memory accesses by the program.</text></s>
<s sid="30"><CoreSc1 advantage="None" conceptID="Con3" novelty="None" type="Con"/><text>Therefore, it better captures the fact that modern machines have memory hierarchies and exploiting spatial and temporal locality on these machines can lead to better performance.</text></s>
<s sid="31"><CoreSc1 advantage="None" conceptID="Obj10" novelty="None" type="Obj"/><text>There are also a number of other models that consider the memory access costs of sequential algorithms in different ways [23-29].</text></s>
<s sid="32"><CoreSc1 advantage="None" conceptID="Obj11" novelty="None" type="Obj"/><text>For parallel computing, the analogue for the RAM model is the Parallel Random Access Machine (PRAM) model [30], and there is a large body of work describing and analyzing algorithms in the PRAM model [31,32].</text></s>
<s sid="33"><CoreSc1 advantage="None" conceptID="Met8" novelty="None" type="Met"/><text>In the PRAM model, the algorithm's complexity is analyzed in terms of its work-the time taken by the algorithm on 1 processor, and span (also called depth and critical-path length)-the time taken by the algorithm on an infinite number of processors.</text></s>
<s sid="34"><CoreSc1 advantage="None" conceptID="Met9" novelty="None" type="Met"/><text>Given a machine with P processors, a PRAM algorithm with work W and span S completes in max(W/P,S) time.</text></s>
<s sid="35"><CoreSc1 advantage="None" conceptID="Met10" novelty="None" type="Met"/><text>The PRAM model also ignores the vagaries of the memory hierarchy and assumes that each memory access by the algorithm takes unit time.</text></s>
<s sid="36"><CoreSc1 advantage="None" conceptID="Met11" novelty="None" type="Met"/><text>For modern machines, however, this assumption seldom holds.</text></s>
<s sid="37"><CoreSc1 advantage="None" conceptID="Met12" novelty="None" type="Met"/><text>Therefore, researchers have designed various models that capture memory hierarchies for various types of machines such as distributed memory machines [33-35], shared memory machines and multi-cores [36-40], or the combination of the two [41,42].</text></s>
<s sid="38"><CoreSc1 advantage="None" conceptID="Met13" novelty="None" type="Met"/><text>All of these models capture particular capabilities and properties of the respective target machines, namely shared memory machines or distributed memory machines.</text></s>
<s sid="39"><CoreSc1 advantage="None" conceptID="Met14" novelty="None" type="Met"/><text>While superficially highly-threaded, many-core machines such as GPUs are shared memory machines, their characteristics are very different from traditional multi-core or multiprocessor shared memory machines.</text></s>
<s sid="40"><CoreSc1 advantage="None" conceptID="Met15" novelty="None" type="Met"/><text>The most important distinction between the multi-cores and highly-threaded, many-core machines is the number of threads per core.</text></s>
<s sid="41"><CoreSc1 advantage="None" conceptID="Met16" novelty="None" type="Met"/><text>On multi-core machines, context switch cost is high, and most models nominally assume that only one (or a small constant number of) thread(s) are running on each machine and this thread blocks when there is a memory access.</text></s>
<s sid="42"><CoreSc1 advantage="None" conceptID="Mot2" novelty="None" type="Mot"/><text>Therefore, many models consider the number of memory transfers from slow memory to fast memory as a performance measure, and algorithms are designed to minimize these, since memory transfers take a significant amount of time.</text></s>
<s sid="43"><CoreSc1 advantage="None" conceptID="Bac3" novelty="None" type="Bac"/><text>In contrast, highly-threaded, many-core machines are explicitly designed to have a large number of threads per core and a fast context switching mechanism.</text></s>
<s sid="44"><CoreSc1 advantage="None" conceptID="Bac4" novelty="None" type="Bac"/><text>Highly-threaded many-cores are explicitly designed to hide memory latency; if a thread stalls on a memory operation, some other thread can be scheduled in its place.</text></s>
<s sid="45"><CoreSc1 advantage="None" conceptID="Bac5" novelty="None" type="Bac"/><text>In principle, the number of memory transfers does not matter as long as there are enough threads to hide their latency.</text></s>
<s sid="46"><CoreSc1 advantage="None" conceptID="Goa3" novelty="None" type="Goa"/><text>Therefore, if there are enough threads, we should, in principle, be able to use PRAM algorithms on such machines, since we can ignore the effect of memory transfers which is exactly what PRAM model does.</text></s>
<s sid="47"><CoreSc1 advantage="None" conceptID="Bac6" novelty="None" type="Bac"/><text>However, the number of threads required to reach the point where one gets PRAM performance depends on both the algorithm and the hardware.</text></s>
<s sid="48"><CoreSc1 advantage="None" conceptID="Mot3" novelty="None" type="Mot"/><text>Since no highly-threaded, many-core machine allows an infinite number of threads, it is important to understand both (1) how many threads does a particular algorithm need to achieve PRAM performance, and (2) how does an algorithm perform when it has fewer threads than required to get PRAM performance?</text></s>
<s sid="49"><CoreSc1 advantage="None" conceptID="Goa4" novelty="None" type="Goa"/><text>In this paper, we attempt to characterize these properties of algorithms.</text></s>
<s sid="50"><CoreSc1 advantage="None" conceptID="Goa5" novelty="None" type="Goa"/><text>To motivate this enterprise and to understand the importance of high thread counts on highly-threaded, many-core machines, let us consider a simple application that performs Bloom filter set membership tests on an input stream of biosequence data on GPUs [3].</text></s>
<s sid="51"><CoreSc1 advantage="None" conceptID="Obj12" novelty="None" type="Obj"/><text>The problem is embarrassingly parallel, each set membership test is independent of every other membership test.</text></s>
<s sid="52"><CoreSc1 advantage="None" conceptID="Obs1" novelty="None" type="Obs"/><text>Fig. 1 shows the performance of this application, varying the number of threads per processor core, for two distinct GPUs.</text></s>
<s sid="53"><CoreSc1 advantage="None" conceptID="Obs2" novelty="None" type="Obs"/><text>For both machines, the pattern is quite similar, at low thread counts, the performance increases (roughly linearly) with the number of threads, up until a transition region, after which the performance no longer increases with increasing thread count.</text></s>
<s sid="54"><CoreSc1 advantage="None" conceptID="Obs3" novelty="None" type="Obs"/><text>While the location of the transition region is different for distinct GPU models, this general pattern is found in many applications.</text></s>
<s sid="55"><CoreSc1 advantage="None" conceptID="Obs4" novelty="None" type="Obs"/><text>Once sufficient threads are present, the PRAM model adequately describes the performance of the application and increasing the number of threads no longer helps.</text></s>
<s sid="56"><CoreSc1 advantage="None" conceptID="Obj13" novelty="None" type="Obj"/><text>In this work, we propose the Threaded Many-core Memory (TMM) model that captures the performance characteristics of these highly-threaded, many-core machines.</text></s>
<s sid="57"><CoreSc1 advantage="None" conceptID="Obj14" novelty="None" type="Obj"/><text>This model explicitly models the large number of threads per processor and the memory latency to slow memory.</text></s>
<s sid="58"><CoreSc1 advantage="None" conceptID="Obj15" novelty="None" type="Obj"/><text>Note that while we motivate this model for highly-threaded many-core machines with synchronous computations, in principle, it can be used in any system which has fast context switching and enough threads to hide memory latency.</text></s>
<s sid="59"><CoreSc1 advantage="None" conceptID="Obj16" novelty="None" type="Obj"/><text>Typical examples of such machines include both NVIDIA and AMD/ATI GPUs and the YarcData uRiKA system.</text></s>
<s sid="60"><CoreSc1 advantage="None" conceptID="Met17" novelty="None" type="Met"/><text>We do not try to model the Intel Xeon Phi, due to its limited use of threading for latency hiding.</text></s>
<s sid="61"><CoreSc1 advantage="None" conceptID="Met18" novelty="None" type="Met"/><text>In contrast, its approach to hide memory latency is primarily based on strided memory access patterns associated with vector computation.</text></s>
<s sid="62"><CoreSc1 advantage="None" conceptID="Met19" novelty="None" type="Met"/><text>If the latency of transfers from slow memory to fast memory is small, or if the number of threads per processor is infinite, then this model generally provides the same analysis results as the PRAM analysis.</text></s>
<s sid="63"><CoreSc1 advantage="None" conceptID="Met20" novelty="None" type="Met"/><text>It, however, provides more intuition.</text></s>
<s sid="64"><CoreSc1 advantage="None" conceptID="Goa6" novelty="None" type="Goa"/><text>(1) Ideally, we want to get the PRAM performance for algorithms using the fewest number of threads possible, since threads do have overhead.</text></s>
<s sid="65"><CoreSc1 advantage="None" conceptID="Goa7" novelty="None" type="Goa"/><text>This model can help us pick such algorithms.</text></s>
<s sid="66"><CoreSc1 advantage="None" conceptID="Res7" novelty="None" type="Res"/><text>(2) It also captures the reality of when memory latency is large and the number of threads is large but finite.</text></s>
<s sid="67"><CoreSc1 advantage="None" conceptID="Res8" novelty="None" type="Res"/><text>In particular, it can distinguish between algorithms that have the same PRAM analysis, but one may be better at hiding latency than another with a bounded number of threads.</text></s>
<s sid="68"><CoreSc1 advantage="None" conceptID="Bac7" novelty="None" type="Bac"/><text>This model is a high-level model meant to be generally applicable to a number of machines which allow a large number of threads with fast context switching.</text></s>
<s sid="69"><CoreSc1 advantage="None" conceptID="Mot4" novelty="None" type="Mot"/><text>Therefore, it abstracts away many implementation details of either the machine or the algorithm.</text></s>
<s sid="70"><CoreSc1 advantage="None" conceptID="Met21" novelty="None" type="Met"/><text>We also assume that the hardware provides 0-cost and perfect scheduling between threads.</text></s>
<s sid="71"><CoreSc1 advantage="None" conceptID="Met22" novelty="None" type="Met"/><text>In addition, it also models the machine as having only 2 levels of memory.</text></s>
<s sid="72"><CoreSc1 advantage="None" conceptID="Met23" novelty="None" type="Met"/><text>In particular, we model a slow global memory and fast local memory shared by one core group.</text></s>
<s sid="73"><CoreSc1 advantage="None" conceptID="Con4" novelty="None" type="Con"/><text>In practice, these machines may have many levels of memory.</text></s>
<s sid="74"><CoreSc1 advantage="None" conceptID="Con5" novelty="None" type="Con"/><text>However, we are interested in the interplay between the farthest level, since the latencies are the largest at that level, and therefore have the biggest impact on the performance.</text></s>
<s sid="75"><CoreSc1 advantage="None" conceptID="Con6" novelty="None" type="Con"/><text>We expect that the model can be extended to also model other levels of the memory hierarchy.</text></s>
<s sid="76"><CoreSc1 advantage="None" conceptID="Obj17" novelty="None" type="Obj"/><text>We analyze 4 classic algorithms for the problem of computing All Pairs Shortest Paths (APSP) on a weighted graph in the TMM model [43].</text></s>
<s sid="77"><CoreSc1 advantage="None" conceptID="Obj18" novelty="None" type="Obj"/><text>We compare the analysis from this model with the PRAM analysis of these 4 algorithms to gain intuition about the usefulness of both our model and the PRAM model for analyzing performance of algorithms on highly-threaded, many-core machines.</text></s>
<s sid="78"><CoreSc1 advantage="None" conceptID="Con7" novelty="None" type="Con"/><text>Our results validate the intuition that this model can provide more information than the PRAM model for the large latency, finite thread case.</text></s>
<s sid="79"><CoreSc1 advantage="None" conceptID="Obj19" novelty="None" type="Obj"/><text>In particular, we compare these algorithms and find specific relationships between hardware parameters (latency, fast memory size, limits on number of threads) under which some algorithms are better than others even if they have the same PRAM cost.</text></s>
<s sid="80"><CoreSc1 advantage="None" conceptID="Obj20" novelty="None" type="Obj"/><text>Following the formal analysis, we assess the utility of the model by comparing empirically measured performance on an individual machine to that predicted by the model.</text></s>
<s sid="81"><CoreSc1 advantage="None" conceptID="Obj21" novelty="None" type="Obj"/><text>For two of the APSP algorithms, we illustrate the impact of various individual parameters on performance, showing the effectiveness of the model at predicting measured performance.</text></s>
<s sid="82"><CoreSc1 advantage="None" conceptID="Res9" novelty="None" type="Res"/><text>This paper is organized as follows.</text></s>
<s sid="83"><CoreSc1 advantage="None" conceptID="Res10" novelty="None" type="Res"/><text>Section 2 presents related work.</text></s>
<s sid="84"><CoreSc1 advantage="None" conceptID="Res11" novelty="None" type="Res"/><text>Section 3 describes the TMM model.</text></s>
<s sid="85"><CoreSc1 advantage="None" conceptID="Res12" novelty="None" type="Res"/><text>Section 4 provides the 4 shortest paths algorithms and their analysis in both the PRAM and TMM models.</text></s>
<s sid="86"><CoreSc1 advantage="None" conceptID="Con8" novelty="None" type="Con"/><text>Section 5 provides the lessons learned from this model; in particular, we see that algorithms that have the same PRAM performance have different performance in the TMM model since they are better at hiding memory latency with fewer threads.</text></s>
<s sid="87"><CoreSc1 advantage="None" conceptID="Obj22" novelty="None" type="Obj"/><text>Section 6 continues the discussion of lessons learned, concentrating on the effects of problem size.</text></s>
<s sid="88"><CoreSc1 advantage="None" conceptID="Res13" novelty="None" type="Res"/><text>Section 7 shows performance measurements for a pair of the APSP algorithms executing on a commercial GPU, illustrating correspondence between model predictions and empirical measurements.</text></s>
Finally, Section 8 concludes.
Related work
<s sid="89"><CoreSc1 advantage="None" conceptID="Obj23" novelty="None" type="Obj"/><text>In this section, we briefly review the related work.</text></s>
<s sid="90"><CoreSc1 advantage="None" conceptID="Obj24" novelty="None" type="Obj"/><text>We first review the work on abstract models of computations for both sequential and parallel machines.</text></s>
<s sid="91"><CoreSc1 advantage="None" conceptID="Obj25" novelty="None" type="Obj"/><text>We then review recent work on algorithms and performance analysis of GPUs which are the most common current instantiations of highly-threaded, many-core machines.</text></s>
<s sid="92"><CoreSc1 advantage="None" conceptID="Obj26" novelty="None" type="Obj"/><text>Many machine and memory models have been designed for various types of parallel and sequential machines.</text></s>
<s sid="93"><CoreSc1 advantage="None" conceptID="Met24" novelty="None" type="Met"/><text>In an early work, Aggarwal et al. [25] present the Hierarchical Memory Model (HMM) and use it for a theoretical investigation of the inherent complexity of solving problems in RAM with a memory hierarchy of multiple levels.</text></s>
<s sid="94"><CoreSc1 advantage="None" conceptID="Met25" novelty="None" type="Met"/><text>It differs from the RAM model by defining that access to location x takes logx time, but it does not consider the concept of block transfers, which collects data into blocks to utilize spatial locality of reference in algorithms.</text></s>
<s sid="95"><CoreSc1 advantage="None" conceptID="Met26" novelty="None" type="Met"/><text>The Block Transfer model (BT) [27] addresses this deficiency by defining that a block of consecutive locations can be copied from memory to memory, taking one unit of time per element after the initial access time.</text></s>
<s sid="96"><CoreSc1 advantage="None" conceptID="Met27" novelty="None" type="Met"/><text>Alpern et al. propose the Memory Hierarchy (MH) Framework [26] that reflects important practical considerations that are hidden by the RAM and HMM models: data are moved in fixed size blocks simultaneously at different levels in the hierarchy, and the memory capacity as well as bus bandwidth are limited at each level.</text></s>
<s sid="97"><CoreSc1 advantage="None" conceptID="Met28" novelty="None" type="Met"/><text>But there are too many parameters in this model that can obscure algorithm analysis.</text></s>
<s sid="98"><CoreSc1 advantage="None" conceptID="Met29" novelty="None" type="Met"/><text>Thus, they simplified and reduced the MH parameters by putting forward a new Uniform Memory Hierarchy (UMH) model [28,29].</text></s>
<s sid="99"><CoreSc1 advantage="None" conceptID="Met30" novelty="None" type="Met"/><text>Later, an 'ideal-cache' model was introduced in [23,24] allowing analysis of cache-oblivious algorithms that use asymptotically optimal amounts of work and move data asymptotically optimally among multiple levels of cache without the necessity of tuning program variables according to hardware configuration parameters.</text></s>
<s sid="100"><CoreSc1 advantage="None" conceptID="Met31" novelty="None" type="Met"/><text>In the parallel case, although widely used, the PRAM [30] model is unrealistic because it assumes all processors work synchronously and that interprocessor communication is free.</text></s>
<s sid="101"><CoreSc1 advantage="None" conceptID="Met32" novelty="None" type="Met"/><text>Quite different to PRAM, the Bulk-Synchronous Parallel (BSP) model [34] attempts to bridge theory and practice by allowing processors to work asynchronously, and it models latency and limited bandwidth for distributed memory machines without shared memory.</text></s>
<s sid="102"><CoreSc1 advantage="None" conceptID="Met33" novelty="None" type="Met"/><text>Culler et al. [33] offer a new parallel machine model called LogP based on BSP, characterizing a parallel machine by four parameters: number of processors, communication bandwidth, delay, and overhead.</text></s>
<s sid="103"><CoreSc1 advantage="None" conceptID="Bac8" novelty="None" type="Bac"/><text>It reflects the convergence towards systems formed by a collection of computers connected by a communication network via message passing.</text></s>
<s sid="104"><CoreSc1 advantage="None" conceptID="Bac9" novelty="None" type="Bac"/><text>Vitter et al. [35] present a two-level memory model and give a realistic treatment of parallel block transfers in parallel machines.</text></s>
<s sid="105"><CoreSc1 advantage="None" conceptID="Bac10" novelty="None" type="Bac"/><text>But this model assumes that processors are interconnected via sharing of internal memory.</text></s>
<s sid="106"><CoreSc1 advantage="None" conceptID="Bac11" novelty="None" type="Bac"/><text>More recently, several models have been proposed emphasizing the use of private-cache chip multiprocessors (CMPs).</text></s>
<s sid="107"><CoreSc1 advantage="None" conceptID="Bac12" novelty="None" type="Bac"/><text>Arge et al. [36] present the Parallel External Memory (PEM) model with P processors and a two-level memory hierarchy, consisting of the main memory as external memory shared by all processors and caches as internal memory exclusive to each of the P processors.</text></s>
<s sid="108"><CoreSc1 advantage="None" conceptID="Bac13" novelty="None" type="Bac"/><text>Blelloch et al. [37] present a multi-core-cache model capturing the fact that multi-core machines have both per-processor private caches and a large shared cache on-chip.</text></s>
<s sid="109"><CoreSc1 advantage="None" conceptID="Bac14" novelty="None" type="Bac"/><text>Bender et al. [44] present a concurrent cache-oblivious model.</text></s>
<s sid="110"><CoreSc1 advantage="None" conceptID="Bac15" novelty="None" type="Bac"/><text>Blelloch et al. [38] also propose a parallel cache-oblivious (PCO) model to account for costs of a wide range of cache hierarchies.</text></s>
<s sid="111"><CoreSc1 advantage="None" conceptID="Met34" novelty="None" type="Met"/><text>Chowdhury et al. [39] present a hierarchical multi-level caching model (HM), consisting of a collection of cores sharing an arbitrarily large main memory through a hierarchy of caches of finite but increasing sizes that are successively shared by larger groups of cores.</text></s>
<s sid="112"><CoreSc1 advantage="None" conceptID="Met35" novelty="None" type="Met"/><text>They in [42] consider three types of caching systems for CMPs: D-CMP with a private cache for each core, S-CMP with a single cache shared by all cores, and multi-core with private L1 caches and a shared L2 cache.</text></s>
<s sid="113"><CoreSc1 advantage="None" conceptID="Met36" novelty="None" type="Met"/><text>All the models above do not accurately describe highly-threaded, many-core systems, due to their distinctive architectures, i.e. the explicit use of many threads for the purpose of hiding memory latency.</text></s>
<s sid="114"><CoreSc1 advantage="None" conceptID="Mot5" novelty="None" type="Mot"/><text>While there has not been much work on abstract machine models for highly-threaded, many-core machines, there has been a lot of recent work on designing calibrated performance models for particular instantiations of these machines such as NVIDIA GPUs.</text></s>
<s sid="115"><CoreSc1 advantage="None" conceptID="Res14" novelty="None" type="Res"/><text>We review some of that work here.</text></s>
<s sid="116"><CoreSc1 advantage="None" conceptID="Res15" novelty="None" type="Res"/><text>Liu et al. [19] describe a general performance model that predicts the performance of a biosequence database scanning application fairly precisely.</text></s>
<s sid="117"><CoreSc1 advantage="None" conceptID="Res16" novelty="None" type="Res"/><text>Their model incorporates the relationship between problem size and performance, but only targets their biosequence application.</text></s>
<s sid="118"><CoreSc1 advantage="None" conceptID="Met37" novelty="None" type="Met"/><text>Govindaraju et al. [45] propose a cache model for efficiently implementing three memory intensive scientific applications with nested loops.</text></s>
<s sid="119"><CoreSc1 advantage="None" conceptID="Met38" novelty="None" type="Met"/><text>It is helpful for applications with 2D-block representations while choosing an appropriate block size by estimating cache misses, but is not completely general.</text></s>
<s sid="120"><CoreSc1 advantage="None" conceptID="Met39" novelty="None" type="Met"/><text>Ryoo et al. [46] summarize five categories of optimization mechanisms, and use two metrics to prune the GPU performance optimization space by 98% via computing the utilization and efficiency of GPU applications.</text></s>
<s sid="121"><CoreSc1 advantage="None" conceptID="Met40" novelty="None" type="Met"/><text>They do not, however, consider memory latency and multiple conflicting performance indicators.</text></s>
<s sid="122"><CoreSc1 advantage="None" conceptID="Met41" novelty="None" type="Met"/><text>Kothapalli et al. are the first to define a general GPU analytical performance model in [47].</text></s>
<s sid="123"><CoreSc1 advantage="None" conceptID="Met42" novelty="None" type="Met"/><text>They propose a simple yet efficient solution combining several well-known parallel computation models: PRAM, BSP, QRQW, but they do not model global memory coalescing.</text></s>
<s sid="124"><CoreSc1 advantage="None" conceptID="Met43" novelty="None" type="Met"/><text>Using a different approach, Hong et al. [17] propose another analytical model to capture the cost of memory operations by counting the number of parallel memory requests in terms of memory-warp parallelism (MWP) and computation-warp parallelism (CWP).</text></s>
<s sid="125"><CoreSc1 advantage="None" conceptID="Con9" novelty="None" type="Con"/><text>Meantime, Baghsorkhi et al. [16] measure performance factors in isolation and later combine them to model the overall performance via workflow graphs so that the interactive effects between different performance factors are modeled correctly.</text></s>
<s sid="126"><CoreSc1 advantage="None" conceptID="Con10" novelty="None" type="Con"/><text>The model can determine data access patterns, branch divergence, and control flow patterns only for a restricted class of kernels on traditional GPU architectures.</text></s>
<s sid="127"><CoreSc1 advantage="None" conceptID="Con11" novelty="None" type="Con"/><text>Zhang and Owens [15] present a quantitative performance model that characterizes an application's performance as being primarily bounded by one of three potential limits: instruction pipeline, shared memory accesses, and global memory accesses.</text></s>
<s sid="128"><CoreSc1 advantage="None" conceptID="Obj27" novelty="None" type="Obj"/><text>More recently, Sim et al. [48] develop a performance analysis framework that consists of an analytical model and profiling tools.</text></s>
<s sid="129"><CoreSc1 advantage="None" conceptID="Obj28" novelty="None" type="Obj"/><text>The framework does a good job in performance diagnostics on case studies of real codes.</text></s>
<s sid="130"><CoreSc1 advantage="None" conceptID="Goa8" novelty="None" type="Goa"/><text>Kim et al. [49] also design a tool to estimate GPU memory performance by collecting performance-critical parameters.</text></s>
<s sid="131"><CoreSc1 advantage="None" conceptID="Goa9" novelty="None" type="Goa"/><text>Parakh et al. [50] present a model to estimate both computation time by precisely counting instructions and memory access time by a method to generate address traces.</text></s>
<s sid="132"><CoreSc1 advantage="None" conceptID="Goa10" novelty="None" type="Goa"/><text>All of these efforts are mainly focused on the practical calibrated performance models.</text></s>
<s sid="133"><CoreSc1 advantage="None" conceptID="Goa11" novelty="None" type="Goa"/><text>No attempts have been made to develop an asymptotic theoretical model applicable to a wide range of highly-threaded machines.</text></s>
TMM model
<s sid="134"><CoreSc1 advantage="None" conceptID="Goa12" novelty="None" type="Goa"/><text>The TMM model is meant to model the asymptotic performance of algorithms on highly-threaded, many-core machines.</text></s>
<s sid="135"><CoreSc1 advantage="None" conceptID="Goa13" novelty="None" type="Goa"/><text>The model should abstract away the details of particular implementations so as to be applicable to many instantiations of these machines, while being particular enough to model the performance of algorithms on these machines with reasonable accuracy.</text></s>
<s sid="136"><CoreSc1 advantage="None" conceptID="Goa14" novelty="None" type="Goa"/><text>In this section, we will describe the important characteristics of these highly-threaded, many-core architectures and our model for analyzing algorithms for these architectures.</text></s>
Highly-threaded, many-core architectures
<s sid="137"><CoreSc1 advantage="None" conceptID="Met44" novelty="None" type="Met"/><text>The most important high-level characteristic of highly-threaded, many-core architectures is that they provide a large number of hardware threads and use fast and low-overhead context-switching in order to hide the memory access latency from slow global memory.</text></s>
<s sid="138"><CoreSc1 advantage="None" conceptID="Met45" novelty="None" type="Met"/><text>Highly-threaded, many-core architectures typically consist of a number of core groups, each containing a number of processors (or cores), a fixed number of registers, and a fixed quantity of fast local on-chip memory shared within a core group.</text></s>
<s sid="139"><CoreSc1 advantage="None" conceptID="Met46" novelty="None" type="Met"/><text>A large slow global memory is shared by all the core groups.</text></s>
<s sid="140"><CoreSc1 advantage="None" conceptID="Met47" novelty="None" type="Met"/><text>Registers and local on-chip memory are the fastest to access, while accessing the global memory may potentially take 100s of cycles.</text></s>
<s sid="141"><CoreSc1 advantage="None" conceptID="Met48" novelty="None" type="Met"/><text>The TMM model models these machines as having a memory hierarchy with two levels of memory: slow global memory and fast local memory.</text></s>
<s sid="142"><CoreSc1 advantage="None" conceptID="Met49" novelty="None" type="Met"/><text>In addition, on most highly-threaded, many-core machines, data is transferred from slow to fast memory in chunks; instead of just transferring one word at a time, the hardware tries to transfer a large number of words during a memory transfer.</text></s>
<s sid="143"><CoreSc1 advantage="None" conceptID="Met50" novelty="None" type="Met"/><text>The chunk can either be a cache line from hardware managed caches, or an explicitly-managed combined read from multiple threads.</text></s>
<s sid="144"><CoreSc1 advantage="None" conceptID="Met51" novelty="None" type="Met"/><text>Since this characteristic of using high-bandwidth transfers in order to counter high latencies is common to most many-core machines (and even most multi-core machines), the TMM model captures the chunk size as one of its parameters.</text></s>
<s sid="145"><CoreSc1 advantage="None" conceptID="Res17" novelty="None" type="Res"/><text>These architectures support a large number of hardware threads, much larger than the number of cores.</text></s>
<s sid="146"><CoreSc1 advantage="None" conceptID="Res18" novelty="None" type="Res"/><text>Cores on a single core group execute in synchronous style where groups of threads execute in lock-step.</text></s>
<s sid="147"><CoreSc1 advantage="None" conceptID="Res19" novelty="None" type="Res"/><text>When a thread group executing on a core group stalls on a slow memory access, in theory, a context switch occurs and another thread group is scheduled on that core group.</text></s>
<s sid="148"><CoreSc1 advantage="None" conceptID="Obs5" novelty="None" type="Obs"/><text>The abstract architecture is shown in Fig. 2.</text></s>
<s sid="149"><CoreSc1 advantage="None" conceptID="Obs6" novelty="None" type="Obs"/><text>Note that this architecture abstraction ignores a number of details about the physical machine, including thread grouping, scheduling, etc.</text></s>
TMM model parameters
<s sid="150"><CoreSc1 advantage="None" conceptID="Obs7" novelty="None" type="Obs"/><text>The TMM model captures the important characteristics of a highly-threaded, many-core architecture by using six parameters shown in Table 1.</text></s>
<s sid="151"><CoreSc1 advantage="None" conceptID="Res20" novelty="None" type="Res"/><text>L is the latency for accessing the slow memory (in our case, the global memory which is shared by all the core groups).</text></s>
<s sid="152"><CoreSc1 advantage="None" conceptID="Res21" novelty="None" type="Res"/><text>P is the total number of cores (or processors) in the machine.</text></s>
<s sid="153"><CoreSc1 advantage="None" conceptID="Res22" novelty="None" type="Res"/><text>C is the maximum chunk size; the number of words that can be read from slow memory to fast memory in one memory transfer.</text></s>
<s sid="154"><CoreSc1 advantage="None" conceptID="Res23" novelty="None" type="Res"/><text>The parameter Z represents the size of fast local memory per core group and Q represents the number of cores per core group.</text></s>
<s sid="155"><CoreSc1 advantage="None" conceptID="Res24" novelty="None" type="Res"/><text>As mentioned earlier, in some instantiations, a core group can have a single core.</text></s>
<s sid="156"><CoreSc1 advantage="None" conceptID="Res25" novelty="None" type="Res"/><text>In this case, a many-core machine looks very much like a multi-core machine with a large number of low-overhead hardware threads.</text></s>
<s sid="157"><CoreSc1 advantage="None" conceptID="Res26" novelty="None" type="Res"/><text>Note that we do not have a parameter for the number of core groups, that quantity is simply P/Q.</text></s>
<s sid="158"><CoreSc1 advantage="None" conceptID="Res27" novelty="None" type="Res"/><text>Finally X is the hardware limit on the number of threads an algorithm is allowed to generate per core.</text></s>
<s sid="159"><CoreSc1 advantage="None" conceptID="Res28" novelty="None" type="Res"/><text>This limit is enforced due to many different constraints, such as constraints on the number of registers each thread uses and an explicit constraint on the number of threads.</text></s>
<s sid="160"><CoreSc1 advantage="None" conceptID="Mod1" novelty="None" type="Mod"/><text>We unify these constraints into one parameter.</text></s>
<s sid="161"><CoreSc1 advantage="None" conceptID="Mod2" novelty="None" type="Mod"/><text>In addition to the architecture parameters, we must also consider the parameters which are determined by the algorithm.</text></s>
<s sid="162"><CoreSc1 advantage="None" conceptID="Mod3" novelty="None" type="Mod"/><text>We assume that the programmer has written a correct synchronous program and taken care to balance the workload across the core groups.</text></s>
<s sid="163"><CoreSc1 advantage="None" conceptID="Res29" novelty="None" type="Res"/><text>These program parameters are shown in Table 2.</text></s>
<s sid="164"><CoreSc1 advantage="None" conceptID="Res30" novelty="None" type="Res"/><text>T1 represents the work of the algorithm, that is, the total number of operations that the program must perform (including fast memory accesses).</text></s>
<s sid="165"><CoreSc1 advantage="None" conceptID="Res31" novelty="None" type="Res"/><text>T∞ represents the span of the algorithm, that is, the total number of operations on the critical path.</text></s>
<s sid="166"><CoreSc1 advantage="None" conceptID="Res32" novelty="None" type="Res"/><text>These are similar to the analogous PRAM parameters of work and time (or depth or critical-path length).</text></s>
<s sid="167"><CoreSc1 advantage="None" conceptID="Res33" novelty="None" type="Res"/><text>Next, we come to program parameters that are specific to many-core programs.</text></s>
<s sid="168"><CoreSc1 advantage="None" conceptID="Res34" novelty="None" type="Res"/><text>M represents the total number of global memory operations performed by the algorithm.</text></s>
<s sid="169"><CoreSc1 advantage="None" conceptID="Res35" novelty="None" type="Res"/><text>Note that this is the total number of operations, not total number of accesses.</text></s>
<s sid="170"><CoreSc1 advantage="None" conceptID="Con12" novelty="None" type="Con"/><text>Since many-core machines often transfer data in large chunks, multiple memory accesses can combine into one memory transfer.</text></s>
<s sid="171"><CoreSc1 advantage="None" conceptID="Con13" novelty="None" type="Con"/><text>For instance, if the many-core machine has a hardware managed cache, and the program accesses data sequentially, then there is only one memory operation for C memory accesses; these will count as one when accounting for M.</text></s>
<s sid="172"><CoreSc1 advantage="None" conceptID="Res36" novelty="None" type="Res"/><text>T is the number of threads created by the program per core.</text></s>
<s sid="173"><CoreSc1 advantage="None" conceptID="Mod4" novelty="None" type="Mod"/><text>We assume that the work is perfectly distributed among cores.</text></s>
<s sid="174"><CoreSc1 advantage="None" conceptID="Res37" novelty="None" type="Res"/><text>Therefore, the total number of threads in the system is TP.</text></s>
<s sid="175"><CoreSc1 advantage="None" conceptID="Res38" novelty="None" type="Res"/><text>On highly-threaded, many-core architectures, thread switching is used to hide memory latency.</text></s>
<s sid="176"><CoreSc1 advantage="None" conceptID="Res39" novelty="None" type="Res"/><text>Therefore, it is beneficial to create as many threads as possible.</text></s>
<s sid="177"><CoreSc1 advantage="None" conceptID="Res40" novelty="None" type="Res"/><text>However, the maximum number of threads is limited by both the hardware and the program.</text></s>
<s sid="178"><CoreSc1 advantage="None" conceptID="Res41" novelty="None" type="Res"/><text>The software limitation has to do with parallelism, the number of threads per core is limited by T≤T1/(T∞⋅P).</text></s>
The hardware limits T≤X.
<s sid="179"><CoreSc1 advantage="None" conceptID="Res42" novelty="None" type="Res"/><text>Finally, we have a parameter S, which is the local memory used per thread.</text></s>
<s sid="180"><CoreSc1 advantage="None" conceptID="Res43" novelty="None" type="Res"/><text>S and T are related parameters, since there is a limited amount of local memory in the system.</text></s>
<s sid="181"><CoreSc1 advantage="None" conceptID="Res44" novelty="None" type="Res"/><text>The number of threads per core is at most T≤Z/(QS).</text></s>
TMM model applicability
<s sid="182"><CoreSc1 advantage="None" conceptID="Res45" novelty="None" type="Res"/><text>The TMM model is a high-level abstract model, meant to be applicable to many instantiations of hardware platforms that feature a large number of threads with fast context switching and a hierarchical memory subsystem of at least two levels with a large memory latency gap in between.</text></s>
<s sid="183"><CoreSc1 advantage="None" conceptID="Res46" novelty="None" type="Res"/><text>Typical examples of this set include NVIDIA GPUs, AMD/ATI GPUs, and the uRiKA machine from YarcData.</text></s>
<s sid="184"><CoreSc1 advantage="None" conceptID="Res47" novelty="None" type="Res"/><text>For NVIDIA GPUs, a number of streaming multiprocessors (core groups in our terminology) share the same global memory.</text></s>
<s sid="185"><CoreSc1 advantage="None" conceptID="Res48" novelty="None" type="Res"/><text>On each of these core groups, there are a number of CUDA cores that share a fixed number of registers and on-chip (fast) memory shared among the cores of the core group.</text></s>
<s sid="186"><CoreSc1 advantage="None" conceptID="Res49" novelty="None" type="Res"/><text>A fast hardware-supported context-switching mechanism enables a large number of threads to execute concurrently.</text></s>
<s sid="187"><CoreSc1 advantage="None" conceptID="Res50" novelty="None" type="Res"/><text>Transfers between slow global memory and fast local memory can occur in chunks of at most 32 words; these chunks can only be created if the memory accesses are within a specified range.</text></s>
<s sid="188"><CoreSc1 advantage="None" conceptID="Obs8" novelty="None" type="Obs"/><text>Accessing the off-chip global memory usually takes 20 to 40 times more clock cycles than accessing the on-chip shared memory/L1 cache [51].</text></s>
<s sid="189"><CoreSc1 advantage="None" conceptID="Obs9" novelty="None" type="Obs"/><text>All these features are well captured in the TMM model.</text></s>
<s sid="190"><CoreSc1 advantage="None" conceptID="Res51" novelty="None" type="Res"/><text>Streaming multiprocessors serve the same role as a core group, while CUDA cores are equivalent to the cores defined in TMM.</text></s>
<s sid="191"><CoreSc1 advantage="None" conceptID="Res52" novelty="None" type="Res"/><text>The width of memory access C is 32 due to the coalescing of the threads in a warp.</text></s>
<s sid="192"><CoreSc1 advantage="None" conceptID="Obs10" novelty="None" type="Obs"/><text>Global memory latency and size of on-chip shared memory/L1 cache are also depicted by L and Z respectively.</text></s>
<s sid="193"><CoreSc1 advantage="None" conceptID="Res53" novelty="None" type="Res"/><text>Considering AMD/ATI GPUs and taking Cypress, the codename for Radeon HD5800 series GPUs, as an example, the architecture is composed of 20 Single-Instruction-Multiple-Data (SIMD) computation engines.</text></s>
<s sid="194"><CoreSc1 advantage="None" conceptID="Res54" novelty="None" type="Res"/><text>In each SIMD engine, there are 16 Thread Processors (TP) and a 32 kB Local Data Store (LDS).</text></s>
<s sid="195"><CoreSc1 advantage="None" conceptID="Res55" novelty="None" type="Res"/><text>Every TP is arranged as a five-way or four-way Very Long Instruction Word (VLIW) processor, and consists of 5 Stream Cores (SC).</text></s>
<s sid="196"><CoreSc1 advantage="None" conceptID="Res56" novelty="None" type="Res"/><text>Low context-switch threading is well supported, and every 64 threads are grouped into a wavefront executing the same instruction.</text></s>
<s sid="197"><CoreSc1 advantage="None" conceptID="Res57" novelty="None" type="Res"/><text>Basically, the SIMD engine can naturally be modeled by core groups.</text></s>
<s sid="198"><CoreSc1 advantage="None" conceptID="Res58" novelty="None" type="Res"/><text>Each SC is modeled as a core in TMM, summing up to 1600 cores totally.</text></s>
<s sid="199"><CoreSc1 advantage="None" conceptID="Res59" novelty="None" type="Res"/><text>LDS is straightforwardly described by the fast local memory of TMM.</text></s>
<s sid="200"><CoreSc1 advantage="None" conceptID="Res60" novelty="None" type="Res"/><text>The width of memory access C in TMM equals to the wavefront width of 64 for AMD/ATI GPUs.</text></s>
<s sid="201"><CoreSc1 advantage="None" conceptID="Res61" novelty="None" type="Res"/><text>The uRiKA system from YarcData is also a potential target for the TMM model.</text></s>
<s sid="202"><CoreSc1 advantage="None" conceptID="Res62" novelty="None" type="Res"/><text>Based on the description from Alverson et al. [52] about the nature of the computations this processor was designed to run, it is a purpose-built appliance for real-time graph analytics featuring graph-optimized hardware that provides up to 512 terabytes of global shared memory, massively-multi-threaded graph processors (named Threadstorm) supporting 128 threads/processor, and highly scalable I/O.</text></s>
<s sid="203"><CoreSc1 advantage="None" conceptID="Res63" novelty="None" type="Res"/><text>Therefore, 128 defines parameter X, the hard limit of number of threads per processor.</text></s>
<s sid="204"><CoreSc1 advantage="None" conceptID="Res64" novelty="None" type="Res"/><text>There can be up to 65,000 threads in a 512 processor system and over 1 million threads at the maximum system size of 8192 processors, so that the latencies are hidden by accommodating many remote memory references in flight.</text></s>
<s sid="205"><CoreSc1 advantage="None" conceptID="Res65" novelty="None" type="Res"/><text>The processor's instruction execution hardware essentially does a context switch every instruction cycle, finding the next thread that is ready to issue an instruction into the execution pipeline.</text></s>
<s sid="206"><CoreSc1 advantage="None" conceptID="Res66" novelty="None" type="Res"/><text>This suggests that the memory access width or chunk size C is 1 on these machines.</text></s>
<s sid="207"><CoreSc1 advantage="None" conceptID="Res67" novelty="None" type="Res"/><text>Threads do not share anything, as the Threadstorm processor has 128 hardware copies of the register set, program counter, stack pointer, etc., necessary to hold the current state of one software thread that is executing on the processor.</text></s>
<s sid="208"><CoreSc1 advantage="None" conceptID="Res68" novelty="None" type="Res"/><text>Conceptually, each of the Threadstorm processors is mapped to a core group in the TMM model but, different than the two GPU architectures, it has only one core on-chip, thus Q equals 1.</text></s>
TMM analysis structure
<s sid="209"><CoreSc1 advantage="None" conceptID="Goa15" novelty="None" type="Goa"/><text>In order to analyze program performance in the TMM model, we must first calculate the program parameters for the particular program.</text></s>
<s sid="210"><CoreSc1 advantage="None" conceptID="Goa16" novelty="None" type="Goa"/><text>Once we have calculated these values, we can then try to understand the performance of the algorithm.</text></s>
<s sid="211"><CoreSc1 advantage="None" conceptID="Con14" novelty="None" type="Con"/><text>We first calculate the effective work of the algorithm TE.</text></s>
<s sid="212"><CoreSc1 advantage="None" conceptID="Con15" novelty="None" type="Con"/><text>The effective work should consider both work due to computation and work due to memory accesses.</text></s>
<s sid="213"><CoreSc1 advantage="None" conceptID="Con16" novelty="None" type="Con"/><text>Total work due to memory accesses is M⋅L, but since this work is hidden by using threads, the real effective work due to memory accesses is (M⋅L)/T Therefore, we have (1)TE=O(max(T1,M⋅LT)).</text></s>
<s sid="214"><CoreSc1 advantage="None" conceptID="Res69" novelty="None" type="Res"/><text>Note that this expression assumes perfect scheduling (the threads are context swapped with no overhead, as soon as they are stalled) and perfect load balance between threads.</text></s>
<s sid="215"><CoreSc1 advantage="None" conceptID="Res70" novelty="None" type="Res"/><text>The time to execute on P cores is represented by TP and is defined as: (2)TP=O(max(TEP,T∞))=O(max(T1P,T∞,M⋅LT⋅P)).</text></s>
<s sid="216"><CoreSc1 advantage="None" conceptID="Res71" novelty="None" type="Res"/><text>Therefore, speedup on P cores, SP, is (3)SP=T1TP=Ω(min(P,T1T∞,P⋅T1⋅TM⋅L)).</text></s>
<s sid="217"><CoreSc1 advantage="None" conceptID="Res72" novelty="None" type="Res"/><text>For linear speedup, SP should be P.</text></s>
<s sid="218"><CoreSc1 advantage="None" conceptID="Res73" novelty="None" type="Res"/><text>More precisely, for PRAM algorithms, SP=min(P,T1/T∞).</text></s>
<s sid="219"><CoreSc1 advantage="None" conceptID="Res74" novelty="None" type="Res"/><text>Therefore, if the first two terms in the min of Eq. (3) dominate, then a highly-threaded, many-core algorithm's performance is the same as the corresponding PRAM algorithm.</text></s>
<s sid="220"><CoreSc1 advantage="None" conceptID="Obs11" novelty="None" type="Obs"/><text>On the other hand, if the last term dominates, then the algorithm's performance depends on other factors.</text></s>
<s sid="221"><CoreSc1 advantage="None" conceptID="Obs12" novelty="None" type="Obs"/><text>If T could be unbounded, then the last term will never dominate.</text></s>
<s sid="222"><CoreSc1 advantage="None" conceptID="Res75" novelty="None" type="Res"/><text>However, as we explained earlier, T is not an unlimited resource and has both hardware and algorithmic upper bounds.</text></s>
<s sid="223"><CoreSc1 advantage="None" conceptID="Con17" novelty="None" type="Con"/><text>Therefore, based on the machine parameters, algorithms that have the same PRAM performance can have different real performance on highly-threaded, many-core machines.</text></s>
<s sid="224"><CoreSc1 advantage="None" conceptID="Con18" novelty="None" type="Con"/><text>Therefore, this model can help us pick algorithms that provide performance as close as possible to PRAM algorithms.</text></s>
<s sid="225"><CoreSc1 advantage="None" conceptID="Obj29" novelty="None" type="Obj"/><text>Analysis of all pairs shortest paths algorithms using the TMM model</text></s>
<s sid="226"><CoreSc1 advantage="None" conceptID="Obj30" novelty="None" type="Obj"/><text>In this section, we demonstrate the usefulness of our model by using it to analyze 4 different algorithms for calculating all pairs shortest paths in graphs.</text></s>
<s sid="227"><CoreSc1 advantage="None" conceptID="Obs13" novelty="None" type="Obs"/><text>All pairs shortest paths is a classic problem for which there are many algorithms.</text></s>
<s sid="228"><CoreSc1 advantage="None" conceptID="Obs14" novelty="None" type="Obs"/><text>We are given a graph G=(V,E) with n vertices and m edges.</text></s>
<s sid="229"><CoreSc1 advantage="None" conceptID="Res76" novelty="None" type="Res"/><text>Each edge e has a weight w(e).</text></s>
<s sid="230"><CoreSc1 advantage="None" conceptID="Con19" novelty="None" type="Con"/><text>We must calculate the shortest weighted path from every vertex to every other vertex.</text></s>
<s sid="231"><CoreSc1 advantage="None" conceptID="Con20" novelty="None" type="Con"/><text>In this section, we are interested in asymptotic insights, therefore, we assume that the graphs are large graphs.</text></s>
In particular n&gt;Z.
<s sid="232"><CoreSc1 advantage="None" conceptID="Res77" novelty="None" type="Res"/><text>Dynamic programming via matrix multiplication</text></s>
<s sid="233"><CoreSc1 advantage="None" conceptID="Res78" novelty="None" type="Res"/><text>Our first algorithm is a dynamic programming algorithm [53] that uses repeated matrix multiplication to calculate all pairs shortest paths.</text></s>
<s sid="234"><CoreSc1 advantage="None" conceptID="Mod5" novelty="None" type="Mod"/><text>The graph is represented as an adjacency matrix A where Aij represents the weight of edge (i,j).</text></s>
<s sid="235"><CoreSc1 advantage="None" conceptID="Mod6" novelty="None" type="Mod"/><text>Al is a transitive matrix where Aijl represents the shortest path from vertex i to vertex j using at most l intermediate edges.</text></s>
<s sid="236"><CoreSc1 advantage="None" conceptID="Mod7" novelty="None" type="Mod"/><text>A1 is the same as the adjacency matrix A and we want to calculate An-1 to calculate all pairs shortest paths.</text></s>
<s sid="237"><CoreSc1 advantage="None" conceptID="Mod8" novelty="None" type="Mod"/><text>A2 can be calculated from A1 as follows: (4)Aij2=min0≤k&lt;n(Aij1,Aik1+Akj1).</text></s>
<s sid="238"><CoreSc1 advantage="None" conceptID="Mod9" novelty="None" type="Mod"/><text>Note that, the structure of this equation is the same as the structure of a matrix multiplication operation where the sum is replaced by a min operation and the multiplication is replaced by an addition operation.</text></s>
<s sid="239"><CoreSc1 advantage="None" conceptID="Mod10" novelty="None" type="Mod"/><text>Therefore, we can use repeated matrix multiplication which calculates An using O(lgn) matrix multiplications.</text></s>
PRAM algorithm and analysis
<s sid="240"><CoreSc1 advantage="None" conceptID="Mod11" novelty="None" type="Mod"/><text>Parallelizing this algorithm for the PRAM model simply involves parallelizing the matrix multiplication algorithm such that each element in the matrix is calculated in parallel.</text></s>
<s sid="241"><CoreSc1 advantage="None" conceptID="Mod12" novelty="None" type="Mod"/><text>The total work of lgn matrix multiplications using a PRAM algorithm is T1=O(n3lgn).</text></s>
<s sid="242"><CoreSc1 advantage="None" conceptID="Mod13" novelty="None" type="Mod"/><text>The span of a single matrix multiplication algorithm is O(n).</text></s>
<s sid="243"><CoreSc1 advantage="None" conceptID="Mod14" novelty="None" type="Mod"/><text>Therefore, the total span of the algorithm is T∞=O(nlgn).</text></s>
<s sid="244"><CoreSc1 advantage="None" conceptID="Mod15" novelty="None" type="Mod"/><text>The time and speedup using P processors are (5)TP=O(max(n3lgnP,nlgn))(6)SP=Ω(min(P,n2)).</text></s>
<s sid="245"><CoreSc1 advantage="None" conceptID="Con21" novelty="None" type="Con"/><text>Therefore, the PRAM algorithm gets linear speedup as long asP≤n2.</text></s>
TMM algorithm and analysis
<s sid="246"><CoreSc1 advantage="None" conceptID="Con22" novelty="None" type="Con"/><text>TMM algorithms are tailored to highly-threaded, many-core architectures generally by using fast on-chip memory to avoid accesses to slow off-chip global memory, coalescing to diminish the time required to access slow memory, and threading to hide the latency of accesses to slow memory.</text></s>
<s sid="247"><CoreSc1 advantage="None" conceptID="Res79" novelty="None" type="Res"/><text>Due to its large size, the adjacency matrix is stored in off-chip global memory.</text></s>
<s sid="248"><CoreSc1 advantage="None" conceptID="Res80" novelty="None" type="Res"/><text>Following traditional block-decomposition techniques, sub-blocks of the result matrix (whose size is denoted by B) are assigned to core groups for computation.</text></s>
<s sid="249"><CoreSc1 advantage="None" conceptID="Res81" novelty="None" type="Res"/><text>The threads in a core group read in the required input sub-blocks, perform the computation of Eq. (4) for their assigned sub-block, and write the sub-block out to global memory.</text></s>
<s sid="250"><CoreSc1 advantage="None" conceptID="Res82" novelty="None" type="Res"/><text>This happens lgn times by repeated squaring.</text></s>
<s sid="251"><CoreSc1 advantage="None" conceptID="Con23" novelty="None" type="Con"/><text>The work and the span of this algorithm remain unchanged from the PRAM algorithm.</text></s>
<s sid="252"><CoreSc1 advantage="None" conceptID="Con24" novelty="None" type="Con"/><text>However, we must also calculate M, the number of memory operations.</text></s>
<s sid="253"><CoreSc1 advantage="None" conceptID="Res83" novelty="None" type="Res"/><text>Let us first consider a single matrix multiplication operation.</text></s>
<s sid="254"><CoreSc1 advantage="None" conceptID="Res84" novelty="None" type="Res"/><text>There are a total of n2 elements and each element is read for the calculation of O(n/B) other blocks.</text></s>
<s sid="255"><CoreSc1 advantage="None" conceptID="Res85" novelty="None" type="Res"/><text>However, due to the regularity in memory accesses, each block can be read fully coalesced.</text></s>
<s sid="256"><CoreSc1 advantage="None" conceptID="Res86" novelty="None" type="Res"/><text>Therefore, the number of memory operations for one matrix multiply is O((n2/C)⋅(n/B))=O(n3/(BC)).</text></s>
<s sid="257"><CoreSc1 advantage="None" conceptID="Res87" novelty="None" type="Res"/><text>Also note that since we must fit a B×B block in a local memory of size Z on one core group, we get B=Θ(Z).</text></s>
<s sid="258"><CoreSc1 advantage="None" conceptID="Res88" novelty="None" type="Res"/><text>Therefore, for lgn matrix multiplication operations, M=O(n3lgn/(Z⋅C)).</text></s>
<s sid="259"><CoreSc1 advantage="None" conceptID="Res89" novelty="None" type="Res"/><text>Now we are ready to calculate the time on P processors. (7)TP=O(max(T1P,T∞,M⋅LT⋅P))(8)=O(max(n3lgnP,nlgn,n3lgn⋅LZ⋅C⋅T⋅P)).</text></s>
<s sid="260"><CoreSc1 advantage="None" conceptID="Res90" novelty="None" type="Res"/><text>Therefore, the speedup on P processors is (9)SP=T1/TP(10)=Ω(min(P,n2,Z⋅C⋅TL⋅P)).</text></s>
<s sid="261"><CoreSc1 advantage="None" conceptID="Res91" novelty="None" type="Res"/><text>We can now compare the PRAM and TMM analysis and note that the speedup is P as long as ZCT/L≥1.</text></s>
<s sid="262"><CoreSc1 advantage="None" conceptID="Res92" novelty="None" type="Res"/><text>We also know that T≤min(X,Z/(QS)), and S=O(1), since each thread only needs constant memory.</text></s>
<s sid="263"><CoreSc1 advantage="None" conceptID="Con25" novelty="None" type="Con"/><text>Therefore, we can conclude that the algorithm achieves linear speedup as long as L≤min(ZCX,Z3/2C/Q).</text></s>
<s sid="264"><CoreSc1 advantage="None" conceptID="Con26" novelty="None" type="Con"/><text>Johnson's algorithm: Dijkstra's algorithm using binary heaps</text></s>
<s sid="265"><CoreSc1 advantage="None" conceptID="Con27" novelty="None" type="Con"/><text>Johnson's algorithm [54] is an all pairs shortest paths algorithm that uses Dijkstra's single source algorithm as the subroutine and calls it n times, once from each source vertex.</text></s>
<s sid="266"><CoreSc1 advantage="None" conceptID="Con28" novelty="None" type="Con"/><text>Dijkstra's algorithm is a greedy algorithm for calculating single source shortest paths.</text></s>
<s sid="267"><CoreSc1 advantage="None" conceptID="Con29" novelty="None" type="Con"/><text>The pseudo-code for Dijkstra's algorithm is given in Algorithm 1 [55].</text></s>
<s sid="268"><CoreSc1 advantage="None" conceptID="Con30" novelty="None" type="Con"/><text>The single source algorithm consists of n insert operations, m decrease-key operations and n delete-min operations from a min-priority queue.</text></s>
<s sid="269"><CoreSc1 advantage="None" conceptID="Con31" novelty="None" type="Con"/><text>The standard way of implementing Dijkstra's algorithm is to use a binary or a Fibonacci heap to store the array elements.</text></s>
<s sid="270"><CoreSc1 advantage="None" conceptID="Con32" novelty="None" type="Con"/><text>We now consider a binary heap implementation so that each operation (insert, decrease-key, and delete-min) takes O(lgn) time.</text></s>
<s sid="271"><CoreSc1 advantage="None" conceptID="Con33" novelty="None" type="Con"/><text>Note that Dijkstra's algorithm does not work when there are negative weight edges in the graph.</text></s>
PRAM algorithm and analysis
<s sid="272"><CoreSc1 advantage="None" conceptID="Con34" novelty="None" type="Con"/><text>A simple parallel implementation of Johnson's algorithm using Dijkstra's algorithm consists of doing each single-source shortest path calculation in parallel.</text></s>
<s sid="273"><CoreSc1 advantage="None" conceptID="Con35" novelty="None" type="Con"/><text>The total work of a single-source computation is O(mlgn+nlgn).</text></s>
<s sid="274"><CoreSc1 advantage="None" conceptID="Con36" novelty="None" type="Con"/><text>For simplicity, we assume that the graph is connected, giving us O(mlgn).</text></s>
<s sid="275"><CoreSc1 advantage="None" conceptID="Con37" novelty="None" type="Con"/><text>Therefore, the total work for all pairs shortest paths is T1=O(mnlgn).</text></s>
<s sid="276"><CoreSc1 advantage="None" conceptID="Res93" novelty="None" type="Res"/><text>The span is T∞=O(mlgn) since each single source computation executes sequentially.</text></s>
<s sid="277"><CoreSc1 advantage="None" conceptID="Res94" novelty="None" type="Res"/><text>The time and speedup using P processors are (11)TP=O(max(mnlgnP,mlgn))(12)SP=Ω(min(P,n)).</text></s>
<s sid="278"><CoreSc1 advantage="None" conceptID="Res95" novelty="None" type="Res"/><text>Therefore, the PRAM algorithm gets linear speedup as long as P≤n.</text></s>
TMM algorithm and analysis
<s sid="279"><CoreSc1 advantage="None" conceptID="Res96" novelty="None" type="Res"/><text>The TMM algorithm is very similar to the PRAM algorithm where each thread computes a single source shortest path.</text></s>
<s sid="280"><CoreSc1 advantage="None" conceptID="Res97" novelty="None" type="Res"/><text>Therefore, each thread requires a min-heap of size n.</text></s>
<s sid="281"><CoreSc1 advantage="None" conceptID="Con38" novelty="None" type="Con"/><text>Since n may be arbitrarily large compared to Z/QT (the share of local memory for each thread), these heaps cannot fit in local memory and must be allocated on the slow global memory.</text></s>
<s sid="282"><CoreSc1 advantage="None" conceptID="Con39" novelty="None" type="Con"/><text>The work and span are the same as the PRAM algorithm.</text></s>
<s sid="283"><CoreSc1 advantage="None" conceptID="Con40" novelty="None" type="Con"/><text>We must now compute M.</text></s>
<s sid="284"><CoreSc1 advantage="None" conceptID="Con41" novelty="None" type="Con"/><text>Note that each time the thread does a heap operation, it must access global memory, since the heaps are stored in global memory.</text></s>
<s sid="285"><CoreSc1 advantage="None" conceptID="Con42" novelty="None" type="Con"/><text>In addition, binary heap accesses are not predictable and regular, so the heap accesses from different threads cannot be coalesced.</text></s>
<s sid="286"><CoreSc1 advantage="None" conceptID="Con43" novelty="None" type="Con"/><text>Therefore, the total number of memory operations is M=O(mnlgn).</text></s>
<s sid="287"><CoreSc1 advantage="None" conceptID="Con44" novelty="None" type="Con"/><text>Now we are ready to calculate the time on P processors. (13)TP=O(max(T1P,T∞,M⋅LT⋅P))(14)=O(max(mnlgnP,mlgn,mnlgn⋅LT⋅P)).</text></s>
<s sid="288"><CoreSc1 advantage="None" conceptID="Con45" novelty="None" type="Con"/><text>Therefore, the speedup on P processors is (15)SP=Ω(min(P,n,TL⋅P)).</text></s>
<s sid="289"><CoreSc1 advantage="None" conceptID="Con46" novelty="None" type="Con"/><text>Note that this algorithm gets linear speedup only if T/L≥1.</text></s>
<s sid="290"><CoreSc1 advantage="None" conceptID="Con47" novelty="None" type="Con"/><text>Therefore, the number of threads this algorithm needs to get linear speedup is very large.</text></s>
<s sid="291"><CoreSc1 advantage="None" conceptID="Con48" novelty="None" type="Con"/><text>We know that T≤min(X,Z/(QS)), and S=O(1) for this algorithm.</text></s>
<s sid="292"><CoreSc1 advantage="None" conceptID="Con49" novelty="None" type="Con"/><text>This allows us to conclude that this algorithm achieves linear speedup only if L≤min(X,Z/Q), since each thread needs only constant memory.</text></s>
<s sid="293"><CoreSc1 advantage="None" conceptID="Con50" novelty="None" type="Con"/><text>These conditions are much stricter than those imposed by the dynamic programming algorithm.</text></s>
<s sid="294"><CoreSc1 advantage="None" conceptID="Con51" novelty="None" type="Con"/><text>Johnson's algorithm: Dijkstra's algorithm using arrays</text></s>
<s sid="295"><CoreSc1 advantage="None" conceptID="Con52" novelty="None" type="Con"/><text>This algorithm is similar to the previous algorithm in that it still uses n single-source Dijkstra's algorithm calculations.</text></s>
<s sid="296"><CoreSc1 advantage="None" conceptID="Con53" novelty="None" type="Con"/><text>However, instead of binary heaps, we use arrays to do delete-min and decrease-key operations.</text></s>
PRAM algorithm and analysis
<s sid="297"><CoreSc1 advantage="None" conceptID="Con54" novelty="None" type="Con"/><text>The PRAM algorithm is very similar to the algorithm that uses binary heaps.</text></s>
<s sid="298"><CoreSc1 advantage="None" conceptID="Con55" novelty="None" type="Con"/><text>Each single source shortest path is computed in parallel.</text></s>
<s sid="299"><CoreSc1 advantage="None" conceptID="Con56" novelty="None" type="Con"/><text>However, in this algorithm, we simply store the current estimates of the shortest path of vertices in an array instead of a binary heap.</text></s>
<s sid="300"><CoreSc1 advantage="None" conceptID="Res98" novelty="None" type="Res"/><text>Therefore, there are n arrays of size n, one for each single source shortest path calculation.</text></s>
<s sid="301"><CoreSc1 advantage="None" conceptID="Res99" novelty="None" type="Res"/><text>Each decrease-key now takes O(1) time, since one can simply decrease the key using random access.</text></s>
<s sid="302"><CoreSc1 advantage="None" conceptID="Con57" novelty="None" type="Con"/><text>Each delete-min, however, takes O(n) work, since one must look at the entire array to find the minimum element.</text></s>
<s sid="303"><CoreSc1 advantage="None" conceptID="Con58" novelty="None" type="Con"/><text>Therefore, the work of the algorithm is T1=O(n3+mn) and the span is O(n2+m).</text></s>
<s sid="304"><CoreSc1 advantage="None" conceptID="Con59" novelty="None" type="Con"/><text>We can improve the span by doing delete-min in parallel, since one can find the smallest element in an array in parallel using O(n) work and O(lgn) time using a parallel prefix computation.</text></s>
<s sid="305"><CoreSc1 advantage="None" conceptID="Con60" novelty="None" type="Con"/><text>This brings the total span to T∞=O(nlgn+m) while the work remains the same.</text></s>
<s sid="306"><CoreSc1 advantage="None" conceptID="Con61" novelty="None" type="Con"/><text>The time and speedup using P processors is (16)TP=O(max(n3P,nlgn+m))(17)=O(max(n3P,nlgn,m))(18)SP=Ω(min(P,n2lgn,n3m)).</text></s>
TMM algorithm and analysis
<s sid="307"><CoreSc1 advantage="None" conceptID="Con62" novelty="None" type="Con"/><text>The TMM algorithm is similar to the PRAM algorithm, except that each core group is responsible for a single-source shortest path calculation.</text></s>
<s sid="308"><CoreSc1 advantage="None" conceptID="Con63" novelty="None" type="Con"/><text>Therefore, all the threads on a single core group (QT in number) cooperate to calculate a single shortest path computation.</text></s>
<s sid="309"><CoreSc1 advantage="None" conceptID="Con64" novelty="None" type="Con"/><text>Since we assume that n&gt;Z, the entire array does not fit in local memory and must be read with each delete-min operation.</text></s>
<s sid="310"><CoreSc1 advantage="None" conceptID="Con65" novelty="None" type="Con"/><text>Therefore, the span of the delete-min operation changes.</text></s>
<s sid="311"><CoreSc1 advantage="None" conceptID="Res100" novelty="None" type="Res"/><text>For each delete-min operation, elements are read into local memory in chunks of size Z.</text></s>
<s sid="312"><CoreSc1 advantage="None" conceptID="Res101" novelty="None" type="Res"/><text>For each chunk, the minimum is computed in parallel in O(lgZ) time.</text></s>
<s sid="313"><CoreSc1 advantage="None" conceptID="Res102" novelty="None" type="Res"/><text>Therefore, the span of each delete-min operation is O((n/Z)lgZ).</text></s>
<s sid="314"><CoreSc1 advantage="None" conceptID="Con66" novelty="None" type="Con"/><text>Therefore, the total span is T∞=O(n2lgZ/Z).</text></s>
<s sid="315"><CoreSc1 advantage="None" conceptID="Con67" novelty="None" type="Con"/><text>The work is the same as the PRAM work.</text></s>
<s sid="316"><CoreSc1 advantage="None" conceptID="Con68" novelty="None" type="Con"/><text>We must now compute the number of memory operations, M.</text></s>
<s sid="317"><CoreSc1 advantage="None" conceptID="Res103" novelty="None" type="Res"/><text>There are n2 delete-min operations in total, and each reads the array of size n coalesced.</text></s>
<s sid="318"><CoreSc1 advantage="None" conceptID="Res104" novelty="None" type="Res"/><text>In addition, there are a total of mn decrease key operations, but these reads cannot be coalesced.</text></s>
Therefore, M=O(n3/C+mn).
(19)TP=O(max(T1P,T∞,M⋅LT⋅P))(20)=O(max(n3P,n2lgZZ,(n3C+mn)⋅LT⋅P))(21)=O(max(n3P,n2lgZZ,n3⋅LC⋅T⋅P,mn⋅LT⋅P)).
Speedup is (22)SP=Ω(min(P,nZlgZ,C⋅TL⋅P,n2⋅Tm⋅L⋅P)).
<s sid="319"><CoreSc1 advantage="None" conceptID="Con69" novelty="None" type="Con"/><text>Again, in this algorithm, T≤min(X,Z/(QS)), and S=O(1) since each thread needs only constant memory.</text></s>
<s sid="320"><CoreSc1 advantage="None" conceptID="Con70" novelty="None" type="Con"/><text>Therefore, the PRAM performance dominates if L≤min(CX,CZ/Q,n2X/m,n2Z/(mQ)).</text></s>
<s sid="321"><CoreSc1 advantage="None" conceptID="Con71" novelty="None" type="Con"/><text>n iterations of Bellman-Ford algorithm</text></s>
<s sid="322"><CoreSc1 advantage="None" conceptID="Con72" novelty="None" type="Con"/><text>This is another all pairs shortest paths algorithm that uses a single-source Bellman-Ford algorithm as a subroutine.</text></s>
<s sid="323"><CoreSc1 advantage="None" conceptID="Con73" novelty="None" type="Con"/><text>The algorithm is given in Algorithm 2 [56,57].</text></s>
PRAM algorithm and analysis
<s sid="324"><CoreSc1 advantage="None" conceptID="Con74" novelty="None" type="Con"/><text>Again, one can do each single source computation in parallel.</text></s>
<s sid="325"><CoreSc1 advantage="None" conceptID="Con75" novelty="None" type="Con"/><text>Each single source computation takes O(mn) work, making the total work of all pairs shortest paths O(mn2) and the total span O(mn).</text></s>
<s sid="326"><CoreSc1 advantage="None" conceptID="Con76" novelty="None" type="Con"/><text>One can improve the span by relaxing all edges in one iteration in parallel making the span O(n).</text></s>
(23)TP=O(max(mn2P,n)).(24)SP=Ω(min(P,mn)).
TMM algorithm and analysis
<s sid="327"><CoreSc1 advantage="None" conceptID="Con77" novelty="None" type="Con"/><text>The TMM algorithm for this problem is more complicated and requires more data structure support.</text></s>
<s sid="328"><CoreSc1 advantage="None" conceptID="Res105" novelty="None" type="Res"/><text>Each core group is responsible for one single-source shortest path calculation.</text></s>
<s sid="329"><CoreSc1 advantage="None" conceptID="Res106" novelty="None" type="Res"/><text>For each single source calculation, we maintain three arrays, A,B and W, of size m, and one array D of size n.</text></s>
<s sid="330"><CoreSc1 advantage="None" conceptID="Res107" novelty="None" type="Res"/><text>D contains the current guess of the shortest path to vertex i.</text></s>
<s sid="331"><CoreSc1 advantage="None" conceptID="Res108" novelty="None" type="Res"/><text>B contains ending vertices of edges, sorted by vertex ID.</text></s>
<s sid="332"><CoreSc1 advantage="None" conceptID="Con78" novelty="None" type="Con"/><text>Therefore B may contain multiple instances of the same vertex if that vertex has multiple incident edges.</text></s>
<s sid="333"><CoreSc1 advantage="None" conceptID="Res109" novelty="None" type="Res"/><text>A[i] contains the starting vertex of the edge that ends at B[i] and W[i] contains the weight of that edge.</text></s>
<s sid="334"><CoreSc1 advantage="None" conceptID="Res110" novelty="None" type="Res"/><text>Therefore, both D and B are sorted.</text></s>
<s sid="335"><CoreSc1 advantage="None" conceptID="Res111" novelty="None" type="Res"/><text>Each thread deals with one index in the array and relaxes that edge in each iteration.</text></s>
<s sid="336"><CoreSc1 advantage="None" conceptID="Res112" novelty="None" type="Res"/><text>All threads relax edges in parallel in order of B.</text></s>
<s sid="337"><CoreSc1 advantage="None" conceptID="Con79" novelty="None" type="Con"/><text>The total work and span are the same as the PRAM algorithm.</text></s>
<s sid="338"><CoreSc1 advantage="None" conceptID="Con80" novelty="None" type="Con"/><text>We can now calculate the time and speedup assuming threads can read all the arrays coalesced, M=O(mn2/C+n3/C)=O(mn2/C) for connected graphs.</text></s>
(25)TP=O(max(T1P,T∞,M⋅LT⋅P))(26)=O(max(mn2P,n,mn2⋅LC⋅T⋅P)).
<s sid="339"><CoreSc1 advantage="None" conceptID="Con81" novelty="None" type="Con"/><text>Therefore, the speedup on P processors is (27)SP=Ω(min(P,mn,C⋅TL⋅P)).</text></s>
<s sid="340"><CoreSc1 advantage="None" conceptID="Con82" novelty="None" type="Con"/><text>In this case, we get linear speedup if CT/L≥1.</text></s>
<s sid="341"><CoreSc1 advantage="None" conceptID="Con83" novelty="None" type="Con"/><text>Subject to the limits on threads of T≤min(X,Z/(QS)) and S=O(1) for constant local memory usage per thread, this requires L≤min(CX,CZ/Q).</text></s>
<s sid="342"><CoreSc1 advantage="None" conceptID="Con84" novelty="None" type="Con"/><text>Comparison of the various algorithms</text></s>
<s sid="343"><CoreSc1 advantage="None" conceptID="Con85" novelty="None" type="Con"/><text>As our analysis of shortest paths algorithms indicates, the TMM model allows us to take the unique properties of highly-threaded, many-core architectures into consideration while analyzing the algorithms.</text></s>
<s sid="344"><CoreSc1 advantage="None" conceptID="Con86" novelty="None" type="Con"/><text>Therefore, the model provides more nuance in the analysis of these algorithms for the highly-threaded, many-core machines than the PRAM model.</text></s>
<s sid="345"><CoreSc1 advantage="None" conceptID="Goa17" novelty="None" type="Goa"/><text>In this section, we will compare the running times of the various algorithms and see what interesting things this analysis tells us.</text></s>
<s sid="346"><CoreSc1 advantage="None" conceptID="Con87" novelty="None" type="Con"/><text>Table 3 indicates the running times of the various algorithms in both the PRAM model and the TMM model, as well as the conditions under which TMM results are the same as the PRAM results.</text></s>
<s sid="347"><CoreSc1 advantage="None" conceptID="Con88" novelty="None" type="Con"/><text>We have ignored the span term, since the span is small relative to work in all of these algorithms.</text></s>
<s sid="348"><CoreSc1 advantage="None" conceptID="Con89" novelty="None" type="Con"/><text>As we can see, if L is small, then highly-threaded, many-core machines provide PRAM performance.</text></s>
<s sid="349"><CoreSc1 advantage="None" conceptID="Con90" novelty="None" type="Con"/><text>However, the cut-off value for L is different for different algorithms where the performance in the TMM model differs from the PRAM model is different for different algorithms.</text></s>
<s sid="350"><CoreSc1 advantage="None" conceptID="Con91" novelty="None" type="Con"/><text>Therefore, the TMM model can be informative when comparing between algorithms.</text></s>
<s sid="351"><CoreSc1 advantage="None" conceptID="Obj31" novelty="None" type="Obj"/><text>We will perform two types of comparison between these algorithms in this section.</text></s>
<s sid="352"><CoreSc1 advantage="None" conceptID="Res113" novelty="None" type="Res"/><text>The first one considers the direct influence of machine parameters on asymptotic performance.</text></s>
<s sid="353"><CoreSc1 advantage="None" conceptID="Con92" novelty="None" type="Con"/><text>Since machine parameters do not scale with problem size, in principle, machine parameters cannot change the asymptotic performance of algorithms in terms of problem size.</text></s>
<s sid="354"><CoreSc1 advantage="None" conceptID="Con93" novelty="None" type="Con"/><text>That is, if the PRAM analysis indicates that some algorithm has a running time of O(n) and another one has the running time of O(nlgn), for large enough n, the first algorithm is always asymptotically better since eventually lgn will dominate whatever machine parameter advantage the second algorithm may have.</text></s>
<s sid="355"><CoreSc1 advantage="None" conceptID="Con94" novelty="None" type="Con"/><text>Therefore, for this first comparison, we only compare algorithms which have the same asymptotic performance under the PRAM model.</text></s>
<s sid="356"><CoreSc1 advantage="None" conceptID="Res114" novelty="None" type="Res"/><text>Second, we will also do a non-asymptotic comparison where we compare algorithms when the problem size is relatively small, but not very small.</text></s>
<s sid="357"><CoreSc1 advantage="None" conceptID="Obs15" novelty="None" type="Obs"/><text>In particular, we look at the case when lgn&lt;Z.</text></s>
<s sid="358"><CoreSc1 advantage="None" conceptID="Res115" novelty="None" type="Res"/><text>In this case, even algorithms that are asymptotically worse in the PRAM model can be better in the TMM model, for large latency L.</text></s>
<s sid="359"><CoreSc1 advantage="None" conceptID="Res116" novelty="None" type="Res"/><text>In the next section, we will look at even smaller problem sizes where the effects are even more dramatic.</text></s>
Influence of machine parameters
<s sid="360"><CoreSc1 advantage="None" conceptID="Res117" novelty="None" type="Res"/><text>As the table shows, the limits on machine parameters to get linear speedup are different for different algorithms.</text></s>
<s sid="361"><CoreSc1 advantage="None" conceptID="Con95" novelty="None" type="Con"/><text>Therefore, even when two algorithms have the same PRAM performance, their performance on highly-threaded, many-core machines may vary significantly.</text></s>
<s sid="362"><CoreSc1 advantage="None" conceptID="Res118" novelty="None" type="Res"/><text>Let us consider a few examples:</text></s>
<s sid="363"><CoreSc1 advantage="None" conceptID="Res119" novelty="None" type="Res"/><text>Dynamic programming vs. Johnson's algorithm using binary heaps when m=O(n2)</text></s>
<s sid="364"><CoreSc1 advantage="None" conceptID="Res120" novelty="None" type="Res"/><text>If m=O(n2) (i.e., the graph is dense), the PRAM performance for both algorithms is the same.</text></s>
<s sid="365"><CoreSc1 advantage="None" conceptID="Res121" novelty="None" type="Res"/><text>However whenZ/Q&lt;L&lt;Z3/2C/Q, Johnson's algorithm has a significantly worse running time.</text></s>
<s sid="366"><CoreSc1 advantage="None" conceptID="Obs16" novelty="None" type="Obs"/><text>Take the example of L=O(Z3/2C/Q).</text></s>
<s sid="367"><CoreSc1 advantage="None" conceptID="Res122" novelty="None" type="Res"/><text>The Johnson running time is O(n3lgnZC/P) while the running time of the dynamic programming algorithm is simply O(n3lgn/P).</text></s>
<s sid="368"><CoreSc1 advantage="None" conceptID="Res123" novelty="None" type="Res"/><text>Johnson's algorithm using binary heaps vs. Johnson's algorithm using arrays when m=O(n2/lgn)</text></s>
<s sid="369"><CoreSc1 advantage="None" conceptID="Res124" novelty="None" type="Res"/><text>If m=O(n2/lgn) (i.e., a somewhat sparse graph), these two algorithms have the same PRAM performance, but if Z/Q&lt;L≤ZC/Q, then the array implementation is better.</text></s>
<s sid="370"><CoreSc1 advantage="None" conceptID="Res125" novelty="None" type="Res"/><text>For L=ZC/Q, the binary heap implementation has a running time of O(n3C/P), while the array implementation has a running time of simply O(n3/P).</text></s>
Influence of graph size
<s sid="371"><CoreSc1 advantage="None" conceptID="Res126" novelty="None" type="Res"/><text>The previous section shows the asymptotic power of the model; the results there hold for large sizes of graphs asymptotically.</text></s>
<s sid="372"><CoreSc1 advantage="None" conceptID="Con96" novelty="None" type="Con"/><text>However, the TMM model can also help decide on what algorithm to use based on the size of the graph.</text></s>
<s sid="373"><CoreSc1 advantage="None" conceptID="Con97" novelty="None" type="Con"/><text>In particular for certain sizes of graphs, algorithm A can be better than algorithm B even if it is asymptotically worse in the PRAM model.</text></s>
<s sid="374"><CoreSc1 advantage="None" conceptID="Con98" novelty="None" type="Con"/><text>Therefore, the TMM model can give us information that the PRAM model cannot.</text></s>
<s sid="375"><CoreSc1 advantage="None" conceptID="Con99" novelty="None" type="Con"/><text>Consider the example of dynamic programming vs. Johnson's algorithm using arrays.</text></s>
<s sid="376"><CoreSc1 advantage="None" conceptID="Con100" novelty="None" type="Con"/><text>In the PRAM model, the dynamic programming algorithm is unquestionably worse than Johnson's.</text></s>
<s sid="377"><CoreSc1 advantage="None" conceptID="Con101" novelty="None" type="Con"/><text>However, if lgn&lt;Z, we may have a different conclusion.</text></s>
<s sid="378"><CoreSc1 advantage="None" conceptID="Con102" novelty="None" type="Con"/><text>In this case, dynamic programming has runtime: (28)n3lgn⋅LZCTP=n2LTP⋅nlgnZC&lt;n2LTP⋅nC.</text></s>
<s sid="379"><CoreSc1 advantage="None" conceptID="Con103" novelty="None" type="Con"/><text>While Johnson's algorithm has runtime: (29)min(n3LCTP,mnLTP)=n2LTP⋅min(nC,mn).</text></s>
<s sid="380"><CoreSc1 advantage="None" conceptID="Con104" novelty="None" type="Con"/><text>If n2/m&lt;C, i.e. dense graphs, n/C&lt;m/n.</text></s>
<s sid="381"><CoreSc1 advantage="None" conceptID="Con105" novelty="None" type="Con"/><text>Combine (28) and (29), we have (30)n3lgn⋅LZCTP&lt;n3LCTP,if n2m&lt;C.</text></s>
<s sid="382"><CoreSc1 advantage="None" conceptID="Con106" novelty="None" type="Con"/><text>This indicates that when for small enough graphs where lgn&lt;Z, there is a dichotomy.</text></s>
<s sid="383"><CoreSc1 advantage="None" conceptID="Con107" novelty="None" type="Con"/><text>For dense graphs n2/m&lt;C, the dynamic programming algorithm should be preferred, while for sparse graphs, Johnson's algorithm with arrays is better.</text></s>
<s sid="384"><CoreSc1 advantage="None" conceptID="Con108" novelty="None" type="Con"/><text>We illustrate this performance dependence on sparsity with experiments in Section 7.</text></s>
<s sid="385"><CoreSc1 advantage="None" conceptID="Con109" novelty="None" type="Con"/><text>We get a similar result when comparing the dynamic programming algorithm with Bellman-Ford when m=O(n).</text></s>
<s sid="386"><CoreSc1 advantage="None" conceptID="Con110" novelty="None" type="Con"/><text>In spite of being worse in the PRAM world, the dynamic programming algorithm is better when lgn&lt;Z.</text></s>
<s sid="387"><CoreSc1 advantage="None" conceptID="Con111" novelty="None" type="Con"/><text>Our model therefore allows us to do two things.</text></s>
<s sid="388"><CoreSc1 advantage="None" conceptID="Con112" novelty="None" type="Con"/><text>First, for a particular machine, given two algorithms which are asymptotically similar, we can pick the more appropriate algorithm for that particular machine given its machine parameters.</text></s>
<s sid="389"><CoreSc1 advantage="None" conceptID="Con113" novelty="None" type="Con"/><text>Second, if we also consider the problem size,then we can do more.</text></s>
<s sid="390"><CoreSc1 advantage="None" conceptID="Con114" novelty="None" type="Con"/><text>For small problem sizes, the asymptotically worse algorithm may in fact be better because it interacts better with the machine.</text></s>
<s sid="391"><CoreSc1 advantage="None" conceptID="Obj32" novelty="None" type="Obj"/><text>We will draw more insights of this type in the next section.</text></s>
Effect of problem size
<s sid="392"><CoreSc1 advantage="None" conceptID="Obj33" novelty="None" type="Obj"/><text>In Section 5, we explored the asymptotic insights that can be drawn from the TMM model.</text></s>
<s sid="393"><CoreSc1 advantage="None" conceptID="Con115" novelty="None" type="Con"/><text>However, the TMM model can also inform insights based on problem size.</text></s>
<s sid="394"><CoreSc1 advantage="None" conceptID="Con116" novelty="None" type="Con"/><text>In particular, some algorithms can take advantage of smaller problems better than others, since they can use fast local memory more effectively.</text></s>
<s sid="395"><CoreSc1 advantage="None" conceptID="Con117" novelty="None" type="Con"/><text>In this section, we explore the insights that the TMM model provides in these cases.</text></s>
<s sid="396"><CoreSc1 advantage="None" conceptID="Res127" novelty="None" type="Res"/><text>Vertices fit in local memory</text></s>
<s sid="397"><CoreSc1 advantage="None" conceptID="Res128" novelty="None" type="Res"/><text>When n&lt;Z, all the vertices fit in local memory.</text></s>
<s sid="398"><CoreSc1 advantage="None" conceptID="Con118" novelty="None" type="Con"/><text>Note that this does not mean that the entire problem fits in local memory, since the number of edges can still be much larger than the number of vertices.</text></s>
<s sid="399"><CoreSc1 advantage="None" conceptID="Con119" novelty="None" type="Con"/><text>In this scenario, the number of memory accesses by the first, second, and fourth algorithms is not affected at all.</text></s>
<s sid="400"><CoreSc1 advantage="None" conceptID="Con120" novelty="None" type="Con"/><text>In the dynamic programming algorithm, we consider the array of size n2 and being able to fit a row into local memory does not reduce the number of memory transfers.</text></s>
<s sid="401"><CoreSc1 advantage="None" conceptID="Con121" novelty="None" type="Con"/><text>In Johnson's algorithm using binary heaps, each thread does its own single source shortest path.</text></s>
<s sid="402"><CoreSc1 advantage="None" conceptID="Con122" novelty="None" type="Con"/><text>Since the local memory Z is shared among QT threads, each thread cannot hold its entire vertex array in local memory.</text></s>
<s sid="403"><CoreSc1 advantage="None" conceptID="Con123" novelty="None" type="Con"/><text>In the Bellman-Ford algorithm, the cost is dominated by the cost of reading the edges.</text></s>
<s sid="404"><CoreSc1 advantage="None" conceptID="Con124" novelty="None" type="Con"/><text>Therefore, the bounds do not change.</text></s>
<s sid="405"><CoreSc1 advantage="None" conceptID="Con125" novelty="None" type="Con"/><text>For Johnson's algorithm using arrays, the cost is lower.</text></s>
<s sid="406"><CoreSc1 advantage="None" conceptID="Con126" novelty="None" type="Con"/><text>Now each core group can store the vertex array and does not need to access it from slow memory.</text></s>
<s sid="407"><CoreSc1 advantage="None" conceptID="Con127" novelty="None" type="Con"/><text>Therefore the bound on the number of memory operations changes to M=O(n2/C+mn)=O(mn) for connected graphs.</text></s>
<s sid="408"><CoreSc1 advantage="None" conceptID="Con128" novelty="None" type="Con"/><text>For these small problem sizes, the TMM model can provide even more insight.</text></s>
<s sid="409"><CoreSc1 advantage="None" conceptID="Con129" novelty="None" type="Con"/><text>As an example, compare the two versions of Johnson's algorithm, the one that uses arrays and the one that uses heaps.</text></s>
<s sid="410"><CoreSc1 advantage="None" conceptID="Con130" novelty="None" type="Con"/><text>When m=O(n2/lg2n), the algorithm that uses heaps is better than the algorithm that uses arrays in the PRAM model.</text></s>
<s sid="411"><CoreSc1 advantage="None" conceptID="Con131" novelty="None" type="Con"/><text>But in the TMM model, for large L, the algorithm that uses heaps has the running time of O(Lmnlgn/(TP))=O(Ln3/(TPlgn)), while the algorithm that uses arrays has the running time of O(Ln3/(TPlg2n)).</text></s>
<s sid="412"><CoreSc1 advantage="None" conceptID="Con132" novelty="None" type="Con"/><text>Therefore, the algorithm that uses arrays is better.</text></s>
<s sid="413"><CoreSc1 advantage="None" conceptID="Con133" novelty="None" type="Con"/><text>Note that asymptotic analysis is a little dubious when we are talking about small problem sizes; therefore, this analysis should be considered skeptically.</text></s>
<s sid="414"><CoreSc1 advantage="None" conceptID="Con134" novelty="None" type="Con"/><text>However, the analysis is rigorous when we consider the circumstance that local memory size grows with problem size (i.e., Z is asymptotic).</text></s>
<s sid="415"><CoreSc1 advantage="None" conceptID="Con135" novelty="None" type="Con"/><text>Moreover, this type of analysis can still provide enough insight that it might guide implementation decisions under the more realistic circumstance of bounded (but potentially large) Z.</text></s>
<s sid="416"><CoreSc1 advantage="None" conceptID="Res129" novelty="None" type="Res"/><text>Edges fit in the combined local memories</text></s>
<s sid="417"><CoreSc1 advantage="None" conceptID="Res130" novelty="None" type="Res"/><text>When m=O(PZ/Q), the edges fit in all the memories of the core groups combined.</text></s>
<s sid="418"><CoreSc1 advantage="None" conceptID="Res131" novelty="None" type="Res"/><text>Again, the running time of the first,second, and third algorithms do not change, since they cannot take advantage of this property.</text></s>
<s sid="419"><CoreSc1 advantage="None" conceptID="Res132" novelty="None" type="Res"/><text>However, the Bellman-Ford algorithm can take advantage of this property and each thread across all core groups is responsible for relaxing a single edge.</text></s>
<s sid="420"><CoreSc1 advantage="None" conceptID="Res133" novelty="None" type="Res"/><text>Now a portion of the arrays A,B and W fit in each core group's local memory and they never have to be read again.</text></s>
<s sid="421"><CoreSc1 advantage="None" conceptID="Con136" novelty="None" type="Con"/><text>Therefore, the number of memory operations reduces to M=O(n3/C).</text></s>
<s sid="422"><CoreSc1 advantage="None" conceptID="Con137" novelty="None" type="Con"/><text>And the run time under the TMM model reduces to O(n3L/(CTP)).</text></s>
<s sid="423"><CoreSc1 advantage="None" conceptID="Con138" novelty="None" type="Con"/><text>Again, compare Bellman-Ford algorithm with Johnson's algorithm using binary heaps.</text></s>
<s sid="424"><CoreSc1 advantage="None" conceptID="Con139" novelty="None" type="Con"/><text>When m=O(n2/lgn), Johnson's algorithm is better than the Bellman-Ford algorithm in the PRAM model.</text></s>
<s sid="425"><CoreSc1 advantage="None" conceptID="Con140" novelty="None" type="Con"/><text>However, in the TMM model, Johnson's has run time of O(Lmnlgn/(TP))=O(Ln3/(TP)), while Bellman-Ford with a run time of O(Ln3/(CTP)) flips to be the better one.</text></s>
Empirical investigation
<s sid="426"><CoreSc1 advantage="None" conceptID="Con141" novelty="None" type="Con"/><text>In this section, we conduct experiments to understand the extent of the applicability of our model in explaining the performance of algorithms on a real machine.</text></s>
<s sid="427"><CoreSc1 advantage="None" conceptID="Con142" novelty="None" type="Con"/><text>This evaluation is a proof-of-concept that the model successfully predicts performance on one example of a highly-threaded, many-core machine.</text></s>
<s sid="428"><CoreSc1 advantage="None" conceptID="Con143" novelty="None" type="Con"/><text>It is not meant to be an exhaustive empirical study of the model's applicability for all instances of highly-threaded, many-core machines.</text></s>
<s sid="429"><CoreSc1 advantage="None" conceptID="Met52" novelty="None" type="Met"/><text>We implemented two all-pairs shortest paths algorithms: the dynamic programming using matrix multiplication and Johnson's algorithm using arrays, on an NVIDIA GPU.</text></s>
<s sid="430"><CoreSc1 advantage="None" conceptID="Obj34" novelty="None" type="Obj"/><text>In these experiments, we investigate the following aspects of the TMM model:</text></s>
<s sid="431"><CoreSc1 advantage="None" conceptID="Con144" novelty="None" type="Con"/><text>• Effect of the number of threads: the fact that the TMM model incorporates the number of threads per processor in the model is the primary differentiator between the PRAM and TMM models.</text></s>
<s sid="432"><CoreSc1 advantage="None" conceptID="Con145" novelty="None" type="Con"/><text>The TMM model predicts that as the number of threads increases the performance increases, up to a certain point.</text></s>
<s sid="433"><CoreSc1 advantage="None" conceptID="Con146" novelty="None" type="Con"/><text>After this point, the number of threads does not matter, and the TMM model behaves the same as the PRAM model.</text></s>
<s sid="434"><CoreSc1 advantage="None" conceptID="Con147" novelty="None" type="Con"/><text>In this set of experiments, we will use both the dynamic programming and Johnson's algorithms to demonstrate this dependence on the number of threads.</text></s>
<s sid="435"><CoreSc1 advantage="None" conceptID="Con148" novelty="None" type="Con"/><text>• Effect of fast local memory size: in some algorithms, including the dynamic programming via matrix multiplication, the size of the fast memory affects the performance of the algorithm in the TMM model.</text></s>
We investigate this dependence.
<s sid="436"><CoreSc1 advantage="None" conceptID="Con149" novelty="None" type="Con"/><text>• Comparison of the dynamic programming algorithm and Johnson's algorithm with arrays: for Johnson's algorithm using arrays, the PRAM performance does not depend on the graph's density.</text></s>
<s sid="437"><CoreSc1 advantage="None" conceptID="Con150" novelty="None" type="Con"/><text>However, the TMM model predicts that performance can depend on the graph's density, when the number of threads is insufficient for the performance to be equivalent to the PRAM model.</text></s>
<s sid="438"><CoreSc1 advantage="None" conceptID="Con151" novelty="None" type="Con"/><text>Therefore, even though Johnson's algorithm is always faster than the dynamic programming algorithm according to the PRAM model (since its work is n3 while the dynamic programming algorithm has work n3lgn), the TMM model predicts that when the number of threads is small, the dynamic programming algorithm may do better, especially for dense graphs.</text></s>
<s sid="439"><CoreSc1 advantage="None" conceptID="Con152" novelty="None" type="Con"/><text>We demonstrate through experiments that, this is a true indicator of performance.</text></s>
Experimental Setup
<s sid="440"><CoreSc1 advantage="None" conceptID="Exp1" novelty="None" type="Exp"/><text>The experiments are carried out on an NVIDIA GTX 480, which has 15 multiprocessors, each with 32 cores.</text></s>
<s sid="441"><CoreSc1 advantage="None" conceptID="Res134" novelty="None" type="Res"/><text>As a typical highly-threaded, many-core machine, it also features a 1.5 GB global memory and 16 kB/48 kB of configurable on-chip shared memory per multiprocessor, which can be accessed with latency significantly lower than the global memory.</text></s>
<s sid="442"><CoreSc1 advantage="None" conceptID="Obj35" novelty="None" type="Obj"/><text>Runtimes are measured across various configurations of each problem, including graph size, thread count, shared memory size, and graph density.</text></s>
<s sid="443"><CoreSc1 advantage="None" conceptID="Res135" novelty="None" type="Res"/><text>When plotted as execution time, the performance units are in seconds.</text></s>
<s sid="444"><CoreSc1 advantage="None" conceptID="Res136" novelty="None" type="Res"/><text>In many cases, however, the trends we wish to see are more readily apparent when performance is shown in terms of speedup rather than execution time.</text></s>
<s sid="445"><CoreSc1 advantage="None" conceptID="Res137" novelty="None" type="Res"/><text>This poses a problem, however, as it is arguably meaningless to attempt to realistically measure the single-core execution time of an application deployed on a modern GPU.</text></s>
<s sid="446"><CoreSc1 advantage="None" conceptID="Obj36" novelty="None" type="Obj"/><text>We address this issue using the following technique: all speedup plots compare the measured, empirical execution time on P cores to the theoretical, asymptotic execution time on 1 core using the PRAM model.</text></s>
<s sid="447"><CoreSc1 advantage="None" conceptID="Con153" novelty="None" type="Con"/><text>As a result, the speedup axis does not represent a quantitatively meaningful scale, and the scale is labeled &quot;arbitrary&quot; on the graphs to reflect this fact; however, the shape of the curves are representative of the speedup achievable relative to a fixed serial execution time.</text></s>
<s sid="448"><CoreSc1 advantage="None" conceptID="Con154" novelty="None" type="Con"/><text>Effect of the number of threads</text></s>
<s sid="449"><CoreSc1 advantage="None" conceptID="Con155" novelty="None" type="Con"/><text>The TMM model indicates that when the number of threads is small, the performance of algorithms depends on the number of threads.</text></s>
<s sid="450"><CoreSc1 advantage="None" conceptID="Con156" novelty="None" type="Con"/><text>With sufficient number of threads, the performance converges to the PRAM performance and only depends on the problem size and the number of processors.</text></s>
<s sid="451"><CoreSc1 advantage="None" conceptID="Con157" novelty="None" type="Con"/><text>We verify this result using both the dynamic programming and Johnson's algorithms.</text></s>
<s sid="452"><CoreSc1 advantage="None" conceptID="Con158" novelty="None" type="Con"/><text>For the dynamic programming algorithm, we generate random graphs with {1k,2k,4k,8k,16k} vertices.</text></s>
<s sid="453"><CoreSc1 advantage="None" conceptID="Con159" novelty="None" type="Con"/><text>To better utilize fast local memory, the problem is decomposed into sub-blocks, and we must also pick a block size.</text></s>
<s sid="454"><CoreSc1 advantage="None" conceptID="Con160" novelty="None" type="Con"/><text>Since we only care about the effect of threads and not the effect of shared memory (to be considered in the next subsection), here we show the results with a block size of 64, as it allows us to generate the maximum number of threads.</text></s>
<s sid="455"><CoreSc1 advantage="None" conceptID="Con161" novelty="None" type="Con"/><text>We increase the number of threads until we reach either the hardware limit or the limit imposed by the algorithm.</text></s>
<s sid="456"><CoreSc1 advantage="None" conceptID="Obs17" novelty="None" type="Obs"/><text>Fig. 3 shows the speedup while varying the number of threads per core.</text></s>
<s sid="457"><CoreSc1 advantage="None" conceptID="Res138" novelty="None" type="Res"/><text>We see that the speedup increases approximately linearly with the number of threads per core (as predicted by Eq. (10)) and then flattens out.</text></s>
<s sid="458"><CoreSc1 advantage="None" conceptID="Con162" novelty="None" type="Con"/><text>This indicates that for this experiment, 16 is an estimated threshold of threads/core where the TMM model switches to the &quot;PRAM range&quot; and the number of threads no longer matters.</text></s>
<s sid="459"><CoreSc1 advantage="None" conceptID="Con163" novelty="None" type="Con"/><text>Note that the expression for this threshold does not depend on the graph size, as it is equal to L/ZC.</text></s>
<s sid="460"><CoreSc1 advantage="None" conceptID="Res139" novelty="None" type="Res"/><text>Also note that the speedup (both in and out of the PRAM range) is not impacted by the size of the graph (again as predicted by Eq. (10)).</text></s>
<s sid="461"><CoreSc1 advantage="None" conceptID="Res140" novelty="None" type="Res"/><text>We see a similar performance dependence on the number of threads in Johnson's algorithm.</text></s>
<s sid="462"><CoreSc1 advantage="None" conceptID="Res141" novelty="None" type="Res"/><text>Here we ran experiments with 8k vertices and varied the number of edges (ranging between 32k and 32 M).</text></s>
<s sid="463"><CoreSc1 advantage="None" conceptID="Obs18" novelty="None" type="Obs"/><text>The speedup graph is shown in Fig. 4.</text></s>
<s sid="464"><CoreSc1 advantage="None" conceptID="Res142" novelty="None" type="Res"/><text>As we increase the number of threads, the speedup increases.</text></s>
<s sid="465"><CoreSc1 advantage="None" conceptID="Res143" novelty="None" type="Res"/><text>We see two other interesting things, however.</text></s>
<s sid="466"><CoreSc1 advantage="None" conceptID="Res144" novelty="None" type="Res"/><text>First, we never see the flattening of performance with increasing thread counts that is seen with the dynamic programming algorithm.</text></s>
<s sid="467"><CoreSc1 advantage="None" conceptID="Con164" novelty="None" type="Con"/><text>Therefore, it appears that Johnson's algorithm requires more threads to reach the PRAM range where the performance no longer depends on the number of threads.</text></s>
<s sid="468"><CoreSc1 advantage="None" conceptID="Con165" novelty="None" type="Con"/><text>This is also predicted by our model as the number of threads/core required by the dynamic programming algorithm to reach PRAM range is T≥L/ZC while the corresponding number of threads required by Johnson's is T≥L/C, clearly a larger threshold.</text></s>
<s sid="469"><CoreSc1 advantage="None" conceptID="Con166" novelty="None" type="Con"/><text>Johnson's algorithm is not taking advantage of the fast local memory, and this factor influences the number of threads required to hide the latency to global memory.</text></s>
<s sid="470"><CoreSc1 advantage="None" conceptID="Res145" novelty="None" type="Res"/><text>Second, we see that the performance depends on the number of edges.</text></s>
<s sid="471"><CoreSc1 advantage="None" conceptID="Res146" novelty="None" type="Res"/><text>This is consistent with the fact that we are in the TMM range where the runtime is (mnL/TP) and not in the PRAM range where the runtime only depends on the number of vertices.</text></s>
<s sid="472"><CoreSc1 advantage="None" conceptID="Obs19" novelty="None" type="Obs"/><text>The dependence on graph density is explored further in Fig. 5.</text></s>
<s sid="473"><CoreSc1 advantage="None" conceptID="Res147" novelty="None" type="Res"/><text>Here, the runtime is plotted vs. number of graph edges for varying threads/core.</text></s>
<s sid="474"><CoreSc1 advantage="None" conceptID="Res148" novelty="None" type="Res"/><text>The linear relationship predicted by the last term of Eq. (21) (for dense graphs) is illustrated clearly in the figure.</text></s>
<s sid="475"><CoreSc1 advantage="None" conceptID="Con167" novelty="None" type="Con"/><text>Effect of fast local memory size</text></s>
<s sid="476"><CoreSc1 advantage="None" conceptID="Con168" novelty="None" type="Con"/><text>In highly-threaded, many-core machines, access to local memory is faster than access to slow global memory.</text></s>
<s sid="477"><CoreSc1 advantage="None" conceptID="Con169" novelty="None" type="Con"/><text>Among our shortest paths algorithms, only the dynamic programming algorithm makes use of the local memory and the running time depends on this fast memory size.</text></s>
<s sid="478"><CoreSc1 advantage="None" conceptID="Con170" novelty="None" type="Con"/><text>In this experiment we verify the effect of this fast memory size on algorithm performance.</text></s>
<s sid="479"><CoreSc1 advantage="None" conceptID="Obj37" novelty="None" type="Obj"/><text>We set the fast memory size on our machine and measure its effect.</text></s>
<s sid="480"><CoreSc1 advantage="None" conceptID="Obs20" novelty="None" type="Obs"/><text>Fig. 6 illustrates how this change has an impact on speedup across a range of threads/core.</text></s>
<s sid="481"><CoreSc1 advantage="None" conceptID="Res149" novelty="None" type="Res"/><text>For a fixed Z (fast memory size), the maximum sub-block size B can be determined.</text></s>
<s sid="482"><CoreSc1 advantage="None" conceptID="Res150" novelty="None" type="Res"/><text>Then, varying thread counts has the same effect as previously illustrated in Fig. 3, increasing threads/core increases performance until the PRAM range is reached.</text></s>
<s sid="483"><CoreSc1 advantage="None" conceptID="Con171" novelty="None" type="Con"/><text>But as we can see from the figure, different block sizes have different performance for the same number of threads/core.</text></s>
<s sid="484"><CoreSc1 advantage="None" conceptID="Con172" novelty="None" type="Con"/><text>This effect is predicted by Eq. (10).</text></s>
<s sid="485"><CoreSc1 advantage="None" conceptID="Con173" novelty="None" type="Con"/><text>As we increase the size of local memory, the performance improves, since we can use bigger blocks.</text></s>
<s sid="486"><CoreSc1 advantage="None" conceptID="Res151" novelty="None" type="Res"/><text>In order to isolate the effect of block size from the effects of other parameters, we also plot this data in a pair of different formats in Figs. 7 and 8, in both cases limiting the number of threads/core to below the PRAM range (i.e., the range where speedup is linear in threads/core).</text></s>
<s sid="487"><CoreSc1 advantage="None" conceptID="Res152" novelty="None" type="Res"/><text>The first curve shows the difference between the speedups for different block sizes.</text></s>
<s sid="488"><CoreSc1 advantage="None" conceptID="Res153" novelty="None" type="Res"/><text>As the curve indicates, the delta speedup increases linearly with the number of threads/core, consistent with the model prediction of (B1-B2)T.</text></s>
<s sid="489"><CoreSc1 advantage="None" conceptID="Res154" novelty="None" type="Res"/><text>The second curve shows the ratio of the performance of block size 64 to block size 32, indicating a flat line, since the thread term cancels out.</text></s>
<s sid="490"><CoreSc1 advantage="None" conceptID="Con174" novelty="None" type="Con"/><text>Comparison between the dynamic programming and Johnson's algorithms</text></s>
<s sid="491"><CoreSc1 advantage="None" conceptID="Con175" novelty="None" type="Con"/><text>It is interesting to compare the dynamic programming algorithm and Johnson's algorithm with arrays, since the PRAM and the TMM model differ in predicting the relative performance of these algorithms.</text></s>
<s sid="492"><CoreSc1 advantage="None" conceptID="Con176" novelty="None" type="Con"/><text>The PRAM model predicts that Johnson's algorithm should always be better.</text></s>
<s sid="493"><CoreSc1 advantage="None" conceptID="Con177" novelty="None" type="Con"/><text>However, from Section 5.2, for a small number of threads/core working on a dense graph, the TMM model predicts that dynamic programming may be better.</text></s>
<s sid="494"><CoreSc1 advantage="None" conceptID="Con178" novelty="None" type="Con"/><text>For the graphs with 8k vertices that we explored earlier, lgn&lt;Z.</text></s>
<s sid="495"><CoreSc1 advantage="None" conceptID="Con179" novelty="None" type="Con"/><text>Consequently, TMM predicts Johnson's algorithm is generally faster than dynamic programming for sparse graphs, but slower for relatively dense ones.</text></s>
<s sid="496"><CoreSc1 advantage="None" conceptID="Con180" novelty="None" type="Con"/><text>Fig. 9 demonstrates this effect concretely.</text></s>
<s sid="497"><CoreSc1 advantage="None" conceptID="Con181" novelty="None" type="Con"/><text>In addition, for the dense graph, the figure also shows the intersection between the runtime curves of the two algorithms.</text></s>
<s sid="498"><CoreSc1 advantage="None" conceptID="Con182" novelty="None" type="Con"/><text>At that point (32 threads/core), dynamic programming has already been in the PRAM range with stable performance since 16 threads/core, while Johnson's has not.</text></s>
<s sid="499"><CoreSc1 advantage="None" conceptID="Con183" novelty="None" type="Con"/><text>Its runtime is still benefiting by increasing the threads/core.</text></s>
<s sid="500"><CoreSc1 advantage="None" conceptID="Con184" novelty="None" type="Con"/><text>As a result, we predict that Johnson's runtime will flip to be the better one if given sufficient threads.</text></s>
<s sid="501"><CoreSc1 advantage="None" conceptID="Con185" novelty="None" type="Con"/><text>The peak performance of Johnson's being better than that of dynamic programming is consistent with what the PRAM model predicts.</text></s>
Conclusions
<s sid="502"><CoreSc1 advantage="None" conceptID="Con186" novelty="None" type="Con"/><text>In this paper, we present a memory access model, called the Threaded Many-core Memory (TMM) model, that is well suited for modern highly-threaded, many-core systems that employ many threads and fast context switching to hide memory latency.</text></s>
<s sid="503"><CoreSc1 advantage="None" conceptID="Con187" novelty="None" type="Con"/><text>The model analyzes the significant factors that affect performance on many-core machines.</text></s>
<s sid="504"><CoreSc1 advantage="None" conceptID="Con188" novelty="None" type="Con"/><text>In particular, it requires the work and depth (like PRAM algorithms), but also requires the analysis of the number of memory accesses.</text></s>
<s sid="505"><CoreSc1 advantage="None" conceptID="Con189" novelty="None" type="Con"/><text>Using these three values, we can properly order algorithms from slow to fast for many different settings of machine parameters on highly-threaded, many-core machines.</text></s>
<s sid="506"><CoreSc1 advantage="None" conceptID="Obj38" novelty="None" type="Obj"/><text>We analyzed 4 shortest paths algorithms in the TMM model and compared the analysis with the PRAM analysis.</text></s>
<s sid="507"><CoreSc1 advantage="None" conceptID="Con190" novelty="None" type="Con"/><text>We find that algorithms with the same PRAM performance can have different TMM performance under certain machine parameter settings.</text></s>
<s sid="508"><CoreSc1 advantage="None" conceptID="Con191" novelty="None" type="Con"/><text>In addition, for certain problem sizes which fit in local memory, algorithms which are faster on PRAM may be slower under the TMM model.</text></s>
<s sid="509"><CoreSc1 advantage="None" conceptID="Con192" novelty="None" type="Con"/><text>Further, we implemented a pair of the algorithms and showed empirical performance is effectively predicted by the TMM model under a variety of circumstances.</text></s>
<s sid="510"><CoreSc1 advantage="None" conceptID="Con193" novelty="None" type="Con"/><text>Therefore, TMM is a model well-suited to compare algorithms and decide which one to implement under particular environments.</text></s>
<s sid="511"><CoreSc1 advantage="None" conceptID="Con194" novelty="None" type="Con"/><text>To our knowledge, this is the first attempt to formalize the analysis of algorithms for highly-threaded, many-core computers using a formal model and asymptotic analysis.</text></s>
<s sid="512"><CoreSc1 advantage="None" conceptID="Con195" novelty="None" type="Con"/><text>There are many directions of future work.</text></s>
<s sid="513"><CoreSc1 advantage="None" conceptID="Con196" novelty="None" type="Con"/><text>One obvious direction is to design more algorithms under the TMM model.</text></s>
<s sid="514"><CoreSc1 advantage="None" conceptID="Con197" novelty="None" type="Con"/><text>Ideally, this model can help us come up with new algorithms for highly-threaded, many-core machines.</text></s>
<s sid="515"><CoreSc1 advantage="None" conceptID="Con198" novelty="None" type="Con"/><text>Empirical validation of the TMM model across a wider number of physical machines and manufacturers is also worth doing.</text></s>
<s sid="516"><CoreSc1 advantage="None" conceptID="Con199" novelty="None" type="Con"/><text>In addition, our current model only incorporates 2 levels of memory hierarchy.</text></s>
<s sid="517"><CoreSc1 advantage="None" conceptID="Con200" novelty="None" type="Con"/><text>While in this paper we assume that it is global memory vs. memory local to core groups, in principle, it can be any two levels of fast and slow memory.</text></s>
<s sid="518"><CoreSc1 advantage="None" conceptID="Con201" novelty="None" type="Con"/><text>We would like to extend it to multi-level hierarchies which are becoming increasingly common.</text></s>
<s sid="519"><CoreSc1 advantage="None" conceptID="Con202" novelty="None" type="Con"/><text>One way to do this is to design a &quot;parameter-oblivious&quot; model where algorithms do not know the machine parameters.</text></s>
<s sid="520"><CoreSc1 advantage="None" conceptID="Con203" novelty="None" type="Con"/><text>Other than the dynamic programming algorithm, all of the algorithms presented in this paper are, in fact, parameter-oblivious.</text></s>
<s sid="521"><CoreSc1 advantage="None" conceptID="Con204" novelty="None" type="Con"/><text>And matrix multiplication in the dynamic programming can easily be made parameter-oblivious.</text></s>
<s sid="522"><CoreSc1 advantage="None" conceptID="Con205" novelty="None" type="Con"/><text>In this case, the algorithms should perform well under all settings of parameters, allowing us to apply the model at any two levels and get the same results.</text></s>
</BODY>
<OTHER>
Acknowledgments
This work was supported by NSF grants CNS-0905368 and CNS-0931693 and Exegy, Inc.

</OTHER>
</PAPER>