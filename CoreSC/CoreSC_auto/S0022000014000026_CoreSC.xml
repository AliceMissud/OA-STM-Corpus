<?xml version="1.0" ?><PAPER><mode2 hasDoc="yes" name="S0022000014000026.tmf1" version="elsevier"/>
<TITLE>Rigorously modeling self-stabilizing fault-tolerant circuits: An ultra-robust clocking scheme for systems-on-chip
</TITLE>
<ABSTRACT>

Abstract
<s sid="1"><CoreSc1 advantage="None" conceptID="Obj1" novelty="None" type="Obj"/><text>We present the first implementation of a distributed clock generation scheme for Systems-on-Chip that recovers from an unbounded number of arbitrary transient faults despite a large number of arbitrary permanent faults.</text></s>
<s sid="2"><CoreSc1 advantage="None" conceptID="Met1" novelty="None" type="Met"/><text>We devise self-stabilizing hardware building blocks and a hybrid synchronous/asynchronous state machine enabling metastability-free transitions of the algorithm's states.</text></s>
<s sid="3"><CoreSc1 advantage="None" conceptID="Met2" novelty="None" type="Met"/><text>We provide a comprehensive modeling approach that permits to prove, given correctness of the constructed low-level building blocks, the high-level properties of the synchronization algorithm (which have been established in a more abstract model).</text></s>
<s sid="4"><CoreSc1 advantage="None" conceptID="Obj2" novelty="None" type="Obj"/><text>We believe this approach to be of interest in its own right, since this is the first technique permitting to mathematically verify, at manageable complexity, high-level properties of a fault-prone system in terms of its very basic components.</text></s>
<s sid="5"><CoreSc1 advantage="None" conceptID="Obj3" novelty="None" type="Obj"/><text>We evaluate a prototype implementation, which has been designed in VHDL, using the Petrify tool in conjunction with some extensions, and synthesized for an Altera Cyclone FPGA.</text></s>
Highlights
•
<s sid="6"><CoreSc1 advantage="None" conceptID="Obj4" novelty="None" type="Obj"/><text>We introduce a novel modeling framework for fault-tolerant VLSI circuits.</text></s>
•
<s sid="7"><CoreSc1 advantage="None" conceptID="Obj5" novelty="None" type="Obj"/><text>We cast a self-stabilizing clocking scheme from a companion article in this model.</text></s>
•
<s sid="8"><CoreSc1 advantage="None" conceptID="Obj6" novelty="None" type="Obj"/><text>We discuss the implications of theory and model for the resulting implementation.</text></s>
•
<s sid="9"><CoreSc1 advantage="None" conceptID="Obj7" novelty="None" type="Obj"/><text>We present the measures taken to avoid metastable upsets despite faults.</text></s>
•
<s sid="10"><CoreSc1 advantage="None" conceptID="Obj8" novelty="None" type="Obj"/><text>We provide experimental data from a prototype FPGA implementation of the algorithm.</text></s>
</ABSTRACT>
<BODY>

Introduction &amp; related work
<s sid="11"><CoreSc1 advantage="None" conceptID="Obj9" novelty="None" type="Obj"/><text>In the past, computers have essentially been viewed as monolithic, synchronous, fault-free systems.</text></s>
<s sid="12"><CoreSc1 advantage="None" conceptID="Met3" novelty="None" type="Met"/><text>If at all, fault-tolerance has been introduced (i) to deal with limited, specific failures (e.g. errors in communication or data read from storage, which are usually handled via error-correcting codes), and (ii) at the level of distributed systems comprised of multiple machines that are fault-prone or subject to attacks (e.g. data centers or peer-to-peer applications, which use some form of replication).</text></s>
<s sid="13"><CoreSc1 advantage="None" conceptID="Mot1" novelty="None" type="Mot"/><text>Except for critical systems and extreme operational conditions (e.g. medical or aerospace applications [1]), there has been little motivation to build systems that are robust on all levels from scratch, a process that involves redesigning-or even reinventing-the very basics of how computations are organized and performed.</text></s>
<s sid="14"><CoreSc1 advantage="None" conceptID="Bac1" novelty="None" type="Bac"/><text>Due to the tremendous advances of Very Large Scale Integration (VLSI) technology, this situation has changed.</text></s>
<s sid="15"><CoreSc1 advantage="None" conceptID="Res1" novelty="None" type="Res"/><text>Enabled by ever decreasing feature sizes and supply voltages, modern circuits nowadays accommodate billions of transistors running at GHz speeds [2].</text></s>
<s sid="16"><CoreSc1 advantage="None" conceptID="Res2" novelty="None" type="Res"/><text>As a consequence, the assumption of chip-global (not to speak of system-global) synchrony [3] and no (or restricted) faults gradually became outdated [4].</text></s>
<s sid="17"><CoreSc1 advantage="None" conceptID="Bac2" novelty="None" type="Bac"/><text>Improved process technology and architectural-level fault-tolerance measures are common nowadays, and the lack of global synchrony has been tackled by accepting a certain level of asynchrony between different parts of the system.</text></s>
<s sid="18"><CoreSc1 advantage="None" conceptID="Met4" novelty="None" type="Met"/><text>In the most extreme form of this approach, computations are completely unsynchronized at all levels [5], which requires to synchronize all dependent activities (like sending and receiving of data) explicitly via handshaking.</text></s>
<s sid="19"><CoreSc1 advantage="None" conceptID="Met5" novelty="None" type="Met"/><text>In contrast, Globally Asynchronous Locally Synchronous (GALS) systems [6] make use of local clock sources to drive synchronous computations within each clock domain.</text></s>
<s sid="20"><CoreSc1 advantage="None" conceptID="Con1" novelty="None" type="Con"/><text>Note that, in the wider sense, most multiprocessors fall into this category, as there is usually no single common clock that drives all processors.</text></s>
<s sid="21"><CoreSc1 advantage="None" conceptID="Con2" novelty="None" type="Con"/><text>GALS systems again can be divided into two general classes: One that operates asynchronously at the inter-domain level, and the other consisting of multi-synchronous systems [7,8] that provide some, albeit reduced, degree of synchronization among clock domains.</text></s>
<s sid="22"><CoreSc1 advantage="None" conceptID="Con3" novelty="None" type="Con"/><text>The former class suffers from the drawback that, for inter-domain communication, either strong synchronizers or stoppable clocks must be foreseen [9].</text></s>
<s sid="23"><CoreSc1 advantage="None" conceptID="Met6" novelty="None" type="Met"/><text>After all, every bit of the sender's data must have stabilized at the receiver before the clock edge used for reading the data occurs.</text></s>
<s sid="24"><CoreSc1 advantage="None" conceptID="Met7" novelty="None" type="Met"/><text>This is avoided in multi-synchronous systems, where high-speed inter-domain communication via FIFO buffers can be implemented due to the available global synchronization [10].</text></s>
<s sid="25"><CoreSc1 advantage="None" conceptID="Met8" novelty="None" type="Met"/><text>Since the latter abstraction is also very useful for other purposes, multi-synchronous GALS is preferable from the viewpoint of a system-level designer.</text></s>
<s sid="26"><CoreSc1 advantage="None" conceptID="Mot2" novelty="None" type="Mot"/><text>Naturally, establishing inter-domain synchronization comes at additional costs.</text></s>
<s sid="27"><CoreSc1 advantage="None" conceptID="Mot3" novelty="None" type="Mot"/><text>While it is not too difficult to achieve and maintain in the absence of faults [11,12], the issue becomes highly challenging once faults of clocking system components enter the picture.</text></s>
Contribution
<s sid="28"><CoreSc1 advantage="None" conceptID="Obj10" novelty="None" type="Obj"/><text>We present an FPGA prototype implementation of a distributed clock generation scheme for SoC that self-stabilizes in the presence of up to f&lt;n/3 faulty nodes.</text></s>
<s sid="29"><CoreSc1 advantage="None" conceptID="Met9" novelty="None" type="Met"/><text>It incorporates the pulse algorithm from [13] that tolerates arbitrary clock drifts and allows for deterministic recovery and (re)joining in constant time if n-f nodes are synchronized; it stabilizes within time O(n) with probability 1-2-n from any arbitrary state.</text></s>
<s sid="30"><CoreSc1 advantage="None" conceptID="Met10" novelty="None" type="Met"/><text>An additional algorithmic layer that interacts weakly with the former provides bounded high-frequency clocks atop of it.</text></s>
<s sid="31"><CoreSc1 advantage="None" conceptID="Met11" novelty="None" type="Met"/><text>Nodes executing the compound algorithm broadcast a mere constant number of bits in constant time.</text></s>
<s sid="32"><CoreSc1 advantage="None" conceptID="Met12" novelty="None" type="Met"/><text>The formal proofs of the properties of the pulse synchronization algorithm and the derived high-frequency clocks are given in [13].</text></s>
<s sid="33"><CoreSc1 advantage="None" conceptID="Met13" novelty="None" type="Met"/><text>Deriving an implementation from the specification of the algorithm in [13] proved to be challenging, as the high-level theoretical model and formulation of the algorithm in [13] abstracts away many details.</text></s>
<s sid="34"><CoreSc1 advantage="None" conceptID="Bac3" novelty="None" type="Bac"/><text>Firstly, it assumes a number of basic self-stabilizing modules above the level of gates and wires to be given.</text></s>
<s sid="35"><CoreSc1 advantage="None" conceptID="Obj11" novelty="None" type="Obj"/><text>We devise and discuss self-stabilizing implementations of these building blocks meeting the specifications required by the high-level algorithm.</text></s>
<s sid="36"><CoreSc1 advantage="None" conceptID="Obj12" novelty="None" type="Obj"/><text>Secondly, the algorithm's description is in terms of state machines performing transitions that are non-trivial in the sense that they do not consist of switching a single binary signal or memory bit.</text></s>
<s sid="37"><CoreSc1 advantage="None" conceptID="Obj13" novelty="None" type="Obj"/><text>This requires careful consideration of metastability issues, since these state transitions are triggered by information from different clock domains.</text></s>
<s sid="38"><CoreSc1 advantage="None" conceptID="Obj14" novelty="None" type="Obj"/><text>In order to resolve this issue, we introduce a generic Hybrid State Transition Machine (HSTM) that asynchronously starts a local synchronous execution of a state transition satisfying the model specification from [13].</text></s>
<s sid="39"><CoreSc1 advantage="None" conceptID="Obj15" novelty="None" type="Obj"/><text>Related to this matter, we thirdly discuss in detail how the algorithm and its implementation make a best effort to guard against metastable upsets.</text></s>
<s sid="40"><CoreSc1 advantage="None" conceptID="Obj16" novelty="None" type="Obj"/><text>Here, we try to get the best out of the design decisions and rely on synchronizers only where absolutely necessary.</text></s>
<s sid="41"><CoreSc1 advantage="None" conceptID="Obj17" novelty="None" type="Obj"/><text>These non-trivial implementation issues and the complex interactions between the basic building blocks raise the question under which circumstances the high-level properties of the algorithm shown in [13] indeed hold for the presented implementation.</text></s>
<s sid="42"><CoreSc1 advantage="None" conceptID="Goa1" novelty="None" type="Goa"/><text>To answer this question, we devised a model that is able to capture the behavior of the constructed modules, including faults, resilience to faults, and self-stabilization, in a hierarchical fashion.</text></s>
<s sid="43"><CoreSc1 advantage="None" conceptID="Goa2" novelty="None" type="Goa"/><text>By specifying the desired behavior of modules in terms of the feasible output generated in response to their inputs, we can also reason about the behavior of (implementations of) modules in a hierarchical manner.</text></s>
<s sid="44"><CoreSc1 advantage="None" conceptID="Con4" novelty="None" type="Con"/><text>This property is crucial, as it permits to determine conditions under which our implementation indeed satisfies the requirements by the abstract model used in [13], and then soundly conclude that if these conditions are met, all statements made in [13] apply to our implementation.</text></s>
<s sid="45"><CoreSc1 advantage="None" conceptID="Goa3" novelty="None" type="Goa"/><text>Since our approach is highly generic and permits to adjust the granularity of the description in order to focus on specific aspects of the system, we believe it to be of general and independent interest in the context of devising fault-tolerant systems.</text></s>
<s sid="46"><CoreSc1 advantage="None" conceptID="Goa4" novelty="None" type="Goa"/><text>In order to verify the predictions from theory,33</text></s>
<s sid="47"><CoreSc1 advantage="None" conceptID="Res3" novelty="None" type="Res"/><text>Or, to be scientifically accurate, we rather successfully failed at falsifying them.</text></s>
<s sid="48"><CoreSc1 advantage="None" conceptID="Res4" novelty="None" type="Res"/><text>Our implementation primarily serves as a proof of concept, as clearly an FPGA implementation can merely hint at the properties of an ASIC.</text></s>
<s sid="49"><CoreSc1 advantage="None" conceptID="Met14" novelty="None" type="Met"/><text>we carried out several experiments incorporating drifting clocks, varying delays, and both transient and permanent faults.</text></s>
<s sid="50"><CoreSc1 advantage="None" conceptID="Met15" novelty="None" type="Met"/><text>This necessitated the development of a testbed that can be efficiently controlled and set up for executing a large number of test runs quickly.</text></s>
<s sid="51"><CoreSc1 advantage="None" conceptID="Res5" novelty="None" type="Res"/><text>In our 8-node prototype implementation, the compound algorithm generates 8-bit clocks that in all runs stabilized within 1.9⋅106d time (where d is the maximal end-to-end communication delay).</text></s>
<s sid="52"><CoreSc1 advantage="None" conceptID="Res6" novelty="None" type="Res"/><text>In our testbed, which runs at roughly 100 kHz, this amounts to less than 12 s.</text></s>
<s sid="53"><CoreSc1 advantage="None" conceptID="Res7" novelty="None" type="Res"/><text>For a system running at GHz speed, this translates to about a millisecond.</text></s>
<s sid="54"><CoreSc1 advantage="None" conceptID="Res8" novelty="None" type="Res"/><text>We also observed that the deterministic stabilization mechanism designed for more benign conditions operates as expected, recovering nodes by about two orders of magnitude faster.</text></s>
Organization of the article
<s sid="55"><CoreSc1 advantage="None" conceptID="Obj18" novelty="None" type="Obj"/><text>In the next section, we summarize the obstacles and design goals that need to be considered for clock synchronization in our setting; we also introduce the basic building blocks assumed in [13], which perform typical operations used by fault-tolerant synchronization algorithms.</text></s>
<s sid="56"><CoreSc1 advantage="None" conceptID="Obj19" novelty="None" type="Obj"/><text>Section 3 introduces the formal model, alongside illustrating examples and proofs of some basic properties.</text></s>
<s sid="57"><CoreSc1 advantage="None" conceptID="Obj20" novelty="None" type="Obj"/><text>Subsequently, in Section 4 we cast the modules informally discussed earlier in our formal framework, and interpret nodes, protocols, and the synchronization problem as modules as well.</text></s>
<s sid="58"><CoreSc1 advantage="None" conceptID="Obj21" novelty="None" type="Obj"/><text>In Section 5, we move on to the description of the algorithm from [13] in terms of this framework.</text></s>
<s sid="59"><CoreSc1 advantage="None" conceptID="Obj22" novelty="None" type="Obj"/><text>We provide high-level intution on the purpose of its various components and summarize the main statements proved in [13].</text></s>
<s sid="60"><CoreSc1 advantage="None" conceptID="Obj23" novelty="None" type="Obj"/><text>Section 6 follows up with presenting our implementations of the basic modules specified in Section 2.2, including the HSTM.</text></s>
<s sid="61"><CoreSc1 advantage="None" conceptID="Obj24" novelty="None" type="Obj"/><text>In this context, we will also cover our efforts to minimize the probability for metastable upsets.</text></s>
<s sid="62"><CoreSc1 advantage="None" conceptID="Obj25" novelty="None" type="Obj"/><text>In Section 7 we describe the testbed setup, the experiments, and their results.</text></s>
<s sid="63"><CoreSc1 advantage="None" conceptID="Goa5" novelty="None" type="Goa"/><text>Finally, in Section 8 we evaluate to what extent our design goals are met and give an outlook on future work.</text></s>
On-chip clock synchronization
<s sid="64"><CoreSc1 advantage="None" conceptID="Goa6" novelty="None" type="Goa"/><text>Our goal is to design a scalable hardware clock generation scheme that is resilient to arbitrary transient and permanent faults and carefully minimizes the risk of metastability.</text></s>
<s sid="65"><CoreSc1 advantage="None" conceptID="Goa7" novelty="None" type="Goa"/><text>We will now discuss our objectives in more detail and explain why tackling them in conjunction proves to be much harder than achieving them individually.</text></s>
<s sid="66"><CoreSc1 advantage="None" conceptID="Goa8" novelty="None" type="Goa"/><text>In accordance with standard notions, in the following we will refer to clock domains as nodes, as they represent the smallest &quot;independent&quot; algorithmic building block we use.</text></s>
<s sid="67"><CoreSc1 advantage="None" conceptID="Goa9" novelty="None" type="Goa"/><text>This is to be understood in the sense that we consider a node faulty if any one of its components is faulty, and non-faulty otherwise (irrespectively of whether other nodes behave correctly or not).</text></s>
<s sid="68"><CoreSc1 advantage="None" conceptID="Mod1" novelty="None" type="Mod"/><text>Denoting by [i..j] the set {k∈N|i⩽k⩽j}, ultimately, each correct node i∈[1..n] must at all times t output a (discrete) logical clock Li(t)∈N that fulfills certain properties despite the aforementioned obstacles; most obviously, we strive for minimizing maxi,j∈[1..n],t⩾0{Li(t)-Lj(t)}.</text></s>
Challenges
<s sid="69"><CoreSc1 advantage="None" conceptID="Bac4" novelty="None" type="Bac"/><text>Inexact local clocks and unknown message delays</text></s>
<s sid="70"><CoreSc1 advantage="None" conceptID="Bac5" novelty="None" type="Bac"/><text>When synchronizing clocks, one needs to face that clocks are not perfect and that it cannot be exactly determined how much time it takes to communicate a clock reading.</text></s>
<s sid="71"><CoreSc1 advantage="None" conceptID="Bac6" novelty="None" type="Bac"/><text>These fundamental uncertainties entail that synchronization can never be perfectly accurate and must be an ongoing process [14].</text></s>
<s sid="72"><CoreSc1 advantage="None" conceptID="Bac7" novelty="None" type="Bac"/><text>We formalize these notions as follows.</text></s>
<s sid="73"><CoreSc1 advantage="None" conceptID="Bac8" novelty="None" type="Bac"/><text>Each node i∈[1..n] can make use of local clocks that are inexact and therefore drift (i.e., do not progress at the same rate).</text></s>
<s sid="74"><CoreSc1 advantage="None" conceptID="Obj26" novelty="None" type="Obj"/><text>Since we are only concerned with synchronizing clock domains with each other, we do not care about Newtonian time.</text></s>
<s sid="75"><CoreSc1 advantage="None" conceptID="Obj27" novelty="None" type="Obj"/><text>Instead, we describe the system in terms of a reference time satisfying that any correctly operating clock progresses at a speed between 1 and some constant ϑ with respect to the reference time t∈R.</text></s>
<s sid="76"><CoreSc1 advantage="None" conceptID="Obj28" novelty="None" type="Obj"/><text>A (local) clock C:R→R that is correct during a period of reference time [t-,t+]⊆R guarantees that ∀t, t′∈[t-,t+], t&lt;t′: t′-t⩽C(t′)-C(t)⩽ϑ(t′-t) (in particular, C is continuous and strictly increasing during [t-,t+]).44</text></s>
<s sid="77"><CoreSc1 advantage="None" conceptID="Obj29" novelty="None" type="Obj"/><text>We use real-valued, unbounded clocks here to simplify the presentation.</text></s>
<s sid="78"><CoreSc1 advantage="None" conceptID="Mot4" novelty="None" type="Mot"/><text>It will later become clear that the algorithm can indeed operate with discrete bounded clocks, as it does not need to access absolute clock values, but rather approximately measures bounded differences in time.</text></s>
<s sid="79"><CoreSc1 advantage="None" conceptID="Goa10" novelty="None" type="Goa"/><text>In contrast to many &quot;traditional&quot; synchronization settings, we would like to tolerate quite large relative clock drifts ϑ-1 of up to about 20%, as accurate and stable oscillators are not available in a System-on-Chip (SoC) at low costs.</text></s>
<s sid="80"><CoreSc1 advantage="None" conceptID="Met16" novelty="None" type="Met"/><text>Tolerating such large drifts permits to utilize very simple ring oscillators even under heavily varying conditions (temperature, supply voltage, etc.) [15].</text></s>
<s sid="81"><CoreSc1 advantage="None" conceptID="Met17" novelty="None" type="Met"/><text>Node i communicates with node j via an abstract FIFO channel that (if correct) continuously makes i's state available to j, albeit delayed by an unknown value between 0 and the maximal delay d.</text></s>
<s sid="82"><CoreSc1 advantage="None" conceptID="Met18" novelty="None" type="Met"/><text>We denote the input port of the channel from node i to node j by Si and its output port by Sj,i.</text></s>
<s sid="83"><CoreSc1 advantage="None" conceptID="Met19" novelty="None" type="Met"/><text>Node i also loops back its own state to itself on a channel.</text></s>
<s sid="84"><CoreSc1 advantage="None" conceptID="Met20" novelty="None" type="Met"/><text>The time required for computations that are triggered by some communicated information is accounted for by d as well, i.e., d is an end-to-end delay.55</text></s>
<s sid="85"><CoreSc1 advantage="None" conceptID="Obj30" novelty="None" type="Obj"/><text>This is the reason why we speak of an abstract channel.</text></s>
<s sid="86"><CoreSc1 advantage="None" conceptID="Obj31" novelty="None" type="Obj"/><text>We will later introduce the (physical) channels that essentially represent the wires on the chip; the maximal delay d is then the sum of the maximal delay of the physical channels and the computing elements of the nodes.</text></s>
<s sid="87"><CoreSc1 advantage="None" conceptID="Mod2" novelty="None" type="Mod"/><text>For the sake of a straightforward presentation, throughout this article we assume that all channels from node i to some node j are part of node i, i.e., faults of the channel are mapped to the sender node.</text></s>
<s sid="88"><CoreSc1 advantage="None" conceptID="Bac9" novelty="None" type="Bac"/><text>We remark, however, that a more detailed treatment (as e.g.</text></s>
<s sid="89"><CoreSc1 advantage="None" conceptID="Bac10" novelty="None" type="Bac"/><text>in [16]) can be beneficial and is supported by the modeling framework underlying this work.</text></s>
Transient faults
<s sid="90"><CoreSc1 advantage="None" conceptID="Bac11" novelty="None" type="Bac"/><text>Increasing soft error rates of modern VLSI circuits [17], originating in ionizing radiation [18-21], cross-talk, and ground bouncing [22,23], make it vital to allow for recovery from transient faults.</text></s>
<s sid="91"><CoreSc1 advantage="None" conceptID="Bac12" novelty="None" type="Bac"/><text>The most extreme transient fault scenario is that the entire system undergoes a period of an unbounded number of arbitrary faults.66</text></s>
<s sid="92"><CoreSc1 advantage="None" conceptID="Bac13" novelty="None" type="Bac"/><text>The only restriction is that transient faults do not affect the non-volatile memory (and in particular not the algorithm itself), as this would induce a permanent fault.</text></s>
<s sid="93"><CoreSc1 advantage="None" conceptID="Bac14" novelty="None" type="Bac"/><text>Algorithms that are capable of re-establishing regular operation after transient faults cease are called self-stabilizing [24].</text></s>
<s sid="94"><CoreSc1 advantage="None" conceptID="Bac15" novelty="None" type="Bac"/><text>This requirement is equivalent to stating that, if the system is fault-free, the algorithm converges to a valid state from an arbitrary initial configuration within a bounded time; we refer to this period as stabilization time.</text></s>
<s sid="95"><CoreSc1 advantage="None" conceptID="Mot5" novelty="None" type="Mot"/><text>Due to this equivalency, self-stabilizing algorithms have the additional advantage of requiring no initialization, i.e., a self-stabilizing clocking system does not need to be booted with any initial synchrony.</text></s>
<s sid="96"><CoreSc1 advantage="None" conceptID="Mot6" novelty="None" type="Mot"/><text>For self-stabilizing algorithms, stabilization time is obviously an important quality measure.</text></s>
<s sid="97"><CoreSc1 advantage="None" conceptID="Mot7" novelty="None" type="Mot"/><text>As the fundamental time unit of the system is d, i.e., the time span it takes to effectively communicate and process any piece of information with certainty, guarantees on the stabilization time are clearly always some multiple of d; the respective prefactor typically is a function of the number of nodes n, the number of sustainable or actual permanent faults, and the clock drift ϑ.</text></s>
<s sid="98"><CoreSc1 advantage="None" conceptID="Mot8" novelty="None" type="Mot"/><text>In our context, the stabilization time is not only of relevance to whether waiting for stabilization is bearable in terms of the down-time of the system; it is important to understand that a failure of the synchronization layer will quickly result in incoherencies of operations on higher layers, entailing the threat of data loss or corruption, potentially without any possibility of future recovery.</text></s>
<s sid="99"><CoreSc1 advantage="None" conceptID="Mot9" novelty="None" type="Mot"/><text>Because of the need of maintaining accurate synchronization in the presence of drifting clocks, quite a few clock synchronization algorithms are self-stabilizing.</text></s>
<s sid="100"><CoreSc1 advantage="None" conceptID="Bac16" novelty="None" type="Bac"/><text>In fact, conventional clock trees [3] are trivially self-stabilizing-after all, they simply disseminate the signal of a single oscillator throughout a chip.</text></s>
<s sid="101"><CoreSc1 advantage="None" conceptID="Bac17" novelty="None" type="Bac"/><text>However, they cannot cope with any permanent fault of the clock source or the network distributing the clock.</text></s>
<s sid="102"><CoreSc1 advantage="None" conceptID="Bac18" novelty="None" type="Bac"/><text>Similarly, one could easily make a system comprising several clock sources self-stabilizing, by picking one master clock and letting all other clocks synchronize to it.</text></s>
<s sid="103"><CoreSc1 advantage="None" conceptID="Bac19" novelty="None" type="Bac"/><text>Again, this simplistic approach will fail if the master or its outgoing communication channels become faulty.</text></s>
Permanent faults
<s sid="104"><CoreSc1 advantage="None" conceptID="Bac20" novelty="None" type="Bac"/><text>Sustaining functionality in the presence of permanent faults necessitates redundancy.</text></s>
<s sid="105"><CoreSc1 advantage="None" conceptID="Bac21" novelty="None" type="Bac"/><text>More precisely, it is known that tolerating f worst-case faults (traditionally called Byzantine faults in this context) is impossible if n⩽3f (without cryptographic assumptions) [14,25].77</text></s>
<s sid="106"><CoreSc1 advantage="None" conceptID="Bac22" novelty="None" type="Bac"/><text>Allowing cryptography would still necessitate n&gt;2f [26,27]; we hence discard this option due to the additional complexity incurred.</text></s>
<s sid="107"><CoreSc1 advantage="None" conceptID="Mot10" novelty="None" type="Mot"/><text>Hence, natural questions are whether assuming worst-case failures is too demanding and whether the fault model could be relaxed in order to circumvent the lower bound.</text></s>
<s sid="108"><CoreSc1 advantage="None" conceptID="Obj32" novelty="None" type="Obj"/><text>Unfortunately, examining the lower bound reveals that it originates in the ability of a faulty node to communicate conflicting information to different receivers.</text></s>
<s sid="109"><CoreSc1 advantage="None" conceptID="Hyp1" novelty="None" type="Hyp"/><text>This behavior can easily emerge from a faulty output stage in a circuit: If an analog voltage level in between the range for a valid &quot;1&quot; and that for a valid &quot;0&quot; is evaluated (for example due to a timing fault, a glitch on a signal line, or a defective driver output) by more than one receiver, some might read a &quot;1&quot; while others read a &quot;0&quot;.</text></s>
<s sid="110"><CoreSc1 advantage="None" conceptID="Hyp2" novelty="None" type="Hyp"/><text>Note that this is a fundamental problem, as mapping the continuous range of possible voltages to discrete binary values entails that there is always a critical threshold close to which it is impossible to ensure that all receivers observe the same binary value.</text></s>
<s sid="111"><CoreSc1 advantage="None" conceptID="Obj33" novelty="None" type="Obj"/><text>It is still an option to argue about the spatial distribution of (permanent) faults within the system, though, as we discuss in Section 8.</text></s>
<s sid="112"><CoreSc1 advantage="None" conceptID="Obj34" novelty="None" type="Obj"/><text>However, in this article, we consider the worst case, which also motivates the choice of full connectivity88</text></s>
<s sid="113"><CoreSc1 advantage="None" conceptID="Obj35" novelty="None" type="Obj"/><text>We are aware that this constitutes a serious scalability issue; again we refer to the discussion in Section 8.</text></s>
<s sid="114"><CoreSc1 advantage="None" conceptID="Res9" novelty="None" type="Res"/><text>between the nodes due to a respective impossibility result [26,27].</text></s>
<s sid="115"><CoreSc1 advantage="None" conceptID="Res10" novelty="None" type="Res"/><text>This lower bound entails that, due to their low connectivity, most existing distributed clock generation schemes [11,12,28,29] cannot cope with a reasonable number of worst-case faults.</text></s>
<s sid="116"><CoreSc1 advantage="None" conceptID="Res11" novelty="None" type="Res"/><text>Nonetheless, dealing with up to f faults in a fully connected system of n⩾3f+1 nodes is-at least from a high-level perspective-still fairly easy, provided that we can rely on synchronization already being established.</text></s>
<s sid="117"><CoreSc1 advantage="None" conceptID="Res12" novelty="None" type="Res"/><text>To illustrate this, consider the simple state machine of a node given in Fig. 1.</text></s>
<s sid="118"><CoreSc1 advantage="None" conceptID="Res13" novelty="None" type="Res"/><text>In the figure, the node's states are depicted in circles and the feasible state transitions are indicated by arrows.</text></s>
<s sid="119"><CoreSc1 advantage="None" conceptID="Bac23" novelty="None" type="Bac"/><text>A node switches, for example, from state ready to state propose if the condition next to the arrow is satisfied.</text></s>
<s sid="120"><CoreSc1 advantage="None" conceptID="Bac24" novelty="None" type="Bac"/><text>In this example, this means that either 3ϑ2d time has passed on its local clock since it switched to state ready or its incoming channels (including its loop-back channel) showed at least f+1 other nodes in state propose since it switched to state ready.</text></s>
<s sid="121"><CoreSc1 advantage="None" conceptID="Exp1" novelty="None" type="Exp"/><text>This behavior is realized by each node i∈[1..n] having (binary) memory flags proposei,j for each node j∈[1..n]: Node i's flag proposei,j is set to 1 at a time t iff Si,j(t)=propose and the flag was in state 0 before.</text></s>
<s sid="122"><CoreSc1 advantage="None" conceptID="Res14" novelty="None" type="Res"/><text>The flag is reset to 0 on node i's state transition to ready (in the figure indicated by the rectangular box on the respective arrow).</text></s>
<s sid="123"><CoreSc1 advantage="None" conceptID="Res15" novelty="None" type="Res"/><text>Deciding whether the transition condition is satisfied at time t thus boils down to checking whether the timeout condition is satisfied or at least f+1 of the propose memory flags are in state 1.</text></s>
<s sid="124"><CoreSc1 advantage="None" conceptID="Res16" novelty="None" type="Res"/><text>Now assume that each node runs a copy of this state machine, and at least n-f non-faulty nodes enter state increase during some time window [t,t+2d).</text></s>
<s sid="125"><CoreSc1 advantage="None" conceptID="Res17" novelty="None" type="Res"/><text>As local clocks run at speeds between 1 and ϑ, all nodes will switch to state ready during [t+3d,t+2d+3ϑd).</text></s>
<s sid="126"><CoreSc1 advantage="None" conceptID="Bac25" novelty="None" type="Bac"/><text>Hence, at the time when a node switches to ready, the delayed state information on the channels will not show non-faulty nodes in state propose any more.</text></s>
<s sid="127"><CoreSc1 advantage="None" conceptID="Hyp3" novelty="None" type="Hyp"/><text>Therefore, no non-faulty node will switch to propose again due to memorizing f+1 nodes (at least one of which must be non-faulty) in state propose before the first non-faulty node switches to propose.</text></s>
<s sid="128"><CoreSc1 advantage="None" conceptID="Met21" novelty="None" type="Met"/><text>Thus, the latter must happen because 3ϑ2d local time passed on a local clock, which takes at least until time t+3d+3ϑd&gt;t+2d+3ϑd.</text></s>
<s sid="129"><CoreSc1 advantage="None" conceptID="Met22" novelty="None" type="Met"/><text>By this time, all nodes will have switched to ready.</text></s>
<s sid="130"><CoreSc1 advantage="None" conceptID="Hyp4" novelty="None" type="Hyp"/><text>This implies that at the time when the first node switches to increase again (which eventually happens because all n-f non-faulty nodes switch to propose), all nodes will already have switched to ready.</text></s>
<s sid="131"><CoreSc1 advantage="None" conceptID="Hyp5" novelty="None" type="Hyp"/><text>Given that n⩾3f+1, we have that n-2f⩾f+1, i.e., if at some non-faulty node n-f channels show state propose, any node will observe f+1 channels in this state (though due to delayed communication maybe not at exactly the same instance in time).</text></s>
<s sid="132"><CoreSc1 advantage="None" conceptID="Met23" novelty="None" type="Met"/><text>This implies that at most d time after the first node switched to increase again, all non-faulty nodes have switched to propose.</text></s>
<s sid="133"><CoreSc1 advantage="None" conceptID="Met24" novelty="None" type="Met"/><text>Another d time later, all n-f non-faulty nodes will have become aware of this and have switched to increase, i.e., within a time window of 2d.</text></s>
<s sid="134"><CoreSc1 advantage="None" conceptID="Met25" novelty="None" type="Met"/><text>Repeating this reasoning inductively and assuming that the nodes increase their logical clocks (that initially are 0) by 1 whenever they switch to increase, well-synchronized logical clocks are obtained: The maximum difference in time between any two correct nodes performing their kth clock tick, the skew, is at most 2d for the above algorithm.</text></s>
<s sid="135"><CoreSc1 advantage="None" conceptID="Met26" novelty="None" type="Met"/><text>A variation of this simple technique [30] is known for long and a closely related approach called DARTS has been implemented in hardware [31,32].</text></s>
<s sid="136"><CoreSc1 advantage="None" conceptID="Met27" novelty="None" type="Met"/><text>However, all these algorithms are not self-stabilizing.</text></s>
<s sid="137"><CoreSc1 advantage="None" conceptID="Res18" novelty="None" type="Res"/><text>In fact, even if clocks would not drift, the delay d was arbitrarily small, and there was only a single faulty node (i.e., even if we allow for f&gt;1, only one node is actually faulty), they still would not stabilize.</text></s>
<s sid="138"><CoreSc1 advantage="None" conceptID="Res19" novelty="None" type="Res"/><text>To see this for the algorithm given in Fig. 1, first consider the following execution with n=3f+1, part of which is depicted in Fig. 2.</text></s>
<s sid="139"><CoreSc1 advantage="None" conceptID="Res20" novelty="None" type="Res"/><text>The correct nodes are split evenly, into three subsets Ai, i∈{1,2,3}, of size f.</text></s>
<s sid="140"><CoreSc1 advantage="None" conceptID="Res21" novelty="None" type="Res"/><text>Set A1 initially is in state ready, with all memory flags corresponding to nodes in A3 in state 1 and all other flags in state 0.</text></s>
<s sid="141"><CoreSc1 advantage="None" conceptID="Bac26" novelty="None" type="Bac"/><text>The nodes in A2 and A3 are in state increase, with the timers of nodes in A2 having progressed halfway towards expiring and the timers in A3 just started (i.e., these nodes just left propose), and their propose signals are memorized by nodes in A1.</text></s>
<s sid="142"><CoreSc1 advantage="None" conceptID="Bac27" novelty="None" type="Bac"/><text>Just when the nodes in A2 are about to switch to ready, the faulty node sends propose signals to the nodes in A1, causing them to switch to propose.</text></s>
<s sid="143"><CoreSc1 advantage="None" conceptID="Bac28" novelty="None" type="Bac"/><text>They will send propose signals, once receiving them memorize 2f+1=n-f nodes in state propose, and thus proceed to state increase.</text></s>
<s sid="144"><CoreSc1 advantage="None" conceptID="Bac29" novelty="None" type="Bac"/><text>However, the nodes in A2 will still observe the propose signals of the nodes in A1 after resetting their memory flags upon switching to ready.</text></s>
<s sid="145"><CoreSc1 advantage="None" conceptID="Obj36" novelty="None" type="Obj"/><text>Thus, we end up in the same situation, except that Ai (indices modulo 3) takes the role of Ai-1.</text></s>
<s sid="146"><CoreSc1 advantage="None" conceptID="Res22" novelty="None" type="Res"/><text>Repetition yields an execution that never stabilizes and has 3 sets of grossly desynchronized nodes that are not faulty.</text></s>
<s sid="147"><CoreSc1 advantage="None" conceptID="Res23" novelty="None" type="Res"/><text>This execution can be generalized to n=kf+1 for integers k⩾3: we split the correct nodes in k sets of size f and make them proceed equidistantly spread in time through the cycle.</text></s>
<s sid="148"><CoreSc1 advantage="None" conceptID="Con5" novelty="None" type="Con"/><text>The difference is that now more than one group will linger in states ready or propose upon arrival of the next; the crucial point is that the single faulty node retains control over when groups proceed to state increase.</text></s>
<s sid="149"><CoreSc1 advantage="None" conceptID="Bac30" novelty="None" type="Bac"/><text>The cases n=kf+2 and n=kf+3 require more involved constructions; it should be intuitive, though, that with 2 actually failing nodes the above construction can be modified to operate with one or two of the sets containing f+1 nodes.</text></s>
<s sid="150"><CoreSc1 advantage="None" conceptID="Bac31" novelty="None" type="Bac"/><text>Combining transient and permanent faults</text></s>
<s sid="151"><CoreSc1 advantage="None" conceptID="Res24" novelty="None" type="Res"/><text>Combining self-stabilization and resilience to permanent faults results in much more robust systems.</text></s>
<s sid="152"><CoreSc1 advantage="None" conceptID="Res25" novelty="None" type="Res"/><text>Both properties synergize in that, as long as at all times there is some sufficiently large set (not necessarily the same!) of nodes that is non-faulty, an arbitrary number of transient faults is transparently masked, i.e., the system remains operational even though over time each individual component may repeatedly undergo transient failures and recover from them.</text></s>
<s sid="153"><CoreSc1 advantage="None" conceptID="Res26" novelty="None" type="Res"/><text>This drastically increases the mean time until overall system failure: In a system that is not resilient to permanent faults, any fault will result in an immediate breakdown of guaranteed properties, whereas a system that is not self-stabilizing will fail (and might not recover without an external reboot) once the sum of faults exceeds one third of the nodes.99</text></s>
<s sid="154"><CoreSc1 advantage="None" conceptID="Met28" novelty="None" type="Met"/><text>One could compromise by guaranteeing that nodes recover in bounded time, provided that the number of faults is never overwhelming.</text></s>
<s sid="155"><CoreSc1 advantage="None" conceptID="Met29" novelty="None" type="Met"/><text>In fact, the algorithm presented in this article has the property that in this case nodes will recover faster and deterministically (in contrast to the slower, probabilistic stabilization from arbitrary system states).</text></s>
<s sid="156"><CoreSc1 advantage="None" conceptID="Met30" novelty="None" type="Met"/><text>However, sacrificing stabilization from arbitrary states will not reduce the complexity of the algorithm significantly, and theory strongly indicates that the respective gain is limited to a constant factor in general.</text></s>
<s sid="157"><CoreSc1 advantage="None" conceptID="Bac32" novelty="None" type="Bac"/><text>There is a considerable body of work on distributed synchronization algorithms that are self-stabilizing as well as resilient to permanent faults.</text></s>
<s sid="158"><CoreSc1 advantage="None" conceptID="Bac33" novelty="None" type="Bac"/><text>However, until recently, there has been no solution worth considering for hardware implementation.</text></s>
<s sid="159"><CoreSc1 advantage="None" conceptID="Met31" novelty="None" type="Met"/><text>Known algorithms exhibit a prohibitively large communication complexity (i.e., nodes send Ω(n) bits over each channel in constant time) [33,34], incur an exponential stabilization time [35], require exponentially small clock drifts [36], or require much stronger assumptions on the system's behavior [37].</text></s>
<s sid="160"><CoreSc1 advantage="None" conceptID="Met32" novelty="None" type="Met"/><text>Recently, we proposed an approach that does not suffer from such drawbacks [13,38,39], whose implementation is the subject of this work.</text></s>
Metastability
<s sid="161"><CoreSc1 advantage="None" conceptID="Res27" novelty="None" type="Res"/><text>In our specific setting, minimizing the potential for metastability is particularly demanding.</text></s>
<s sid="162"><CoreSc1 advantage="None" conceptID="Res28" novelty="None" type="Res"/><text>Metastability results from violating a stateful circuit's input timing constraints, e.g., by changing the data input of a flip-flop at the time of the clock transition.</text></s>
<s sid="163"><CoreSc1 advantage="None" conceptID="Res29" novelty="None" type="Res"/><text>While this can be safely avoided during normal operation, a faulty node might exhibit arbitrary timing and hence cause such a violation.</text></s>
<s sid="164"><CoreSc1 advantage="None" conceptID="Obj37" novelty="None" type="Obj"/><text>As this can never be prevented in the first place if worst-case faults are considered, it is mandatory to guard the channels against propagating metastability, e.g.</text></s>
by using synchronizers.
<s sid="165"><CoreSc1 advantage="None" conceptID="Goa11" novelty="None" type="Goa"/><text>In order to minimize the required length of synchronizer chains, decreasing latency and area consumption (the latter also on higher layers of the system), however, it is beneficial to avoid the potential for upsets by construction wherever possible.</text></s>
<s sid="166"><CoreSc1 advantage="None" conceptID="Met33" novelty="None" type="Met"/><text>Apart from the (unavoidable) threat originating from faulty nodes, safely preventing timing violations is hindered by the lack of a common time base during the stabilization phase after an excessive number of transient faults.</text></s>
<s sid="167"><CoreSc1 advantage="None" conceptID="Met34" novelty="None" type="Met"/><text>It has been shown that it is impossible to guarantee with certainty that no metastable upsets occur if the system is in an arbitrary initial state, even if all nodes adhere to the protocol [40].</text></s>
<s sid="168"><CoreSc1 advantage="None" conceptID="Met35" novelty="None" type="Met"/><text>Careful design is thus required in order to minimize the probability of upsets during stabilization, in particular since such upsets might obstruct the stabilization process.</text></s>
<s sid="169"><CoreSc1 advantage="None" conceptID="Met36" novelty="None" type="Met"/><text>Once the system stabilized, i.e., the non-faulty nodes are synchronized, the algorithm can use this synchronization to structure communication in a way that entirely avoids metastable upsets caused by non-faulty nodes.</text></s>
<s sid="170"><CoreSc1 advantage="None" conceptID="Con6" novelty="None" type="Con"/><text>Thus, in the absence of faults, we require that the system operates metastability-free.</text></s>
<s sid="171"><CoreSc1 advantage="None" conceptID="Con7" novelty="None" type="Con"/><text>Note that even this seemingly simple task is not trivial, as one cannot employ the classical wait-for-all paradigm: Doing so would imply that just a single non-responsive node would cause the entire system to deadlock.</text></s>
<s sid="172"><CoreSc1 advantage="None" conceptID="Con8" novelty="None" type="Con"/><text>Therefore, when depending on other nodes in the decision to take a state transition, it is necessary to wait for at most n-f signals.</text></s>
<s sid="173"><CoreSc1 advantage="None" conceptID="Met37" novelty="None" type="Met"/><text>Safely reading signals thus cannot rely on handshaking, but must be based on suitable monotonicity and/or timing conditions (guaranteed by the use of memory flags and local clocks, for example).</text></s>
<s sid="174"><CoreSc1 advantage="None" conceptID="Met38" novelty="None" type="Met"/><text>The bounded-delay &quot;interlocking condition&quot; used in DARTS [32] and the simple algorithm in Fig. 1 are showcases for such techniques.</text></s>
<s sid="175"><CoreSc1 advantage="None" conceptID="Mot11" novelty="None" type="Mot"/><text>Operating frequency vs. clock precision</text></s>
<s sid="176"><CoreSc1 advantage="None" conceptID="Mot12" novelty="None" type="Mot"/><text>In order to be practical, the logical clocks need to run at a frequency in the GHz range.</text></s>
<s sid="177"><CoreSc1 advantage="None" conceptID="Bac34" novelty="None" type="Bac"/><text>While one could obviously utilize frequency multiplication to achieve this goal, this is not straightforward to build in the self-stabilizing context.</text></s>
<s sid="178"><CoreSc1 advantage="None" conceptID="Bac35" novelty="None" type="Bac"/><text>After all, clock multipliers involve complex devices like phase-locked loops and are hence not obviously self-stabilizing.</text></s>
<s sid="179"><CoreSc1 advantage="None" conceptID="Bac36" novelty="None" type="Bac"/><text>Moreover, for a fixed guaranteed skew (of say 2d), naive frequency amplification also increases the logical clock imprecision maxi,j∈[1..n],t⩾0{Li(t)-Lj(t)} by the scaling factor, which may adversely affect certain services.</text></s>
<s sid="180"><CoreSc1 advantage="None" conceptID="Bac37" novelty="None" type="Bac"/><text>For example, the size of the FIFO buffers used for inter-domain communication in [10] depends on the clock imprecision and must hence be adapted accordingly.</text></s>
<s sid="181"><CoreSc1 advantage="None" conceptID="Bac38" novelty="None" type="Bac"/><text>On the other hand, by dividing frequencies, it is clearly possible to guarantee that maxi,j∈[1..n],t⩾0{Li(t)-Lj(t)}=1.</text></s>
<s sid="182"><CoreSc1 advantage="None" conceptID="Mot13" novelty="None" type="Mot"/><text>Therefore, it is an important design goal to minimize clock imprecision while at the same time maximizing the frequency at which clocks run.</text></s>
<s sid="183"><CoreSc1 advantage="None" conceptID="Goa12" novelty="None" type="Goa"/><text>Naturally, this becomes much more involved due to the design goals already presented.</text></s>
Scalability
<s sid="184"><CoreSc1 advantage="None" conceptID="Goa13" novelty="None" type="Goa"/><text>Being able to meet all the above design goals is meaningless if one cannot control the amount of resources devoted to the task of clock generation.</text></s>
<s sid="185"><CoreSc1 advantage="None" conceptID="Obj38" novelty="None" type="Obj"/><text>Pivotal issues are the following:•</text></s>
<s sid="186"><CoreSc1 advantage="None" conceptID="Mod3" novelty="None" type="Mod"/><text>Area consumption: The chip area used by the components of the synchronization algorithm decomposes into the area consumed by the nodes and their interconnections.</text></s>
<s sid="187"><CoreSc1 advantage="None" conceptID="Mod4" novelty="None" type="Mod"/><text>The former can be captured by the gate complexity, i.e., the number of (constant fan-in) gates required to perform the algorithm's computations.</text></s>
<s sid="188"><CoreSc1 advantage="None" conceptID="Bac39" novelty="None" type="Bac"/><text>The latter significantly depends on the chip layout, which is highly application-dependent and hence outside our scope of control.</text></s>
<s sid="189"><CoreSc1 advantage="None" conceptID="Bac40" novelty="None" type="Bac"/><text>It is clear, however, that the number of channels and their bandwidth play a crucial role.</text></s>
•
<s sid="190"><CoreSc1 advantage="None" conceptID="Mot14" novelty="None" type="Mot"/><text>Communication complexity: Apart from whether two nodes are connected or not, it is of interest how many wires are required.</text></s>
<s sid="191"><CoreSc1 advantage="None" conceptID="Res30" novelty="None" type="Res"/><text>This is well-represented by the bit complexity of an algorithm, i.e., the number of bits it exchanges per time unit between communication partners.</text></s>
<s sid="192"><CoreSc1 advantage="None" conceptID="Res31" novelty="None" type="Res"/><text>Note that while the number of wires can be reduced by means of time division, this will require additional memory and computational resources on the receiver's side and increase the communication delay.</text></s>
<s sid="193"><CoreSc1 advantage="None" conceptID="Met39" novelty="None" type="Met"/><text>In any case, it is highly desirable to devise algorithms of (small) constant bit complexity.</text></s>
<s sid="194"><CoreSc1 advantage="None" conceptID="Met40" novelty="None" type="Met"/><text>Moreover, broadcasting the same information to all nodes instead of different information to different receivers is to be preferred, as it allows us to use communication buses.</text></s>
•
<s sid="195"><CoreSc1 advantage="None" conceptID="Goa14" novelty="None" type="Goa"/><text>Stabilization time: For reasons stated earlier, we would like to minimize the stabilization time.</text></s>
<s sid="196"><CoreSc1 advantage="None" conceptID="Mot15" novelty="None" type="Mot"/><text>In particular, it is not good enough to know that an algorithm eventually stabilizes, as the required time might be well above what makes the algorithm self-stabilizing in any practical sense.</text></s>
•
<s sid="197"><CoreSc1 advantage="None" conceptID="Hyp6" novelty="None" type="Hyp"/><text>Resilience: The number f of faults that can be concurrently sustained without losing synchronization or the capability to stabilize should grow with system size, as otherwise a larger system will suffer from more frequent outages.</text></s>
<s sid="198"><CoreSc1 advantage="None" conceptID="Hyp7" novelty="None" type="Hyp"/><text>Note that while we must accept that stabilization is a random process (due to the unavoidable probability of metastable upsets), we demand that a system that is stable will always remain so as long as there are not too many faults (including upsets).</text></s>
<s sid="199"><CoreSc1 advantage="None" conceptID="Res32" novelty="None" type="Res"/><text>As mentioned earlier, a lower bound shows that always f&lt;n/3, giving a precise meaning to &quot;too many&quot; here.</text></s>
•
<s sid="200"><CoreSc1 advantage="None" conceptID="Res33" novelty="None" type="Res"/><text>Delays: As the maximal delay d accounts both for the delay incurred by communication as well as computation, it is vital to minimize both.</text></s>
<s sid="201"><CoreSc1 advantage="None" conceptID="Res34" novelty="None" type="Res"/><text>Notwithstanding the fact that the communication delay and computing speed is mostly determined by parameters outside our control (technology, spatial distances, number of nodes, etc.), minimizing the gate complexity and, in particular, the depth of the circuits implementing the nodes' algorithms (that determine the computing delays) is important.</text></s>
•
<s sid="202"><CoreSc1 advantage="None" conceptID="Res35" novelty="None" type="Res"/><text>Metastability: In larger and faster systems, the number of events per time unit that could cause metastable upsets is obviously larger.</text></s>
<s sid="203"><CoreSc1 advantage="None" conceptID="Res36" novelty="None" type="Res"/><text>Therefore, it is vital to safely exclude metastability from occurring during regular operation by construction.1010</text></s>
<s sid="204"><CoreSc1 advantage="None" conceptID="Met41" novelty="None" type="Met"/><text>Note that in this regard our approach is superior to standard GALS systems using synchronizers, where the risk of metastability is immanent (at every clock transition) also in normal operation.</text></s>
<s sid="205"><CoreSc1 advantage="None" conceptID="Met42" novelty="None" type="Met"/><text>We admit metastability only during rare exceptional phases of system operation where it cannot be avoided in principle, like during stabilization or in case of faults.</text></s>
<s sid="206"><CoreSc1 advantage="None" conceptID="Mod5" novelty="None" type="Mod"/><text>As the probabilities for metastable upsets are hard to quantify even in a final product, we do not use a &quot;hard&quot; measure here.1111</text></s>
<s sid="207"><CoreSc1 advantage="None" conceptID="Res37" novelty="None" type="Res"/><text>It is worth mentioning, though, that the asymptotic increase in the number of events per time unit that could cause metastable upsets is clearly at least polynomial in n.</text></s>
<s sid="208"><CoreSc1 advantage="None" conceptID="Res38" novelty="None" type="Res"/><text>As synchronizer chains decrease the probability of upsets exponentially, the required length of synchronizer chains will asymptotically grow as Ω(logn), increasing the system cost and (effectively) decreasing its operational frequency.</text></s>
•
<s sid="209"><CoreSc1 advantage="None" conceptID="Obj39" novelty="None" type="Obj"/><text>Connectivity: In order to facilitate efficient placement and routing on a chip, it is vital to ensure that the communication network is sparse.</text></s>
<s sid="210"><CoreSc1 advantage="None" conceptID="Obj40" novelty="None" type="Obj"/><text>Also, a sparse network will consume less area and is beneficial to fault containment.1212</text></s>
<s sid="211"><CoreSc1 advantage="None" conceptID="Mot16" novelty="None" type="Mot"/><text>If a single event such as e.g.</text></s>
<s sid="212"><CoreSc1 advantage="None" conceptID="Mot17" novelty="None" type="Mot"/><text>an ionizing particle hit can render multiple nodes faulty, even tolerating a large number of faulty nodes is of little use w.r.t.</text></s>
<s sid="213"><CoreSc1 advantage="None" conceptID="Mot18" novelty="None" type="Mot"/><text>the overall resilience of the system.</text></s>
<s sid="214"><CoreSc1 advantage="None" conceptID="Mot19" novelty="None" type="Mot"/><text>Tackling this issue is subject to our future work and hence beyond the scope of this article, however.</text></s>
•
<s sid="215"><CoreSc1 advantage="None" conceptID="Mot20" novelty="None" type="Mot"/><text>Clock size: If the logical clocks have too few bits, i.e., overflow too frequently, they might be unsuitable for the application logic of the SoC.</text></s>
<s sid="216"><CoreSc1 advantage="None" conceptID="Obj41" novelty="None" type="Obj"/><text>The algorithm we present in this article can in principle provide clocks of arbitrary bounded size.</text></s>
<s sid="217"><CoreSc1 advantage="None" conceptID="Obj42" novelty="None" type="Obj"/><text>However, its stabilization time would grow linearly with the maximum clock value once we scale above 8-bit clocks.</text></s>
<s sid="218"><CoreSc1 advantage="None" conceptID="Obj43" novelty="None" type="Obj"/><text>In a recent publication, we show how to construct larger clocks efficiently [41].</text></s>
<s sid="219"><CoreSc1 advantage="None" conceptID="Obj44" novelty="None" type="Obj"/><text>Typical modules for clock synchronization protocols</text></s>
<s sid="220"><CoreSc1 advantage="None" conceptID="Obj45" novelty="None" type="Obj"/><text>We next introduce the basic modules that are assumed by the model used in [13].</text></s>
<s sid="221"><CoreSc1 advantage="None" conceptID="Obj46" novelty="None" type="Obj"/><text>We first give an intuitive description of the required modules.</text></s>
<s sid="222"><CoreSc1 advantage="None" conceptID="Obj47" novelty="None" type="Obj"/><text>Subsequently, we introduce a novel formal framework for specifying self-stabilizing fault-tolerant modules and specify our basic modules in this framework.</text></s>
<s sid="223"><CoreSc1 advantage="None" conceptID="Goa15" novelty="None" type="Goa"/><text>Any implementation satisfying this specification can be plugged into the high-level algorithm in order to yield a system guaranteeing the properties proved in [13].</text></s>
<s sid="224"><CoreSc1 advantage="None" conceptID="Goa16" novelty="None" type="Goa"/><text>We now list the building blocks beyond standard logic gates that will explicitly or implicitly be used by the algorithm presented in Section 5.</text></s>
<s sid="225"><CoreSc1 advantage="None" conceptID="Obj48" novelty="None" type="Obj"/><text>Each of these building blocks computes output signals that are constrained by (the history of) its input signals.</text></s>
<s sid="226"><CoreSc1 advantage="None" conceptID="Met43" novelty="None" type="Met"/><text>If the logic function implies an output transition in reaction to an input change, this transition is not required to occur immediately; it must occur within a known time bound, however.</text></s>
<s sid="227"><CoreSc1 advantage="None" conceptID="Met44" novelty="None" type="Met"/><text>Given the time bounds for the individual modules and the connecting wires, one can compute the maximum delay d.</text></s>
<s sid="228"><CoreSc1 advantage="None" conceptID="Res39" novelty="None" type="Res"/><text>Moreover, informally speaking, it must be avoided that a single change in the input(s) causes multiple transitions of the output signal, as this could undermine the high-level algorithm's logic.</text></s>
<s sid="229"><CoreSc1 advantage="None" conceptID="Bac41" novelty="None" type="Bac"/><text>Note also that statefulness, i.e., any sort of memory (including positive feedback loops), bears the potential for metastable upsets and requires careful attention in order to ensure self-stabilization.1313</text></s>
<s sid="230"><CoreSc1 advantage="None" conceptID="Mot21" novelty="None" type="Mot"/><text>In other words, the module must recover from arbitrary corruptions of its memory.</text></s>
<s sid="231"><CoreSc1 advantage="None" conceptID="Mot22" novelty="None" type="Mot"/><text>Purely combinational elements, on the other hand, differ in their ability to prevent metastable inputs from reaching the output under certain conditions.</text></s>
<s sid="232"><CoreSc1 advantage="None" conceptID="Obj49" novelty="None" type="Obj"/><text>Each node will be a union of state machines that communicate via channels (both among each other and with remote nodes) and are composed of standard logic gates and all other modules we describe below.</text></s>
<s sid="233"><CoreSc1 advantage="None" conceptID="Obj50" novelty="None" type="Obj"/><text>Fig. 5 depicts such a state machine.•</text></s>
Communication channels.
<s sid="234"><CoreSc1 advantage="None" conceptID="Met45" novelty="None" type="Met"/><text>We previously introduced the communication channels from node i to node j as abstract devices that convey the states with a delay of at most d that also accounts for computations.</text></s>
<s sid="235"><CoreSc1 advantage="None" conceptID="Met46" novelty="None" type="Met"/><text>Viewed as a module, the (physical) communication channels do account for the time to communicate the state information only, whereas computations are performed by standard logic gates and the modules we will describe next.</text></s>
<s sid="236"><CoreSc1 advantage="None" conceptID="Res40" novelty="None" type="Res"/><text>A communication channel of this type simply maps its input signal to its output signal.</text></s>
<s sid="237"><CoreSc1 advantage="None" conceptID="Con9" novelty="None" type="Con"/><text>The reason why communication channels are nonetheless listed as modules here is that encoding a non-binary state signal in a glitch- and metastability-free manner is a non-trivial task, as in the absence of (reliable) synchrony both parallel and sequential communication present challenges.</text></s>
<s sid="238"><CoreSc1 advantage="None" conceptID="Con10" novelty="None" type="Con"/><text>In our abstraction, this encoding is performed by the channel, which requires additional logic and thus potentially results in delays beyond the mere wire delays as well as the necessity to consider issues concerning metastability and self-stabilization.</text></s>
•
Memory flags.
<s sid="239"><CoreSc1 advantage="None" conceptID="Met47" novelty="None" type="Met"/><text>These are just simple binary storage elements that can be set to 1 by means of one input signal and can be reset to 0 by a second input signal; their state is externally accessible via an output signal.</text></s>
<s sid="240"><CoreSc1 advantage="None" conceptID="Res41" novelty="None" type="Res"/><text>Simply put, a memory flag just &quot;remembers&quot; which input signal was 1 most recently.</text></s>
<s sid="241"><CoreSc1 advantage="None" conceptID="Res42" novelty="None" type="Res"/><text>In our algorithms, memory flags will be used to memorize wether an input signal from a remote node was in state 1 at some time after the most recent reset upon a state transition of one of the node's state machines (in Fig. 1, e.g., a node resets its propose flags when switching from increase to ready).</text></s>
•
Threshold gates.
<s sid="242"><CoreSc1 advantage="None" conceptID="Obs1" novelty="None" type="Obs"/><text>Frequently, nodes will need to decide whether a certain threshold number (f+1 or n-f) of signals (or sets of signals) satisfy some Boolean predicate (e.g., the conditions for switching to propose and increase in Fig. 1 involve such a threshold).</text></s>
<s sid="243"><CoreSc1 advantage="None" conceptID="Obs2" novelty="None" type="Obs"/><text>A threshold gate takes the respective binary input signals and outputs 1 if the threshold is reached and 0 otherwise.</text></s>
•
Watchdog timers.
<s sid="244"><CoreSc1 advantage="None" conceptID="Res43" novelty="None" type="Res"/><text>Watchdog timers are the nodes' sense for the progress of time.</text></s>
<s sid="245"><CoreSc1 advantage="None" conceptID="Res44" novelty="None" type="Res"/><text>Each timer (T,s,C), where T in R+ is a duration and C a clock, is associated with state s and is either expired (output 1) or not expired (output 0).</text></s>
<s sid="246"><CoreSc1 advantage="None" conceptID="Obs3" novelty="None" type="Obs"/><text>The timer is reset to 0 when the node's state switches to s and will expire after T time has passed according to clock C, unless it is reset again beforehand.</text></s>
<s sid="247"><CoreSc1 advantage="None" conceptID="Res45" novelty="None" type="Res"/><text>Hence, if it is reset at (reference) time t, it will expire at some time t′∈[t+T/ϑ,t+T].</text></s>
<s sid="248"><CoreSc1 advantage="None" conceptID="Res46" novelty="None" type="Res"/><text>For instance, the transition to ready in Fig. 1 is triggered by a watchdog timer.</text></s>
•
Randomized watchdog timers.
<s sid="249"><CoreSc1 advantage="None" conceptID="Res47" novelty="None" type="Res"/><text>A randomized watchdog timer is identical to a regular watchdog timer except that the duration T of the timeout is drawn from a (bounded) random distribution D over R+.</text></s>
<s sid="250"><CoreSc1 advantage="None" conceptID="Res48" novelty="None" type="Res"/><text>That is, if randomized timer (D,s,C) is reset at time t, it will expire at time t′∈[t+T/ϑ,t+T], where T is drawn from D.</text></s>
<s sid="251"><CoreSc1 advantage="None" conceptID="Goa17" novelty="None" type="Goa"/><text>How this randomness is implemented subtly affects resilience and security properties of the system, see [13] for a formal definition of the way randomness is to be employed.</text></s>
<s sid="252"><CoreSc1 advantage="None" conceptID="Met48" novelty="None" type="Met"/><text>For the purpose of this article, we confine ourselves to pointing out that, essentially, we require that faulty nodes do not have access to the value T drawn from D before time t′.</text></s>
•
State transition modules.
<s sid="253"><CoreSc1 advantage="None" conceptID="Obs4" novelty="None" type="Obs"/><text>The algorithm does not demand zero-time state transitions.</text></s>
<s sid="254"><CoreSc1 advantage="None" conceptID="Obs5" novelty="None" type="Obs"/><text>Nevertheless, it is non-trivial to ensure metastability-free state transitions and consistent memory states in our setting.</text></s>
<s sid="255"><CoreSc1 advantage="None" conceptID="Res49" novelty="None" type="Res"/><text>On a transition from state s to state s′, the node needs to switch its state signal (i.e., the input to its outgoing communication channels) exactly once from s to s′ and reset any memory flag that is to be reset upon transitioning from s to s′.</text></s>
<s sid="256"><CoreSc1 advantage="None" conceptID="Res50" novelty="None" type="Res"/><text>This is complicated because the condition under which the state transition occurs, a Boolean predicate over a blend of signals from incoming communication channels and local modules' outputs, may be satisfied for a brief period of time only, or a condition for switching to a different state s″ might become satisfied at almost the same instant.</text></s>
<s sid="257"><CoreSc1 advantage="None" conceptID="Res51" novelty="None" type="Res"/><text>Even worse, it may e.g.</text></s>
<s sid="258"><CoreSc1 advantage="None" conceptID="Res52" novelty="None" type="Res"/><text>be required that memory flags whose output is part of a predicate expressing the condition for switching from s to s′ are to be reset upon the transition to s′.</text></s>
<s sid="259"><CoreSc1 advantage="None" conceptID="Res53" novelty="None" type="Res"/><text>Resolving these issues is the purpose of a state transition module that controls the safe transition from one state to another.</text></s>
<s sid="260"><CoreSc1 advantage="None" conceptID="Res54" novelty="None" type="Res"/><text>A formal modeling framework for self-stabilizing fault-tolerant circuits</text></s>
<s sid="261"><CoreSc1 advantage="None" conceptID="Obj51" novelty="None" type="Obj"/><text>In this section, we introduce a novel formal framework for specifying self-stabilizing fault-tolerant modules.</text></s>
<s sid="262"><CoreSc1 advantage="None" conceptID="Obj52" novelty="None" type="Obj"/><text>It is a non-trivial extension of [32,42] that allows us to rigorously express the properties related to self-stabilization as used in [13].</text></s>
<s sid="263"><CoreSc1 advantage="None" conceptID="Obj53" novelty="None" type="Obj"/><text>Using this framework, we then give a precise formal specification of our basic modules' behavior.</text></s>
Signals
<s sid="264"><CoreSc1 advantage="None" conceptID="Obj54" novelty="None" type="Obj"/><text>We define (the trace of) a signal to be a timed event trace over a finite alphabet S of possible signal states: Formally, signal σ⊆S×R.</text></s>
<s sid="265"><CoreSc1 advantage="None" conceptID="Mod6" novelty="None" type="Mod"/><text>The elements of σ are called events, and for each event (s,t) we call s the state of event (s,t) and t the time of event (s,t).</text></s>
<s sid="266"><CoreSc1 advantage="None" conceptID="Res55" novelty="None" type="Res"/><text>Note that we allow for events (s,t) and (s,t′)∈σ, where t&lt;t′, without having an event (s′,t″)∈σ with s′≠s and t&lt;t″&lt;t′.</text></s>
<s sid="267"><CoreSc1 advantage="None" conceptID="Res56" novelty="None" type="Res"/><text>In this case, we call event (s,t′) idempotent.</text></s>
<s sid="268"><CoreSc1 advantage="None" conceptID="Res57" novelty="None" type="Res"/><text>In general, a signal σ is required to fulfill the following conditions:(i)</text></s>
<s sid="269"><CoreSc1 advantage="None" conceptID="Res58" novelty="None" type="Res"/><text>From (s,t)∈σ and (s′,t)∈σ follows that s=s′.</text></s>
(ii)
<s sid="270"><CoreSc1 advantage="None" conceptID="Res59" novelty="None" type="Res"/><text>For each time interval [t-,t+]⊆R of finite length, the number of non-idempotent events in σ with times within [t-,t+] is finite.</text></s>
(iii)
<s sid="271"><CoreSc1 advantage="None" conceptID="Res60" novelty="None" type="Res"/><text>For any time t, there exists an event (s,t′)∈σ with t′⩽t.</text></s>
<s sid="272"><CoreSc1 advantage="None" conceptID="Res61" novelty="None" type="Res"/><text>We say that signal σ switches to s at time t iff event (s,t)∈σ is not idempotent.</text></s>
<s sid="273"><CoreSc1 advantage="None" conceptID="Res62" novelty="None" type="Res"/><text>Due to property (ii), there is always a non-zero amount of time between two such events.</text></s>
<s sid="274"><CoreSc1 advantage="None" conceptID="Res63" novelty="None" type="Res"/><text>These events describe when the corresponding physical signal undergoes an actual transition.</text></s>
<s sid="275"><CoreSc1 advantage="None" conceptID="Res64" novelty="None" type="Res"/><text>Therefore, we define that signals σ and σ′ are equivalent, iff they differ in idempotent events only, and identify all signals of an equivalence class.</text></s>
<s sid="276"><CoreSc1 advantage="None" conceptID="Res65" novelty="None" type="Res"/><text>Each equivalence class [σ] contains a unique signal σmax that contains an event for each time t∈R.</text></s>
<s sid="277"><CoreSc1 advantage="None" conceptID="Res66" novelty="None" type="Res"/><text>We identify this signal (and thus the entire class) with the function that maps each time t to the state σ(t):=s satisfying that (s,t)∈σmax.</text></s>
<s sid="278"><CoreSc1 advantage="None" conceptID="Obs6" novelty="None" type="Obs"/><text>We call σ(⋅) the state function of signal σ, and σ(t) the state of signal σ at time t.</text></s>
<s sid="279"><CoreSc1 advantage="None" conceptID="Met49" novelty="None" type="Met"/><text>Note that since the state function of a signal σ depends on [σ] only, we may add or remove idempotent events at will without changing the state function.</text></s>
Modules and executions
<s sid="280"><CoreSc1 advantage="None" conceptID="Exp2" novelty="None" type="Exp"/><text>Each module comprises a (possibly empty) set of input ports and a (possibly empty) set of output ports.</text></s>
<s sid="281"><CoreSc1 advantage="None" conceptID="Res67" novelty="None" type="Res"/><text>These sets must be disjoint, i.e., we do not allow a module's output to be identical to one of its inputs; it may be identical to the input port of another module, however.</text></s>
<s sid="282"><CoreSc1 advantage="None" conceptID="Obs7" novelty="None" type="Obs"/><text>An execution of ports P on interval I⊆R assigns a state to each port in P at any time in I.</text></s>
<s sid="283"><CoreSc1 advantage="None" conceptID="Obs8" novelty="None" type="Obs"/><text>For convenience of notation, for any port p, we identify p and its state function whenever the respective execution is clear from the context.</text></s>
<s sid="284"><CoreSc1 advantage="None" conceptID="Obs9" novelty="None" type="Obs"/><text>Moreover, we may omit the &quot;on R&quot; for all executions for which I=R when clear from the context.</text></s>
<s sid="285"><CoreSc1 advantage="None" conceptID="Mod7" novelty="None" type="Mod"/><text>An execution of module M on I is an execution of M's input and output ports on I.</text></s>
<s sid="286"><CoreSc1 advantage="None" conceptID="Mod8" novelty="None" type="Mod"/><text>The restriction of execution E on I to I′⊆I is the restriction of all of E's state functions to the interval I′.</text></s>
<s sid="287"><CoreSc1 advantage="None" conceptID="Mod9" novelty="None" type="Mod"/><text>Let E be an execution of ports P on I.</text></s>
<s sid="288"><CoreSc1 advantage="None" conceptID="Mod10" novelty="None" type="Mod"/><text>Then the restriction of execution E on I to ports P′⊆P is the execution of ports P′ on I with state functions equal to E's state functions on P′.</text></s>
<s sid="289"><CoreSc1 advantage="None" conceptID="Res68" novelty="None" type="Res"/><text>Besides input and output ports, a module M further has a module specification.</text></s>
<s sid="290"><CoreSc1 advantage="None" conceptID="Res69" novelty="None" type="Res"/><text>We allow two kinds of module specifications for a module M, distinguishing between basic and compound modules.</text></s>
Basic modules.
<s sid="291"><CoreSc1 advantage="None" conceptID="Con11" novelty="None" type="Con"/><text>In this case, the module specification is a function ΦM that, for all I, maps every execution of the module's input ports on I to a set of executions of the module's output ports on I.</text></s>
<s sid="292"><CoreSc1 advantage="None" conceptID="Goa18" novelty="None" type="Goa"/><text>The intended meaning of ΦM is to map each and every conceivable execution of M's input ports on I to the resulting possible reactions of M during the same time, which may be many different ones.</text></s>
<s sid="293"><CoreSc1 advantage="None" conceptID="Res70" novelty="None" type="Res"/><text>For example, a module that may behave arbitrarily for some input execution Ein is specified by setting ΦM(Ein) to be the set of all conceivable output executions.</text></s>
<s sid="294"><CoreSc1 advantage="None" conceptID="Res71" novelty="None" type="Res"/><text>For a basic module M we say that execution E of M on some I is feasible iff Eout∈ΦM(Ein), where Ein and Eout are the restrictions of execution E to M's input and output ports, respectively.</text></s>
<s sid="295"><CoreSc1 advantage="None" conceptID="Res72" novelty="None" type="Res"/><text>We require two properties for ΦM to hold:(i)</text></s>
<s sid="296"><CoreSc1 advantage="None" conceptID="Res73" novelty="None" type="Res"/><text>Non-emptiness: Φ(Ein)≠∅ for all executions Ein of M's input ports.</text></s>
(ii)
<s sid="297"><CoreSc1 advantage="None" conceptID="Res74" novelty="None" type="Res"/><text>Properness: Any restriction (in time) of a feasible execution of M is feasible.</text></s>
<s sid="298"><CoreSc1 advantage="None" conceptID="Res75" novelty="None" type="Res"/><text>These properties are motivated by the facts that (i) any given input of a correctly operating module will produce some output and (ii) correct operation on a given time interval implies correct operation on any subinterval of this interval.</text></s>
Example 3.1
<s sid="299"><CoreSc1 advantage="None" conceptID="Mod11" novelty="None" type="Mod"/><text>A simple basic module is a (zero-time) inverter with (binary) input port i, (binary) output port o, and module specification ΦInv defined by: For each interval I⊆R and each execution Ein of input port i, an execution Eout of output port o on I is in ΦInv(Ein) iff for all t∈I it holds that o(t)=¬i(t).</text></s>
 Example 3.2
<s sid="300"><CoreSc1 advantage="None" conceptID="Mod12" novelty="None" type="Mod"/><text>As an example of a timed basic module, consider a fixed-delay channel with input port i, output port o, and delay d&gt;0.</text></s>
<s sid="301"><CoreSc1 advantage="None" conceptID="Mod13" novelty="None" type="Mod"/><text>Its module specification ΦC is defined by: For each interval [t-,t+)⊆R and each execution Ein of input port i on [t-,t+), an execution Eout of output port o on I is in ΦC(Ein) iff for all t∈[t-+d,t+) we have that o(t)=i(t-d).</text></s>
<s sid="302"><CoreSc1 advantage="None" conceptID="Con12" novelty="None" type="Con"/><text>Clearly, a basic module needs to adhere to ΦM only on intervals I during which it is correct, and may behave arbitrarily when it is faulty.</text></s>
<s sid="303"><CoreSc1 advantage="None" conceptID="Con13" novelty="None" type="Con"/><text>Subtle issues originate in the fact that a module may become correct after an earlier (transient) fault, in the sense that its internal components work as intended afterwards.</text></s>
<s sid="304"><CoreSc1 advantage="None" conceptID="Con14" novelty="None" type="Con"/><text>At this point in time, it may or may not be the case that all traces of the transient fault have been vanished from the internal state of the module.</text></s>
<s sid="305"><CoreSc1 advantage="None" conceptID="Res76" novelty="None" type="Res"/><text>The typical use of basic modules is the description of a (sub)problem.</text></s>
<s sid="306"><CoreSc1 advantage="None" conceptID="Res77" novelty="None" type="Res"/><text>For instance, the module specification of a threshold module will be such that the output is required to indicate whether a certain number of binary input ports is in state 1.</text></s>
<s sid="307"><CoreSc1 advantage="None" conceptID="Res78" novelty="None" type="Res"/><text>Basic modules are then employed with the understanding that they require an implementation matching their specification.</text></s>
<s sid="308"><CoreSc1 advantage="None" conceptID="Con15" novelty="None" type="Con"/><text>This use of basic modules in our algorithms entails that correct modules have correct internal states.</text></s>
<s sid="309"><CoreSc1 advantage="None" conceptID="Con16" novelty="None" type="Con"/><text>Although a basic module description abstracts away its internal state, this property can be characterized in a natural way by another constraint on the module specification: We say that the specification of some module M is extendable iff each feasible execution of M on some interval [t-,t+)⊂R is the restriction of a feasible execution of M on R.</text></s>
<s sid="310"><CoreSc1 advantage="None" conceptID="Res79" novelty="None" type="Res"/><text>In other words, for each execution on some interval [t-,t+), (i) a fault-free history exists that leads to the internal state of the module at time t- (and therefore, for the given input signals, to the same execution on [t-,t+)), and (ii) the module is capable of continuing to operate correctly in the future (i.e., there is no fault-free execution that eventually cannot be continued in a way that adheres to ΦM).</text></s>
<s sid="311"><CoreSc1 advantage="None" conceptID="Hyp8" novelty="None" type="Hyp"/><text>While (ii) should always be true for essentially the same reason that we demand non-emptiness for, (i) can be seen as the requirement of a correct internal state: For a correct module with an extensible specification, it is required that both all internal components of a physical implementation of the module operate according to their specification and all traces of an earlier transient fault (if any) from the internal state must have worn off, in the sense that it could have been reached by a non-faulty history.</text></s>
<s sid="312"><CoreSc1 advantage="None" conceptID="Hyp9" novelty="None" type="Hyp"/><text>Most of the basic module specifications we are going to introduce will be extendable.</text></s>
<s sid="313"><CoreSc1 advantage="None" conceptID="Res80" novelty="None" type="Res"/><text>However, there are also basic modules with non-extendable specifications; we will provide an example later on.</text></s>
<s sid="314"><CoreSc1 advantage="None" conceptID="Res81" novelty="None" type="Res"/><text>The next lemma shows that any extendable module specification is uniquely characterized by its values on input executions of the respective module on R.</text></s>
Lemma 3.3
<s sid="315"><CoreSc1 advantage="None" conceptID="Res82" novelty="None" type="Res"/><text>Any function Φ that maps each execution Ein of a set of input ports Pin on R to a non-empty set of executions Eout of a set of output ports Pout on R specifies a unique module M such that (i) ΦM is extendable and (ii) ΦM(Ein)=Φ(Ein) for each execution of M's input ports Pin on R.</text></s>
<s sid="316"><CoreSc1 advantage="None" conceptID="Res83" novelty="None" type="Res"/><text>Conversely, for each module M whose specification is extendable, ΦM is uniquely characterized by its values on executions of ports Pin on R.</text></s>
Proof
<s sid="317"><CoreSc1 advantage="None" conceptID="Res84" novelty="None" type="Res"/><text>We show first that for a function Φ as assumed by the lemma there exists a module M such that ΦM(Ein)=Φ(Ein) for executions Ein of ports Pin on R and ΦM is extendable.</text></s>
<s sid="318"><CoreSc1 advantage="None" conceptID="Res85" novelty="None" type="Res"/><text>To this end, we simply define that M is a module with input ports Pin and output ports Pout whose specification ΦM is given by Eout∈ΦM(Ein) iff Ein and Eout are restrictions of executions Ein′ and Eout′ on R, respectively, such that Eout′∈Φ(Ein′).</text></s>
<s sid="319"><CoreSc1 advantage="None" conceptID="Res86" novelty="None" type="Res"/><text>Clearly, ΦM satisfies properness and extendability by construction.</text></s>
<s sid="320"><CoreSc1 advantage="None" conceptID="Res87" novelty="None" type="Res"/><text>Regarding non-emptiness, we can extend any input execution Ein on some interval I arbitrarily to an execution Ein′ on R.</text></s>
<s sid="321"><CoreSc1 advantage="None" conceptID="Res88" novelty="None" type="Res"/><text>By assumption, Φ(Ein′)≠∅, so there must be some execution Eout′∈Φ(Ein′).</text></s>
<s sid="322"><CoreSc1 advantage="None" conceptID="Res89" novelty="None" type="Res"/><text>By definition, its restriction Eout to I is in ΦM(Ein), proving non-emptiness.</text></s>
<s sid="323"><CoreSc1 advantage="None" conceptID="Res90" novelty="None" type="Res"/><text>Hence ΦM is a valid specification of a module M with input ports Pin and output ports Pout.</text></s>
<s sid="324"><CoreSc1 advantage="None" conceptID="Res91" novelty="None" type="Res"/><text>We now establish the second claim of the lemma, which will also show that the module M we constructed from Φ is unique and thus complete the proof.</text></s>
<s sid="325"><CoreSc1 advantage="None" conceptID="Res92" novelty="None" type="Res"/><text>To this end, let Ein and Eout be any executions of the input and output ports of M, respectively.</text></s>
<s sid="326"><CoreSc1 advantage="None" conceptID="Res93" novelty="None" type="Res"/><text>If Eout∈ΦM(Ein), by extendability Ein and Eout are restrictions of executions Ein′ and Eout′ on R, respectively, such that Eout′∈ΦM(Ein′).</text></s>
<s sid="327"><CoreSc1 advantage="None" conceptID="Res94" novelty="None" type="Res"/><text>On the other hand, if Eout∉ΦM(Ein), properness necessitates that there are no such executions Ein′ and Eout′.</text></s>
<s sid="328"><CoreSc1 advantage="None" conceptID="Res95" novelty="None" type="Res"/><text>This shows the second statement and the lemma follows. □</text></s>
<s sid="329"><CoreSc1 advantage="None" conceptID="Res96" novelty="None" type="Res"/><text>Enabled by this lemma, we will specify most of our basic modules by defining ΦM(Ein)≠∅ for input executions Ein on R only.</text></s>
<s sid="330"><CoreSc1 advantage="None" conceptID="Res97" novelty="None" type="Res"/><text>For instance, the fixed delay channel from Example 3.2 can now be specified equivalently as follows.</text></s>
Example 3.4
<s sid="331"><CoreSc1 advantage="None" conceptID="Res98" novelty="None" type="Res"/><text>A fixed-delay channel with input port i, output port o, and delay d&gt;0 is a basic module with extendable module specification ΦC defined by: For each execution Ein of input port i on R, an execution Eout of output port o on R is in ΦC(Ein) iff for all t∈R we have that o(t)=i(t-d).</text></s>
<s sid="332"><CoreSc1 advantage="None" conceptID="Con17" novelty="None" type="Con"/><text>Because we do not need to describe partial feasible executions on a finite interval I=[t-,t+) here, the fact that the condition o(t)=i(t-d) applies only to a subinterval of I in Example 3.2 becomes superfluous: (-∞+d;∞)=R, so no additional specification is required to describe the module's behavior near the lower bound of intervals [t-,t+) with t-≠∞.</text></s>
<s sid="333"><CoreSc1 advantage="None" conceptID="Con18" novelty="None" type="Con"/><text>By contrast, in Example 3.2, we had to specify this explicitly, by stating that any behavior of the output in [t-,t-+d) is feasible (by leaving the behavior unspecified in this interval).</text></s>
<s sid="334"><CoreSc1 advantage="None" conceptID="Res99" novelty="None" type="Res"/><text>Whereas the specification in Example 3.4 simplifies the one of Example 3.2 only marginally, this is very different for the modules exhibiting more complex relations between input and output introduced later on: Accurately describing all suffixes of a non-trivial partial execution can be error-prone and may result in unnecessarily involved module specifications.</text></s>
Compound modules.
<s sid="335"><CoreSc1 advantage="None" conceptID="Obs10" novelty="None" type="Obs"/><text>In this case, the module M is given by a finite set SM of submodules of M.</text></s>
<s sid="336"><CoreSc1 advantage="None" conceptID="Obj55" novelty="None" type="Obj"/><text>These submodules are interconnected, i.e., an output port of one submodule can be the input port of another, and the ports of M may also be ports of submodules.</text></s>
<s sid="337"><CoreSc1 advantage="None" conceptID="Obj56" novelty="None" type="Obj"/><text>We require that SM is well-formed, i.e., the following conditions are satisfied:1.</text></s>
<s sid="338"><CoreSc1 advantage="None" conceptID="Obs11" novelty="None" type="Obs"/><text>For each output port p of M, there is exactly one submodule S∈SM such that S has output port p.</text></s>
2.
<s sid="339"><CoreSc1 advantage="None" conceptID="Obs12" novelty="None" type="Obs"/><text>For each input port p of a submodule of M, either p is an input port of M, or there is exactly one submodule S∈SM such that S has output port p.</text></s>
<s sid="340"><CoreSc1 advantage="None" conceptID="Res100" novelty="None" type="Res"/><text>The idea behind this definition is to &quot;plug together&quot; ports in a way such that no port is &quot;driven&quot; by more than one port or be left &quot;floating&quot;.</text></s>
<s sid="341"><CoreSc1 advantage="None" conceptID="Mod14" novelty="None" type="Mod"/><text>For a compound module M, we now define the corresponding basic module β(M). β(M)'s input and output ports are the input and output ports of M, whereas its module specification Φβ(M) is defined as follows: Let E be an execution of M on I⊆R and denote by Ein and Eout its restrictions to M's input and output ports, respectively.</text></s>
<s sid="342"><CoreSc1 advantage="None" conceptID="Res101" novelty="None" type="Res"/><text>Then Eout∈Φβ(M)(Ein), iff there exists an execution EM,SM on I of all ports of modules in {M}∪SM, such that, for each module S∈SM, the restriction of execution EM,SM to the ports of S is feasible for S, and EM,SM restricted to M's ports is equal to E. Note that Φβ(M) satisfies properness: This follows directly from the definition and properness of all submodules if all submodules are basic, and for more complex modules by structural induction.</text></s>
<s sid="343"><CoreSc1 advantage="None" conceptID="Res102" novelty="None" type="Res"/><text>We further require that the choice of SM is such that Φβ(M) fulfills non-emptiness.</text></s>
<s sid="344"><CoreSc1 advantage="None" conceptID="Res103" novelty="None" type="Res"/><text>Hence, β(M) is indeed a basic module with specification Φβ(M).</text></s>
<s sid="345"><CoreSc1 advantage="None" conceptID="Res104" novelty="None" type="Res"/><text>We say that execution E of M is feasible for M iff it is feasible for the corresponding basic module β(M).</text></s>
<s sid="346"><CoreSc1 advantage="None" conceptID="Res105" novelty="None" type="Res"/><text>Intuitively, this means that we consider a compound module correct whenever all its submodules are correct.</text></s>
Example 3.5
<s sid="347"><CoreSc1 advantage="None" conceptID="Mod15" novelty="None" type="Mod"/><text>Consider a compound module InvChain with (binary) input port i and (binary) output port o.</text></s>
<s sid="348"><CoreSc1 advantage="None" conceptID="Mod16" novelty="None" type="Mod"/><text>InvChain is specified by the set of modules SInvChain={Inv1,Inv2,Chan1,Chan2}, where Inv1 and Inv2 are (zero-time) inverters, and Chan1 and Chan2 are (binary) fixed-delay channels with delay d&gt;0.</text></s>
<s sid="349"><CoreSc1 advantage="None" conceptID="Mod17" novelty="None" type="Mod"/><text>We connect the modules' ports sequentially, as depicted in Fig. 3: The input port i of InvChain is also the input port of Inv1, whose output port a is the input port of Chan1; the output b of Chan1 is fed into Inv2, whose output c in turn is input to Chan2; finally, the output port o of Chan2 is also the output port of M.</text></s>
<s sid="350"><CoreSc1 advantage="None" conceptID="Mod18" novelty="None" type="Mod"/><text>Let Ein be the execution of port i whose state function is i(t)=0 for t∈(-∞,0) and i(t)=1 for t∈[0,∞).</text></s>
<s sid="351"><CoreSc1 advantage="None" conceptID="Mod19" novelty="None" type="Mod"/><text>For the module specification of InvChain's corresponding basic module β(InvChain) it then holds that Φβ(InvChain)(Ein)={Eout}, where Eout is the execution of port o with o(t)=0 for t∈(-∞,2d) and o(t)=1 for t∈[2d,∞).</text></s>
<s sid="352"><CoreSc1 advantage="None" conceptID="Mod20" novelty="None" type="Mod"/><text>Note that Φβ(InvChain) is extendable: Any feasible execution E on some interval [t-,t+) satisfies o(t)=c(t-d)=¬b(t-d)=¬a(t-2d)=i(t-2d) for all times t∈[t-+2d,t+).</text></s>
<s sid="353"><CoreSc1 advantage="None" conceptID="Mod21" novelty="None" type="Mod"/><text>An infinite feasible execution of InvChain on R whose restriction to [t-,t+) matches E is easily found by (i) extending the input ports execution to [t--2d,t-) by using i(t-2d)=o(t) on [t-,t-+2d), (ii) extending the output ports execution to [t+,t++2d) by using o(t+2d)=i(t) on [t+-2d,t+), and finally defining o(t)=i(t-2d) arbitrarily for t∈(-∞,t-) and t∈[t++2d,∞).</text></s>
<s sid="354"><CoreSc1 advantage="None" conceptID="Res106" novelty="None" type="Res"/><text>From this observation, we can infer by Lemma 3.3 that the module specification of InvChain matches that of a fixed-delay channel with delay 2d, as in feasible executions of InvChain on R we clearly have o(t)=i(t-2d) at all times t∈R.</text></s>
<s sid="355"><CoreSc1 advantage="None" conceptID="Con19" novelty="None" type="Con"/><text>This demonstrates that compound modules permit to reason about the behavior of complex systems in a hierarchical fashion, based on basic modules that can be understood much easier.</text></s>
<s sid="356"><CoreSc1 advantage="None" conceptID="Met50" novelty="None" type="Met"/><text>The level of detail can be adjusted by the granularity at which one relies on basic modules; in particular, it is feasible to refine the analysis later on by further breaking down basic modules into compound modules.</text></s>
Example 3.6
<s sid="357"><CoreSc1 advantage="None" conceptID="Obs13" novelty="None" type="Obs"/><text>As an example of a non-extendable specification, consider the compound module Osc, a simple oscillator that is started at, say, time 0.</text></s>
<s sid="358"><CoreSc1 advantage="None" conceptID="Res107" novelty="None" type="Res"/><text>The module has no input port and one (binary) output port c, the clock signal.1414</text></s>
<s sid="359"><CoreSc1 advantage="None" conceptID="Res108" novelty="None" type="Res"/><text>In practice, one would of course need an input conveying the control signal starting the oscillator; we conveniently hide this detail within the module specification.</text></s>
<s sid="360"><CoreSc1 advantage="None" conceptID="Res109" novelty="None" type="Res"/><text>It comprises a binary resettable fixed-delay channel (RChan) of delay d&gt;0, with input port c and output port o, whose extendable specification demands that its output fulfills o(t)=0 for all times t∈(-∞,0) and o(t)=c(t-d) for all times t∈[0,∞).</text></s>
<s sid="361"><CoreSc1 advantage="None" conceptID="Res110" novelty="None" type="Res"/><text>Port o further is the input port of a (zero-time) inverter (Inv), whose output port is c, the clock signal.</text></s>
<s sid="362"><CoreSc1 advantage="None" conceptID="Res111" novelty="None" type="Res"/><text>It is not hard to see that the only feasible execution of Osc on R maintains signal 0 on port o and signal 1 on port c until time 0.</text></s>
<s sid="363"><CoreSc1 advantage="None" conceptID="Res112" novelty="None" type="Res"/><text>Then the feedback loop starts to oscillate with frequency 1/(2d), since the channel reproduces its input with fixed delay d and the inverter inverts the signal.</text></s>
<s sid="364"><CoreSc1 advantage="None" conceptID="Res113" novelty="None" type="Res"/><text>The specification of Osc is not extendable.</text></s>
<s sid="365"><CoreSc1 advantage="None" conceptID="Res114" novelty="None" type="Res"/><text>This can be seen from the execution on [0,∞) where o(t)=kmod2 for all t∈[kd/3,(k+1)d/3) and k∈N0, and c(t)=¬o(t) for all t∈[0,∞).</text></s>
<s sid="366"><CoreSc1 advantage="None" conceptID="Res115" novelty="None" type="Res"/><text>This execution is feasible when restricted to each submodule since c(t)=¬o(t)=o(t-d) for all t∈[d,∞), but the output signal c(t) oscillates at frequency 3/(2d).</text></s>
<s sid="367"><CoreSc1 advantage="None" conceptID="Res116" novelty="None" type="Res"/><text>Hence this execution is feasible, but not a restriction of the unique feasible execution of Osc on R.</text></s>
<s sid="368"><CoreSc1 advantage="None" conceptID="Res117" novelty="None" type="Res"/><text>Note carefully that an oscillator according to Fig. 4 involving a channel with delay d=0 would be an example of a well-formed set of modules M that is not a compound module, since it violates non-emptyness of β(M).</text></s>
Forgetfulness
<s sid="369"><CoreSc1 advantage="None" conceptID="Res118" novelty="None" type="Res"/><text>Examples 3.5 and 3.6 of InvChain and Osc reveal a crucial insight about the behavior of compound modules.</text></s>
<s sid="370"><CoreSc1 advantage="None" conceptID="Res119" novelty="None" type="Res"/><text>While InvChain behaves like a fixed-delay channel in all feasible executions, it cannot be said for Osc that it always behaves like an oscillator of frequency 1/(2d).</text></s>
<s sid="371"><CoreSc1 advantage="None" conceptID="Res120" novelty="None" type="Res"/><text>Even though Osc will run at the fixed frequency of 1/(2d) if it is feasible at all times, an inconsistent initialization or a transient fault can cause it to run forever at an arbitrarily high frequency.</text></s>
<s sid="372"><CoreSc1 advantage="None" conceptID="Res121" novelty="None" type="Res"/><text>In general, when devising compound modules, typically our design goal will be to implement a certain basic module, that is, essentially ensure that the feasible executions of the compound module are also feasible according to the (usually more idealized) specification of the respective basic module.</text></s>
<s sid="373"><CoreSc1 advantage="None" conceptID="Res122" novelty="None" type="Res"/><text>For InvChain, the intended basic module is a fixed-delay channel, where for Osc we had an (externally started) oscillator of frequency 1/(2d) in mind.</text></s>
<s sid="374"><CoreSc1 advantage="None" conceptID="Res123" novelty="None" type="Res"/><text>Since we strive for self-stabilization properties, intuitively InvChain would be a &quot;good&quot; implementation of a fixed-delay channel, whereas Osc is not satisfactory because it does not recover from transient faults.</text></s>
<s sid="375"><CoreSc1 advantage="None" conceptID="Res124" novelty="None" type="Res"/><text>Before we formalize the concepts of implementation and self-stabilization, let us get a better understanding of the critical difference between InvChain and Osc.</text></s>
<s sid="376"><CoreSc1 advantage="None" conceptID="Res125" novelty="None" type="Res"/><text>In both cases, the output depends on past inputs, so both compound modules have a sort of memory.</text></s>
<s sid="377"><CoreSc1 advantage="None" conceptID="Res126" novelty="None" type="Res"/><text>In any real-world system, this cannot be avoided since physics entails positive delays for any building block we might use.</text></s>
<s sid="378"><CoreSc1 advantage="None" conceptID="Res127" novelty="None" type="Res"/><text>However, InvChain eventually &quot;forgets&quot; about previous inputs, while Osc contains a feedback-loop that, in a feasible execution, determines the future output as a periodic function of the first d time units of the execution.</text></s>
<s sid="379"><CoreSc1 advantage="None" conceptID="Res128" novelty="None" type="Res"/><text>This motivates the following definitions.</text></s>
<s sid="380"><CoreSc1 advantage="None" conceptID="Res129" novelty="None" type="Res"/><text>For F∈R0+, we say that a module is F-forgetful, iff its specification permits the following construction:1.</text></s>
<s sid="381"><CoreSc1 advantage="None" conceptID="Obs14" novelty="None" type="Obs"/><text>Take an arbitrary feasible execution E of M on some interval [t-,t+]⊆R.</text></s>
<s sid="382"><CoreSc1 advantage="None" conceptID="Obs15" novelty="None" type="Obs"/><text>Denote by Ein and Eout its restrictions to the input and output ports of M, respectively.</text></s>
2.
<s sid="383"><CoreSc1 advantage="None" conceptID="Obs16" novelty="None" type="Obs"/><text>Restrict Eout to Eout′ on [t-+F,t+).</text></s>
<s sid="384"><CoreSc1 advantage="None" conceptID="Res130" novelty="None" type="Res"/><text>Then for each execution Ein′ on R whose restriction to [t-,t+] equals Ein, there is a feasible execution of M on R whose respective restrictions to the input and output ports (and in time) are Ein′ and Eout′.</text></s>
<s sid="385"><CoreSc1 advantage="None" conceptID="Res131" novelty="None" type="Res"/><text>We say a module is forgetful iff it is F-forgetful for some F∈R0+.</text></s>
<s sid="386"><CoreSc1 advantage="None" conceptID="Res132" novelty="None" type="Res"/><text>Essentially, F-forgetfulness requires that feasibility of the output at time t merely depends on the inputs during [t-F,t].</text></s>
<s sid="387"><CoreSc1 advantage="None" conceptID="Res133" novelty="None" type="Res"/><text>That is, the effects of earlier inputs, as well as all traces of a possible transient fault, wear out from the internal state within time F.</text></s>
<s sid="388"><CoreSc1 advantage="None" conceptID="Res134" novelty="None" type="Res"/><text>Note that 0-forgetful modules are exactly those with extendable specifications, i.e., forgetfulness is a generalization of extendability.</text></s>
<s sid="389"><CoreSc1 advantage="None" conceptID="Res135" novelty="None" type="Res"/><text>Let the circuit graph of a compound module M be the directed graph whose nodes are the submodules SM, and for each port p that is an output port of S∈SM and an input port of S′∈SM it contains a (directed) edge from S to S′.</text></s>
<s sid="390"><CoreSc1 advantage="None" conceptID="Res136" novelty="None" type="Res"/><text>We recursively define that M is feedback-free iff (i) all its basic submodules are forgetful, (ii) all its compound submodules are feedback-free, and (iii) its circuit graph is acyclic.</text></s>
<s sid="391"><CoreSc1 advantage="None" conceptID="Obj57" novelty="None" type="Obj"/><text>Finally, we define the idealized basic module ι(M) corresponding to compound module M (with the same ports as M) by execution E of ι(M) being feasible iff it is the restriction of a feasible execution of M on R.</text></s>
<s sid="392"><CoreSc1 advantage="None" conceptID="Obs17" novelty="None" type="Obs"/><text>According to these definitions, InvChain is clearly 2d-forgetful and feedback-free.</text></s>
<s sid="393"><CoreSc1 advantage="None" conceptID="Obs18" novelty="None" type="Obs"/><text>Since we observed that Φβ(InvChain) is extendable, it is in fact 0-forgetful and its corresponding basic and idealized basic modules are identical.</text></s>
<s sid="394"><CoreSc1 advantage="None" conceptID="Res137" novelty="None" type="Res"/><text>In contrast, Osc satisfies neither of these properties.</text></s>
<s sid="395"><CoreSc1 advantage="None" conceptID="Res138" novelty="None" type="Res"/><text>Note, however, that replacing a basic module in InvChain by a compound module might result in a module that does not have these properties either.</text></s>
<s sid="396"><CoreSc1 advantage="None" conceptID="Res139" novelty="None" type="Res"/><text>We are now in the position to formulate a theorem that states that any feedback-free module will eventually behave like its idealized basic module in a feasible execution.</text></s>
Theorem 3.7
<s sid="397"><CoreSc1 advantage="None" conceptID="Res140" novelty="None" type="Res"/><text>Suppose M is a feedback-free compound module.</text></s>
<s sid="398"><CoreSc1 advantage="None" conceptID="Obs19" novelty="None" type="Obs"/><text>Let P be the set of paths in its circuit graph and submodule S∈SM be FS-forgetful.</text></s>
<s sid="399"><CoreSc1 advantage="None" conceptID="Obs20" novelty="None" type="Obs"/><text>Then M is F-forgetful withF:=max(S1,…,Sk)∈P{∑i=1kFSi}.</text></s>
Proof
<s sid="400"><CoreSc1 advantage="None" conceptID="Goa19" novelty="None" type="Goa"/><text>Consider an arbitrary execution E on [t-,t+) on the ports of M and its submodules whose restriction to the ports of M (and thus also its restrictions to each submodule of M) is feasible, its restrictions Ein and Eout to the input and output ports of M, the restriction Eout′ of Eout to [t-+F,t+), and an arbitrary execution Ein′ on R satisfying that its restriction to [t-,t+) equals EI.</text></s>
Denote for each S∈SMFS,M:=max(S1,S2,…,Sk=S)∈P{∑i=1kFSi}.
Note that maxS∈SM{FS,M}=F.
<s sid="401"><CoreSc1 advantage="None" conceptID="Goa20" novelty="None" type="Goa"/><text>By induction on the submodules of M, we will construct a feasible execution Eimax on the ports of M and its submodules on R that is feasible for M when restricted to the ports of M and whose respective restrictions equal Ein′ and Eout′.</text></s>
<s sid="402"><CoreSc1 advantage="None" conceptID="Goa21" novelty="None" type="Goa"/><text>In each step of the induction, we will add the output ports of some submodule of M to the execution.</text></s>
<s sid="403"><CoreSc1 advantage="None" conceptID="Res141" novelty="None" type="Res"/><text>The induction halts once we run out of submodules of M after imax⩽|SM| steps.</text></s>
<s sid="404"><CoreSc1 advantage="None" conceptID="Res142" novelty="None" type="Res"/><text>The induction hypothesis states for the execution Ei on the union of the input ports of M and the ports of a subset of its submodules that•</text></s>
<s sid="405"><CoreSc1 advantage="None" conceptID="Res143" novelty="None" type="Res"/><text>its respective restrictions to submodules are feasible,</text></s>
•
<s sid="406"><CoreSc1 advantage="None" conceptID="Res144" novelty="None" type="Res"/><text>its restriction to the input ports of M equals Ein′,</text></s>
•
<s sid="407"><CoreSc1 advantage="None" conceptID="Res145" novelty="None" type="Res"/><text>its restriction to output ports of M on [t-+F,t+) matches the restriction of Eout′ to such ports, and</text></s>
•
<s sid="408"><CoreSc1 advantage="None" conceptID="Mod22" novelty="None" type="Mod"/><text>for each submodule S∈SM whose execution ES is already fully specified by Ei, we have that ES restricted to [t-+FS,M,t+)⊇[t-+F,t+) equals the corresponding restriction of E.</text></s>
<s sid="409"><CoreSc1 advantage="None" conceptID="Mod23" novelty="None" type="Mod"/><text>Note that these properties together prove the theorem, since they show the claims we made on the properties of Eimax, and E and Ein are chosen arbitrarily (respecting the constraints imposed by the definition of forgetfulness).</text></s>
<s sid="410"><CoreSc1 advantage="None" conceptID="Mod24" novelty="None" type="Mod"/><text>We set E0:=Ein′ and anchor the induction at E0, trivially satisfying the induction hypothesis for i=0 and guaranteeing that all Ei restricted to the input ports of M equal Ein′.1515</text></s>
<s sid="411"><CoreSc1 advantage="None" conceptID="Res146" novelty="None" type="Res"/><text>Note that if any submodule is already covered by E0, it must not have any output port and due to non-emptiness the only possible (trivial) execution on the output ports is always feasible.</text></s>
<s sid="412"><CoreSc1 advantage="None" conceptID="Con20" novelty="None" type="Con"/><text>Recall that the input ports of M cannot be output ports of its submodules.</text></s>
<s sid="413"><CoreSc1 advantage="None" conceptID="Con21" novelty="None" type="Con"/><text>Any other ports of M and its submodules are output ports of a submodule, hence the final execution will indeed contain all ports and thus cover all submodules.</text></s>
<s sid="414"><CoreSc1 advantage="None" conceptID="Met51" novelty="None" type="Met"/><text>To perform the induction step, suppose we have already constructed Ei-1 for some i∈[1..imax].</text></s>
<s sid="415"><CoreSc1 advantage="None" conceptID="Obs21" novelty="None" type="Obs"/><text>By definition of imax, there is an uncovered submodule left, i.e., Ei-1 does not specify the execution on all ports of all submodules.</text></s>
<s sid="416"><CoreSc1 advantage="None" conceptID="Res147" novelty="None" type="Res"/><text>Because M is feedback-free, its circuit graph is acyclic.</text></s>
<s sid="417"><CoreSc1 advantage="None" conceptID="Res148" novelty="None" type="Res"/><text>Consider the subgraph of the circuit graph induced by the remaining uncovered submodules.</text></s>
<s sid="418"><CoreSc1 advantage="None" conceptID="Res149" novelty="None" type="Res"/><text>As it is acyclic as well, there must be a module S without an incoming edge, implying that Ei-1 specifies executions of all its input ports.</text></s>
<s sid="419"><CoreSc1 advantage="None" conceptID="Res150" novelty="None" type="Res"/><text>By the induction hypothesis, we have that these executions restricted to [t-+FS,M-FS,t+) equal the respective restrictions of E: Any such port is either an input port of M and therefore its executions in Ei-1 and E are identical on [t-,t+), or it is an output port of some module S′ satisfying that there is some path (S1,…,Sk-1=S′,Sk=S)∈P and therefore the executions of the port in Ei-1 and E are identical on [t-+FS′,M,t+)⊇[t-+FS,M-FS,t+).</text></s>
<s sid="420"><CoreSc1 advantage="None" conceptID="Res151" novelty="None" type="Res"/><text>Since the output ports of S cannot be output ports of other modules and the input ports of M cannot be output ports of submodules, Ei-1 does not specify executions for any of the output ports of S.</text></s>
<s sid="421"><CoreSc1 advantage="None" conceptID="Res152" novelty="None" type="Res"/><text>We apply that S is FS-forgetful to the restriction of E to the ports of S on the interval [t-+FS,M-FS,t+) and the input execution given by the restriction of Ei-1 to the input ports of S, which is possible since we observed that the restrictions of these executions to the input ports of S on [t-+FS,M-FS,t+) are identical.</text></s>
<s sid="422"><CoreSc1 advantage="None" conceptID="Res153" novelty="None" type="Res"/><text>We obtain a feasible execution of S on R whose restriction to the input ports matches the restriction of Ei-1 to these ports and whose restriction to the output ports and [t-+FS,M,t+) matches the respective restriction of E.</text></s>
<s sid="423"><CoreSc1 advantage="None" conceptID="Res154" novelty="None" type="Res"/><text>We define Ei by adding these executions of the output ports of S to Ei-1.</text></s>
<s sid="424"><CoreSc1 advantage="None" conceptID="Con22" novelty="None" type="Con"/><text>Overall, the first, second, and fourth claim of the induction hypothesis thus hold by construction.</text></s>
<s sid="425"><CoreSc1 advantage="None" conceptID="Con23" novelty="None" type="Con"/><text>Taking into account that [t-+FS,M,t+)⊇[t-+F,t+) and Eout′ is a restriction of E, the same holds for the third claim.</text></s>
<s sid="426"><CoreSc1 advantage="None" conceptID="Con24" novelty="None" type="Con"/><text>Hence the induction step succeeds, finishing the proof. □</text></s>
<s sid="427"><CoreSc1 advantage="None" conceptID="Con25" novelty="None" type="Con"/><text>As a consequence, any feedback-free compound module will eventually &quot;forget&quot; about transient faults and behave according to some fault-free history.</text></s>
<s sid="428"><CoreSc1 advantage="None" conceptID="Con26" novelty="None" type="Con"/><text>Note that in general that does not mean that if the module's execution is feasible on two separate intervals there is any guaranteed relation between the behavior on these intervals, as arbitrary transient faults result in arbitrary modifications of the module's state.</text></s>
<s sid="429"><CoreSc1 advantage="None" conceptID="Con27" novelty="None" type="Con"/><text>However, we can assume that such a module, once it becomes non-faulty and thus follows its specification, eventually behaves correctly in the strong sense given by the specification of its corresponding idealized basic module.</text></s>
<s sid="430"><CoreSc1 advantage="None" conceptID="Con28" novelty="None" type="Con"/><text>The bad news is that we have to introduce feedback-loops into the system at some point: After all, any clock source is some kind of oscillator.</text></s>
<s sid="431"><CoreSc1 advantage="None" conceptID="Con29" novelty="None" type="Con"/><text>As demonstrated by Osc, we cannot expect a strong generic result like Theorem 3.7 for compound modules that are not feedback-free.</text></s>
<s sid="432"><CoreSc1 advantage="None" conceptID="Res155" novelty="None" type="Res"/><text>Also, clearly such a structure cannot be forgetful.</text></s>
<s sid="433"><CoreSc1 advantage="None" conceptID="Res156" novelty="None" type="Res"/><text>Hence, let us turn to a less restrictive concept of recovery from transient faults.</text></s>
Self-stabilization
<s sid="434"><CoreSc1 advantage="None" conceptID="Res157" novelty="None" type="Res"/><text>Self-stabilization is the property of a system to recover from arbitrary transient faults in finite time, provided that all transient faults cease.</text></s>
<s sid="435"><CoreSc1 advantage="None" conceptID="Con30" novelty="None" type="Con"/><text>Since arbitrary transient faults may result in arbitrary states, this requirement can be rephrased as the need to reach a valid state within finite time from any initial state.</text></s>
<s sid="436"><CoreSc1 advantage="None" conceptID="Con31" novelty="None" type="Con"/><text>For basic modules, our framework encapsulates this concept by the notion of feasibility; executions completely hide the internal state of a basic module, and we assume (or hope) that the utilized implementation of the module will recover from faults that are considered transient, e.g.</text></s>
<s sid="437"><CoreSc1 advantage="None" conceptID="Con32" novelty="None" type="Con"/><text>particle hits, crosstalk, magnetic fields, or power outages.</text></s>
<s sid="438"><CoreSc1 advantage="None" conceptID="Con33" novelty="None" type="Con"/><text>For compound modules, one possible meaning of &quot;valid state&quot; in our context is given by the specification of the corresponding idealized basic module: If, viewed from the outside, the execution is indistinguishable from one that could occur if the module was entirely fault-free, we can safely assume that its state is valid; any fault that is masked is irrelevant to the higher layers of the system anyway, as it solely relies on the guarantees of the utilized model specification on the module's ports' executions.</text></s>
<s sid="439"><CoreSc1 advantage="None" conceptID="Obj58" novelty="None" type="Obj"/><text>A more general concretization of the same underlying idea is the notion of self-stabilization we present next.</text></s>
<s sid="440"><CoreSc1 advantage="None" conceptID="Obj59" novelty="None" type="Obj"/><text>It introduces two relaxations of the constraints on the behavior of a module.</text></s>
<s sid="441"><CoreSc1 advantage="None" conceptID="Con34" novelty="None" type="Con"/><text>Firstly, we do not expect that the recovering module is perceived as functional by an outsider immediately after its execution becomes feasible.</text></s>
<s sid="442"><CoreSc1 advantage="None" conceptID="Con35" novelty="None" type="Con"/><text>Rather, we allow for some stabilization time during which the module can &quot;clean up&quot; its internal state.</text></s>
<s sid="443"><CoreSc1 advantage="None" conceptID="Con36" novelty="None" type="Con"/><text>Secondly, we do not require that the module fulfills its corresponding idealized basic module specification.</text></s>
<s sid="444"><CoreSc1 advantage="None" conceptID="Con37" novelty="None" type="Con"/><text>Instead, we choose another, weaker specification that is sufficient for our purposes.</text></s>
<s sid="445"><CoreSc1 advantage="None" conceptID="Res158" novelty="None" type="Res"/><text>One specification being stronger than another is captured by the following notion.</text></s>
<s sid="446"><CoreSc1 advantage="None" conceptID="Res159" novelty="None" type="Res"/><text>We define that module M implements module M′ iff both modules have the same sets of input and output ports and ΦM(Ein)⊆ΦM′(Ein) for all Ein.</text></s>
<s sid="447"><CoreSc1 advantage="None" conceptID="Con38" novelty="None" type="Con"/><text>Thus, ΦM is stronger than ΦM′ in the sense that it imposes at least as many constraints on the output executions as ΦM′.</text></s>
<s sid="448"><CoreSc1 advantage="None" conceptID="Con39" novelty="None" type="Con"/><text>For T∈R0+, module M now is a T-stabilizing implementation of module M′, iff the restriction of each feasible execution of M on [t-,t+)⊆R to [t-+T,t+) is a feasible execution of M′.</text></s>
<s sid="449"><CoreSc1 advantage="None" conceptID="Con40" novelty="None" type="Con"/><text>Clearly, for T&gt;0, this allows for ΦM(Ein)⊈ΦM′(Ein), i.e., a T-stabilizing implementation of M′ needs not be an implementation of M′.</text></s>
<s sid="450"><CoreSc1 advantage="None" conceptID="Con41" novelty="None" type="Con"/><text>For brevity, we may say that &quot;M is T-stabilizing&quot; to express that M is a T-stabilizing implementation of its corresponding idealized basic module.</text></s>
<s sid="451"><CoreSc1 advantage="None" conceptID="Obj60" novelty="None" type="Obj"/><text>We simply say M is a self-stabilizing (implementation of M′) iff it is a T-stabilizing (implementation of M′) for some T∈R0+.</text></s>
<s sid="452"><CoreSc1 advantage="None" conceptID="Obj61" novelty="None" type="Obj"/><text>From our previous definitions and results, we immediately can derive the following statements.</text></s>
Lemma 3.8
1.
<s sid="453"><CoreSc1 advantage="None" conceptID="Obs22" novelty="None" type="Obs"/><text>Every F-forgetful module is F-stabilizing.</text></s>
2.
<s sid="454"><CoreSc1 advantage="None" conceptID="Obs23" novelty="None" type="Obs"/><text>Every feedback-free compound module is self-stabilizing.</text></s>
3.
<s sid="455"><CoreSc1 advantage="None" conceptID="Obs24" novelty="None" type="Obs"/><text>If M is a self-stabilizing implementation of M′ and ΦM is extendable, then M is an implementation of M′.</text></s>
4.
<s sid="456"><CoreSc1 advantage="None" conceptID="Res160" novelty="None" type="Res"/><text>Every feedback-free compound module whose specification is extendable implements its corresponding idealized basic module.</text></s>
 Proof
<s sid="457"><CoreSc1 advantage="None" conceptID="Res161" novelty="None" type="Res"/><text>The first statement follows directly from the definition of F-forgetfulness.</text></s>
<s sid="458"><CoreSc1 advantage="None" conceptID="Res162" novelty="None" type="Res"/><text>The second statement follows from Theorem 3.7 and the first statement.</text></s>
<s sid="459"><CoreSc1 advantage="None" conceptID="Con42" novelty="None" type="Con"/><text>Regarding the third statement, by extendability every feasible execution of M on some interval I⊆R is the restriction of a feasible execution E of M on R.</text></s>
<s sid="460"><CoreSc1 advantage="None" conceptID="Con43" novelty="None" type="Con"/><text>By the definition of self-stabilizing implementations, there is some T∈R0+ such that E restricted to (-∞+T,∞)=R is feasible for M′.</text></s>
<s sid="461"><CoreSc1 advantage="None" conceptID="Con44" novelty="None" type="Con"/><text>By properness, restricting E to I yields a feasible execution of M′.</text></s>
<s sid="462"><CoreSc1 advantage="None" conceptID="Con45" novelty="None" type="Con"/><text>Since this restriction equals the original feasible execution of M, it follows that every feasible execution of M is a feasible execution of M′.</text></s>
<s sid="463"><CoreSc1 advantage="None" conceptID="Res163" novelty="None" type="Res"/><text>The last statement follows from the second and third. □</text></s>
<s sid="464"><CoreSc1 advantage="None" conceptID="Res164" novelty="None" type="Res"/><text>Recall that InvChain from Example 3.5 behaves like a fixed-delay channel with delay 2d.</text></s>
<s sid="465"><CoreSc1 advantage="None" conceptID="Res165" novelty="None" type="Res"/><text>As InvChain is feedback-free and its specification is extendable, we could also infer this statement directly from Lemma 3.8.</text></s>
<s sid="466"><CoreSc1 advantage="None" conceptID="Res166" novelty="None" type="Res"/><text>In contrast, Osc is not self-stabilizing at all, i.e., it is not self-stabilizing for any T∈R0+, which follows from the same arguments that we used to show that its specification is not extendable.</text></s>
<s sid="467"><CoreSc1 advantage="None" conceptID="Con46" novelty="None" type="Con"/><text>In general, determining whether a module M is a self-stabilizing implementation of some other module M′ (or even bounding the stabilization time) requires a detailed stabilization analysis.</text></s>
<s sid="468"><CoreSc1 advantage="None" conceptID="Con47" novelty="None" type="Con"/><text>One may as well generalize the above definition to also account for probabilistic stabilization by defining an appropriate probability space on the set of executions of M. For the sake of brevity and better readability, we only informally state the probability space in this work by introducing basic modules that act in a probabilistic manner, namely the Randomized Watchdog Timers.</text></s>
<s sid="469"><CoreSc1 advantage="None" conceptID="Obj62" novelty="None" type="Obj"/><text>Probabilistic bounds on the stabilization time of compound implementations then are naturally derived from the respective distributions of their submodules.</text></s>
<s sid="470"><CoreSc1 advantage="None" conceptID="Obj63" novelty="None" type="Obj"/><text>The interested reader is referred to [13], where we give a precise definition of the probability space over which our probabilistic stabilization guarantees are expressed.</text></s>
Persistent faults
<s sid="471"><CoreSc1 advantage="None" conceptID="Obj64" novelty="None" type="Obj"/><text>We next generalize the definition of feasibility for compound modules to potentially faulty submodules.</text></s>
<s sid="472"><CoreSc1 advantage="None" conceptID="Mod25" novelty="None" type="Mod"/><text>For a compound module M and f⩾0, we define the corresponding f-faulty basic module βf(M) as follows: βf(M)'s input and output ports are the input and output ports of M. Let E be any execution of M on I⊆R and denote by Ein and Eout its restrictions to the input and output ports of M, respectively.</text></s>
<s sid="473"><CoreSc1 advantage="None" conceptID="Res167" novelty="None" type="Res"/><text>Then Eout∈Φβf(M)(Ein), iff there exists an execution EM,SM of all ports of modules in {M}∪SM and a subset F of SM of size at most f, such that, for each module S in SM∖F, the restriction of execution EM,SM to the ports of S is feasible, and EM,SM restricted to M's input and output ports is equal to E. As in the fault-free case, we require that Φβf(M) fulfills non-emptiness.</text></s>
<s sid="474"><CoreSc1 advantage="None" conceptID="Res168" novelty="None" type="Res"/><text>For compound module M and f⩾0, we say an execution E of M is f-faulty feasible iff it is feasible for the corresponding f-faulty basic module βf(M).</text></s>
<s sid="475"><CoreSc1 advantage="None" conceptID="Res169" novelty="None" type="Res"/><text>Modules whose behavior is unrestricted in execution E (i.e., that belong to the set F) are called faulty (in execution E).</text></s>
<s sid="476"><CoreSc1 advantage="None" conceptID="Res170" novelty="None" type="Res"/><text>Clearly, a compound module M cannot mask faults of submodules that have an output port that is also output port of M. Consequently, tolerance of ongoing faults necessitates to weaken the constraints on M's output.</text></s>
<s sid="477"><CoreSc1 advantage="None" conceptID="Con48" novelty="None" type="Con"/><text>Hence, for f⩾0, compound module M, and some other module M′ with the same set of input and output ports, we say that M is an f-tolerant implementation of M′ iff every f-faulty feasible execution of M is a feasible execution of M′, i.e., the corresponding f-faulty basic module of M implements M′.</text></s>
<s sid="478"><CoreSc1 advantage="None" conceptID="Con49" novelty="None" type="Con"/><text>Analogously, M is an f-tolerant T-stabilizing implementation of M′ iff its corresponding f-faulty basic module is a T-stabilizing implementation of M′.</text></s>
<s sid="479"><CoreSc1 advantage="None" conceptID="Con50" novelty="None" type="Con"/><text>Note that this entails that the union of output ports of any set consisting of f submodules of M can be arbitrary.</text></s>
<s sid="480"><CoreSc1 advantage="None" conceptID="Con51" novelty="None" type="Con"/><text>The module specification of M′ thus must not put concurrent restrictions on all its output ports to allow for fault-tolerance.</text></s>
<s sid="481"><CoreSc1 advantage="None" conceptID="Con52" novelty="None" type="Con"/><text>Hence, one demands constraints on the outputs of non-faulty submodules of M (i.e., those whose executions are feasible) only.</text></s>
<s sid="482"><CoreSc1 advantage="None" conceptID="Res171" novelty="None" type="Res"/><text>Formal specification of clock synchronization protocols</text></s>
<s sid="483"><CoreSc1 advantage="None" conceptID="Res172" novelty="None" type="Res"/><text>In the formal framework introduced above, a node is simply a compound module that will operate according to some specification whenever it is non-faulty.</text></s>
<s sid="484"><CoreSc1 advantage="None" conceptID="Obj65" novelty="None" type="Obj"/><text>We now specify the submodules of a node introduced informally in Section 2.2 and reveal how they are connected.</text></s>
<s sid="485"><CoreSc1 advantage="None" conceptID="Mod26" novelty="None" type="Mod"/><text>The reader might want to take a look at Fig. 5 to get an idea of the general layout of a node at this point.</text></s>
<s sid="486"><CoreSc1 advantage="None" conceptID="Mod27" novelty="None" type="Mod"/><text>Each node i∈[1..n] has n input ports Si,j, where j∈[1..n], and n+1 output ports, namely Li and Sj,i for all j∈[1..n].</text></s>
<s sid="487"><CoreSc1 advantage="None" conceptID="Mod28" novelty="None" type="Mod"/><text>We present all submodules as basic modules whose specifications are sufficiently strong to implement the model used in [13].</text></s>
<s sid="488"><CoreSc1 advantage="None" conceptID="Res173" novelty="None" type="Res"/><text>As a result, not all specifications can be satisfied by (physical) implementations of the modules for all input executions; we discuss these limitations and their implications in Section 6.</text></s>
<s sid="489"><CoreSc1 advantage="None" conceptID="Res174" novelty="None" type="Res"/><text>All of the following specifications are extendable and therefore, by Lemma 3.3, are uniquely characterized by describing them on executions on R only.</text></s>
<s sid="490"><CoreSc1 advantage="None" conceptID="Res175" novelty="None" type="Res"/><text>It is trivial to verify non-emptyness for the given specifications, hence omit respective discussions.•</text></s>
Communication channels.
<s sid="491"><CoreSc1 advantage="None" conceptID="Res176" novelty="None" type="Res"/><text>For each j∈[1..n], there is a communication channel of delay dChan∈R+ from node i's internal port Si to its output port Sj,i.</text></s>
<s sid="492"><CoreSc1 advantage="None" conceptID="Res177" novelty="None" type="Res"/><text>Formally, this communication channel is a basic module with input port Si and output port Sj,i.</text></s>
<s sid="493"><CoreSc1 advantage="None" conceptID="Res178" novelty="None" type="Res"/><text>The module specification ΦChan of the communication channel is as follows.</text></s>
<s sid="494"><CoreSc1 advantage="None" conceptID="Res179" novelty="None" type="Res"/><text>Let Ein be an execution of input port Si and Eout an execution of output port Sj,i.</text></s>
<s sid="495"><CoreSc1 advantage="None" conceptID="Res180" novelty="None" type="Res"/><text>Then Eout∈ΦChan(Ein) iff there exists a continuous, strictly increasing (and thus invertible) delay function τ:R→R such that for all t∈R both (i) Sj,i(t)=Si(τ-1(t)) and (ii) 0⩽t-τ-1(t)&lt;dChan hold.</text></s>
<s sid="496"><CoreSc1 advantage="None" conceptID="Res181" novelty="None" type="Res"/><text>It is important to note that the assumptions on the communication channels are strictly weaker than those on fixed-delay channels, as the delay of a communication channel may vary arbitrarily (within bounds) during an execution.</text></s>
•
Memory flags.
<s sid="497"><CoreSc1 advantage="None" conceptID="Res182" novelty="None" type="Res"/><text>For each state s∈S and each node j∈[1..n], there is a memory flag Memi,j,s at node i.</text></s>
<s sid="498"><CoreSc1 advantage="None" conceptID="Res183" novelty="None" type="Res"/><text>It has two input ports, namely Si,j and a binary reset port, as well as a binary output port Memi,j,s whose name is for simplicity identical to the memory flag's name.</text></s>
<s sid="499"><CoreSc1 advantage="None" conceptID="Res184" novelty="None" type="Res"/><text>Given an execution Ein of the flag's input ports, an execution Eout of the output port is in ΦMem(Ein) iff properties (Reset) and (Set) hold:-</text></s>
<s sid="500"><CoreSc1 advantage="None" conceptID="Res185" novelty="None" type="Res"/><text>(Reset) For all times t∈R, Memi,j,s(t)=0 iff the reset port has been in state 1 at some time between sup{t′∈(-∞,t]|Si,j(t′)=s} and t.</text></s>
-
<s sid="501"><CoreSc1 advantage="None" conceptID="Res186" novelty="None" type="Res"/><text>(Set) For all times t∈R, Memi,j,s(t)=1 iff the reset port continuously has been in state 0 between sup{t′∈(-∞,t]|Si,j(t′)=s} and t.</text></s>
<s sid="502"><CoreSc1 advantage="None" conceptID="Res187" novelty="None" type="Res"/><text>We say that node i memorizes node j in state s at time t iff Memi,j,s(t)=1.</text></s>
•
Threshold gates.
<s sid="503"><CoreSc1 advantage="None" conceptID="Res188" novelty="None" type="Res"/><text>Node i may comprise an arbitrary number of threshold gates with arbitrary thresholds.</text></s>
<s sid="504"><CoreSc1 advantage="None" conceptID="Res189" novelty="None" type="Res"/><text>The module specification ΦThr of a threshold gate with binary input ports a1,…,am, binary output port o, and threshold k∈[1..m] is defined as follows.</text></s>
<s sid="505"><CoreSc1 advantage="None" conceptID="Res190" novelty="None" type="Res"/><text>Let Ein be an execution of the input ports and Eout an execution of output port o.</text></s>
<s sid="506"><CoreSc1 advantage="None" conceptID="Res191" novelty="None" type="Res"/><text>Then Eout∈ΦThr(EI) iff for all t∈R, o(t)=1 if at least k input ports are in state 1 at time t, and o(t)=0 otherwise.</text></s>
•
Watchdog timers.
<s sid="507"><CoreSc1 advantage="None" conceptID="Res192" novelty="None" type="Res"/><text>For each state s∈S, there can be watchdog timers (T,s,C) at node i, where T∈R+ is the duration of the timer and C is a clock.</text></s>
<s sid="508"><CoreSc1 advantage="None" conceptID="Res193" novelty="None" type="Res"/><text>The watchdog timer has input port Si,i and a binary output port TimeT,s,C.</text></s>
<s sid="509"><CoreSc1 advantage="None" conceptID="Res194" novelty="None" type="Res"/><text>The timer's module specification ΦTime is defined as follows.</text></s>
<s sid="510"><CoreSc1 advantage="None" conceptID="Res195" novelty="None" type="Res"/><text>Let Ein be an execution of the timer's input port Si,i and Eout an execution of its output port TimeT,s,C.</text></s>
<s sid="511"><CoreSc1 advantage="None" conceptID="Res196" novelty="None" type="Res"/><text>Then Eout∈ΦTime(Ein), iff the following holds:-</text></s>
<s sid="512"><CoreSc1 advantage="None" conceptID="Res197" novelty="None" type="Res"/><text>(Clock) Clock C is correct at all times, i.e., t′-t⩽C(t′)-C(t)⩽ϑ(t′-t) for all t,t′∈R, t&lt;t′.</text></s>
-
<s sid="513"><CoreSc1 advantage="None" conceptID="Res198" novelty="None" type="Res"/><text>(Reset) There exists a (binary) signal σT,s,C∈[TimeT,s,C] (the equivalence class of the output port's signal) such that, for each time ts∈R when Si,i switches to state s, there is a time t∈[ts,ts+dTime] such that (T,s,C) is reset, i.e., event (0,t)∈σT,s,C.</text></s>
<s sid="514"><CoreSc1 advantage="None" conceptID="Res199" novelty="None" type="Res"/><text>This is a one-to-one correspondence, i.e., for each such ts this time t is unique and (T,s,C) is not reset at any other times.</text></s>
-
<s sid="515"><CoreSc1 advantage="None" conceptID="Obs25" novelty="None" type="Obs"/><text>(Expire) Denote by R⊂R the set of times when (T,s,C) is reset.</text></s>
<s sid="516"><CoreSc1 advantage="None" conceptID="Res200" novelty="None" type="Res"/><text>For each time tR∈R, denote by tE(tR) the unique time satisfying that C(tE(tR))-C(tR)=T.</text></s>
<s sid="517"><CoreSc1 advantage="None" conceptID="Res201" novelty="None" type="Res"/><text>Then, for each t∈R, TimeT,s,C(t)=0 iff t∈⋃tR∈R[tR,tE(tR)).</text></s>
<s sid="518"><CoreSc1 advantage="None" conceptID="Res202" novelty="None" type="Res"/><text>Iff (tE(tR),1)∈TimeT,s,C, i.e., (T,s,C) is not reset during (tR,tE(tR)] again and hence TimeT,s,C switches to 1 at time tE(tR), we say that (T,s,C) expires at time tE(tR).</text></s>
<s sid="519"><CoreSc1 advantage="None" conceptID="Res203" novelty="None" type="Res"/><text>(T,s,C) is expired at time t∈R iff TimeT,s,C(t)=1.</text></s>
<s sid="520"><CoreSc1 advantage="None" conceptID="Res204" novelty="None" type="Res"/><text>For notational convenience, we will omit the clock C and simply write (T,s) for both the timeout and its signal.</text></s>
•
Randomized watchdog timers.
<s sid="521"><CoreSc1 advantage="None" conceptID="Res205" novelty="None" type="Res"/><text>A randomized watchdog timer (D,s,C) is a module with input port Si,i and output port TimeD,s,C, where D is a bounded random distribution on (0,D]⊂R+, s is a state, and C a clock.</text></s>
<s sid="522"><CoreSc1 advantage="None" conceptID="Res206" novelty="None" type="Res"/><text>The module specification of (D,s,C) is analogous to the module specification of a watchdog timer, except that property (Expire) is replaced by:-</text></s>
<s sid="523"><CoreSc1 advantage="None" conceptID="Obs26" novelty="None" type="Obs"/><text>(Expire') Denote by R⊂R the set of times when (T,s,C) is reset.</text></s>
<s sid="524"><CoreSc1 advantage="None" conceptID="Mod29" novelty="None" type="Mod"/><text>For each time tR∈R, denote by tE(tR) the unique time satisfying that C(tE)-C(tR)=T(tR), where T(tR) is drawn (independently) from D.</text></s>
<s sid="525"><CoreSc1 advantage="None" conceptID="Mod30" novelty="None" type="Mod"/><text>Then, for each t∈R, TimeT,s,C(t)=0 iff t∈⋃tR∈R[tR,tE(tR)).</text></s>
<s sid="526"><CoreSc1 advantage="None" conceptID="Mod31" novelty="None" type="Mod"/><text>We apply the same notational conventions as for watchdog timers.</text></s>
•
State transition modules.
<s sid="527"><CoreSc1 advantage="None" conceptID="Mod32" novelty="None" type="Mod"/><text>Node i's state transition module has input ports Si,j for each node j∈[1..n] as well as one binary input port for each of the memory flags, (randomized) watchdog timers and threshold gates it uses.</text></s>
<s sid="528"><CoreSc1 advantage="None" conceptID="Mod33" novelty="None" type="Mod"/><text>Furthermore it has an output port Si as well as one binary Reset output port for each of the memory flags it uses.</text></s>
<s sid="529"><CoreSc1 advantage="None" conceptID="Mod34" novelty="None" type="Mod"/><text>A node's state transition module executes a state machine specified by (i) a finite set S of states, (ii) a function tr, called the transition function, from T⊆S2 to the set of Boolean predicates on the alphabet consisting of expressions of the form &quot;p=s&quot; (used for expressing guards), where p is from the state transition module's input ports and s is from the set of possible states of signal p, and (iii) a function re, called the reset function, from T to the power set of the node's memory flags.</text></s>
<s sid="530"><CoreSc1 advantage="None" conceptID="Mod35" novelty="None" type="Mod"/><text>Intuitively, the transition function specifies the conditions (guards) under which a node switches states, and the reset function determines which memory flags to reset upon the state change.</text></s>
<s sid="531"><CoreSc1 advantage="None" conceptID="Mod36" novelty="None" type="Mod"/><text>Formally, let P be a predicate on the input ports of node i's state transition module.</text></s>
<s sid="532"><CoreSc1 advantage="None" conceptID="Mod37" novelty="None" type="Mod"/><text>We define P holds at time t by structural induction: If P is equal to p=s, where p is an input port of node i's state transition module and s is one of the states signal p can obtain, then P holds at time t iff p(t)=s.</text></s>
<s sid="533"><CoreSc1 advantage="None" conceptID="Mod38" novelty="None" type="Mod"/><text>Otherwise, if P is of the form ¬P1, P1∧P2, or P1∨P2, we define P holds at time t in the straightforward manner.</text></s>
<s sid="534"><CoreSc1 advantage="None" conceptID="Mod39" novelty="None" type="Mod"/><text>For a given transition delay dTrans&gt;0, the module specification ΦSTM of node i's state transition module is defined as follows.</text></s>
<s sid="535"><CoreSc1 advantage="None" conceptID="Res207" novelty="None" type="Res"/><text>Let Ein be an execution of the state transition module's input ports and Eout an execution of its output ports.</text></s>
<s sid="536"><CoreSc1 advantage="None" conceptID="Res208" novelty="None" type="Res"/><text>Then Eout∈ΦSTM(Ein) iff there is some ε&gt;0 and a signal locked such that the following requirements are met.</text></s>
<s sid="537"><CoreSc1 advantage="None" conceptID="Res209" novelty="None" type="Res"/><text>(The intuition is that locked(t)=0 means that the node is ready to perform the next state transition once a guard becomes true, whereas in case of locked(t)=1 the node is currently executing a previously &quot;locked&quot; transition.)-</text></s>
<s sid="538"><CoreSc1 advantage="None" conceptID="Res210" novelty="None" type="Res"/><text>(Safety) The node (i.e., Si) does not switch states at any time t with locked(t)=0.</text></s>
<s sid="539"><CoreSc1 advantage="None" conceptID="Res211" novelty="None" type="Res"/><text>In every maximal interval [tl,tu)⊆R satisfying that locked≡1 on [tl,tu), it switches states exactly once.</text></s>
-
<s sid="540"><CoreSc1 advantage="None" conceptID="Res212" novelty="None" type="Res"/><text>(Delay) For each interval [tl,tu) as above, tu-tl⩽dTrans-ε.</text></s>
-
<s sid="541"><CoreSc1 advantage="None" conceptID="Res213" novelty="None" type="Res"/><text>(Guard) For each interval [tl,tu) as above, (Si(tl),Si(tu))∈T and tr(Si(t),Si(tu)) is satisfied at some time t∈[tl-ε,tl].</text></s>
-
<s sid="542"><CoreSc1 advantage="None" conceptID="Res214" novelty="None" type="Res"/><text>(Responsiveness) If locked(t)=0 and there is a state s∈S such that (Si(t),s)∈T and tr(Si(t),s) holds at time t, then locked(t+ε)=1.</text></s>
-
<s sid="543"><CoreSc1 advantage="None" conceptID="Res215" novelty="None" type="Res"/><text>(Flags) For an arbitrary interval [tl,tu) as above, suppose that the node switches from state Si(tl) to state Si(tu) at time ts∈[tl,tu).</text></s>
<s sid="544"><CoreSc1 advantage="None" conceptID="Res216" novelty="None" type="Res"/><text>Then for each memory flag specified by re(Si(tl),Si(tu)), the corresponding reset output port of the state transition module is in state 1 at some time in (tl,ts] (and therefore the flag is reset).</text></s>
<s sid="545"><CoreSc1 advantage="None" conceptID="Res217" novelty="None" type="Res"/><text>Outside these time intervals, reset ports are in state 0.</text></s>
<s sid="546"><CoreSc1 advantage="None" conceptID="Res218" novelty="None" type="Res"/><text>A node may run multiple, say k∈N, state machines in parallel (i.e., contain several state machines as submodules).</text></s>
<s sid="547"><CoreSc1 advantage="None" conceptID="Res219" novelty="None" type="Res"/><text>In this case, its state signal is the joint signal Si=(Si1,…,Sik), where Sil, l∈[1..k], denotes the lth state machine of the node.</text></s>
<s sid="548"><CoreSc1 advantage="None" conceptID="Res220" novelty="None" type="Res"/><text>Throughout this article, the different state machines of each node i have disjoint state spaces.</text></s>
<s sid="549"><CoreSc1 advantage="None" conceptID="Res221" novelty="None" type="Res"/><text>For simplicity, we hence may say &quot;node i is in state s at time t&quot; instead of &quot;state machine l of node i is in state s at time t&quot; when referring to Sil(t)=s, etc.</text></s>
<s sid="550"><CoreSc1 advantage="None" conceptID="Res222" novelty="None" type="Res"/><text>To account for the latency of the memory flags, threshold gates and (randomized) watchdog timers, their ports are not directly connected to the state transition module's ports, but via binary communication channels with respective delays.</text></s>
<s sid="551"><CoreSc1 advantage="None" conceptID="Res223" novelty="None" type="Res"/><text>The resulting structure of the compound module node i is depicted in Fig. 5.</text></s>
<s sid="552"><CoreSc1 advantage="None" conceptID="Res224" novelty="None" type="Res"/><text>Note that additional communication channels at the threshold gates' and memory flags' input ports allow to model the fact that memory flags are not necessarily reset at the same time, and signals may arrive shifted in time at the threshold gates.</text></s>
<s sid="553"><CoreSc1 advantage="None" conceptID="Res225" novelty="None" type="Res"/><text>As mentioned earlier, for simplicity we consider the outgoing channels to remote nodes as part of the node.</text></s>
<s sid="554"><CoreSc1 advantage="None" conceptID="Res226" novelty="None" type="Res"/><text>Hence, the output ports of node i comprise the output ports Sj,i, j∈[1..n], of the channels disseminating its state Si.</text></s>
<s sid="555"><CoreSc1 advantage="None" conceptID="Obj66" novelty="None" type="Obj"/><text>In addition, in order to solve the actual problem of clock generation, we include the locally computed discrete clock value Li as an output port.</text></s>
Protocols and problem formulation
<s sid="556"><CoreSc1 advantage="None" conceptID="Res227" novelty="None" type="Res"/><text>We next formalize the concept of a protocol, like the one presented in Section 5, followed by what it means for a protocol to solve self-stabilizing clock synchronization in spite of f faults.</text></s>
<s sid="557"><CoreSc1 advantage="None" conceptID="Res228" novelty="None" type="Res"/><text>Formally, a protocol (for an n-node system) is a compound module consisting of n modules referred to as nodes.</text></s>
<s sid="558"><CoreSc1 advantage="None" conceptID="Res229" novelty="None" type="Res"/><text>The nodes are to be specified as modules themselves, and in our case will follow the layout we just described.</text></s>
<s sid="559"><CoreSc1 advantage="None" conceptID="Res230" novelty="None" type="Res"/><text>It thus remains to state in Section 5 which (randomized) watchdog timers, memory flags and threshold gates our protocol uses as well as the state transition modules' transition and reset functions.</text></s>
<s sid="560"><CoreSc1 advantage="None" conceptID="Res231" novelty="None" type="Res"/><text>A clock synchronization module with n∈N nodes, clock imprecision Σ, amortized frequency bounds A-,A+∈R+, slacks τ-,τ+∈R0+, maximum frequency F+, and at most f∈N faults is a module without input ports and with output ports Li, i∈[1..n].</text></s>
<s sid="561"><CoreSc1 advantage="None" conceptID="Res232" novelty="None" type="Res"/><text>Its module specification is extendable.</text></s>
<s sid="562"><CoreSc1 advantage="None" conceptID="Res233" novelty="None" type="Res"/><text>An execution of the module on R is feasible, iff there exists a subset C of [1..n] of size at least n-f satisfying that•</text></s>
∀t∈R, i,j∈C: |Li(t)-Lj(t)|⩽Σ,
•
<s sid="563"><CoreSc1 advantage="None" conceptID="Res234" novelty="None" type="Res"/><text>∀t, t′∈R, t&lt;t′, i∈C: A-(t′-t)-τ-⩽Li(t′)-Li(t)⩽A+(t′-t)+τ+, and</text></s>
•
∀t, t′∈R,t&lt;t′, i∈C: Li(t′)-Li(t)⩽⌈F+(t′-t)⌉.
<s sid="564"><CoreSc1 advantage="None" conceptID="Res235" novelty="None" type="Res"/><text>We say a protocol Π (for an n-node system) with no input ports and output ports Li, i∈[1..n], solves self-stabilizing clock synchronization with clock imprecision Σ, amortized frequency bounds A-,A+, slacks τ-,τ+∈R0+, maximum frequency F+, at most f faults, and stabilization time T (with probability p) iff it is an f-tolerant, (with probability at least p) T-stabilizing implementation of the clock synchronization module with the respective parameters.</text></s>
<s sid="565"><CoreSc1 advantage="None" conceptID="Res236" novelty="None" type="Res"/><text>A (real-world) implementation will output bounded clocks of size K∈N only.</text></s>
<s sid="566"><CoreSc1 advantage="None" conceptID="Res237" novelty="None" type="Res"/><text>In this case the output ports do not yield Li(t), but only Li(t)modK.</text></s>
<s sid="567"><CoreSc1 advantage="None" conceptID="Con53" novelty="None" type="Con"/><text>Nevertheless, we introduced the signals Li as abstract functions in this setting, as they allow to state the frequency bounds concisely.</text></s>
<s sid="568"><CoreSc1 advantage="None" conceptID="Con54" novelty="None" type="Con"/><text>Note that there is no physical counterpart of these values in the real-world system; to be strictly accurate, it would be necessary to qualify the above definitions further by &quot;with bounded clocks of size K&quot; in order to distinguish this version of the problem from the abstract one with unbounded clocks.</text></s>
Practical implementability issues
<s sid="569"><CoreSc1 advantage="None" conceptID="Con55" novelty="None" type="Con"/><text>Our formal model incorporates a precise semantics of what it means for a module to implement some other module, namely, inclusion of all feasible (sub-)executions.</text></s>
<s sid="570"><CoreSc1 advantage="None" conceptID="Con56" novelty="None" type="Con"/><text>Unfortunately, however, this strong requirement must often be relaxed when it comes to real implementations.</text></s>
<s sid="571"><CoreSc1 advantage="None" conceptID="Con57" novelty="None" type="Con"/><text>This is primarily a consequence of the fact that there is no physical implementation of a circuit that can avoid metastability.</text></s>
<s sid="572"><CoreSc1 advantage="None" conceptID="Con58" novelty="None" type="Con"/><text>Since preventing certain inputs to a module requires output guarantees from others, this is a challenging problem to systems that are self-stabilizing or tolerate persistent faults; combining these properties complicates this issue further.</text></s>
<s sid="573"><CoreSc1 advantage="None" conceptID="Con59" novelty="None" type="Con"/><text>More specifically, in order to faithfully implement their specifications, basic modules must be able to (i) deal with all possible inputs and (ii) recover reliably from transient faults.</text></s>
<s sid="574"><CoreSc1 advantage="None" conceptID="Con60" novelty="None" type="Con"/><text>Unfortunately, (i) is often impossible to achieve with real circuits.</text></s>
<s sid="575"><CoreSc1 advantage="None" conceptID="Con61" novelty="None" type="Con"/><text>For example, simultaneous input changes may drive any implementation of a Muller C-gate into a metastable state, which implies that its output ports do not even carry signals according to our definition, and are hence not feasible.</text></s>
<s sid="576"><CoreSc1 advantage="None" conceptID="Con62" novelty="None" type="Con"/><text>Of course, metastability can also be caused by physical faults affecting the module; such faults can obviously not be analyzed within our model either.</text></s>
<s sid="577"><CoreSc1 advantage="None" conceptID="Con63" novelty="None" type="Con"/><text>This possibility obviously invalidates any guarantees that compound implementations containing this instance may provide, unless they can mask the error due to fault-tolerance.</text></s>
<s sid="578"><CoreSc1 advantage="None" conceptID="Con64" novelty="None" type="Con"/><text>Moreover, real circuits cannot guarantee (ii) under all circumstances either, as it is impossible to always prohibit the propagation of metastable inputs to the outputs and the system may contain feedback-loops.</text></s>
<s sid="579"><CoreSc1 advantage="None" conceptID="Con65" novelty="None" type="Con"/><text>In principle, it would be possible to extend the presented model to cover also generation and propagation of metastability explicitly, by replacing the finite alphabet S and discrete events with a continuous range of signal values (the voltages) [43].</text></s>
<s sid="580"><CoreSc1 advantage="None" conceptID="Con66" novelty="None" type="Con"/><text>Since this would dramatically increase the complexity of any analysis, we choose a different approach that also allows us to handle other implementation intricacies in a pragmatic way.</text></s>
<s sid="581"><CoreSc1 advantage="None" conceptID="Con67" novelty="None" type="Con"/><text>In fact, even in the absence of metastability, it is not necessarily simple and even possible for real implementations to guarantee (ii) under all circumstances.</text></s>
<s sid="582"><CoreSc1 advantage="None" conceptID="Con68" novelty="None" type="Con"/><text>Apart from the fact that transient faults may lead to permanent errors by damaging physical components,1616</text></s>
<s sid="583"><CoreSc1 advantage="None" conceptID="Con69" novelty="None" type="Con"/><text>We remark that, technically speaking, excessively high voltages on the input wires could also be interpreted as an &quot;input violation&quot;, as this violates the definition of our signals.</text></s>
<s sid="584"><CoreSc1 advantage="None" conceptID="Res238" novelty="None" type="Res"/><text>However, it makes sense to interpret such (hopefully exceptional) events as a fault of the module.</text></s>
<s sid="585"><CoreSc1 advantage="None" conceptID="Res239" novelty="None" type="Res"/><text>our model does not prohibit that temporarily infeasible inputs result in permanent infeasibility, i.e., even when inputs become benign again at a later state of the execution of the module in question, there is no suffix of the execution that is feasible.</text></s>
<s sid="586"><CoreSc1 advantage="None" conceptID="Res240" novelty="None" type="Res"/><text>The oscillator implementation given in Example 3.6 demonstrates this issue, and further modules exhibiting persistently faulty behavior after temporary violations of input constraints are easily conceived.</text></s>
<s sid="587"><CoreSc1 advantage="None" conceptID="Con70" novelty="None" type="Con"/><text>As we aim for self-stabilization, it is clear that we cannot allow implementations that suffer from such drawbacks: Neither transient faults nor their consequences, i.e., temporarily arbitrary executions, may result in permanent faults.</text></s>
<s sid="588"><CoreSc1 advantage="None" conceptID="Res241" novelty="None" type="Res"/><text>Clearly, both recovery from transient failures and resilience of a basic module to erroneous inputs, and hence the whole definition of what actually constitutes a transient fault in our model, is implicitly defined by the physical realization of an implementation.</text></s>
<s sid="589"><CoreSc1 advantage="None" conceptID="Res242" novelty="None" type="Res"/><text>These observations have important consequences.</text></s>
<s sid="590"><CoreSc1 advantage="None" conceptID="Res243" novelty="None" type="Res"/><text>On the one hand, careful design of the basic modules is of paramount importance.</text></s>
<s sid="591"><CoreSc1 advantage="None" conceptID="Con71" novelty="None" type="Con"/><text>For instance, in a final product, a watchdog timer must not have its duration stored in a memory register that can be corrupted by a temporary charge injection (e.g. due to a particle hit), a ring oscillator should not be able to run unchecked at e.g.</text></s>
<s sid="592"><CoreSc1 advantage="None" conceptID="Con72" novelty="None" type="Con"/><text>twice its frequency indefinitely (e.g. triggered by a voltage pulse), and one has to make sure that stateful components like memory flags or state transition modules eventually &quot;forget&quot; about potentially erroneous inputs in the past, and eventually behave according to their specification again.</text></s>
<s sid="593"><CoreSc1 advantage="None" conceptID="Con73" novelty="None" type="Con"/><text>As discussed above, however, this cannot usually be perfect: There will always be (rare) scenarios, where an implemented circuit will not work like an ideal one, i.e., violate its specification.</text></s>
<s sid="594"><CoreSc1 advantage="None" conceptID="Con74" novelty="None" type="Con"/><text>We incorporate this in our model, in a pragmatic well-known from critical system design, by means of the notion of imperfect implementation coverage.</text></s>
<s sid="595"><CoreSc1 advantage="None" conceptID="Con75" novelty="None" type="Con"/><text>For a given module implementation, the coverage implicitly or explicitly determines the fraction of all possibly executions in which the implementation works as specified.</text></s>
<s sid="596"><CoreSc1 advantage="None" conceptID="Con76" novelty="None" type="Con"/><text>Since exceptional scenarios like metastability are usually extremely rare, we do not bother with defining the notion of coverage formally here: The coverage should be very close to 100% anyway.</text></s>
<s sid="597"><CoreSc1 advantage="None" conceptID="Con77" novelty="None" type="Con"/><text>In Section 6, we will argue that each of our basic modules will work as specified, except for very rare situations that may trigger metastability due to a violation of input timing constraints.</text></s>
<s sid="598"><CoreSc1 advantage="None" conceptID="Res244" novelty="None" type="Res"/><text>Thanks to this approach, algorithms and proofs can rely on sufficiently simple specifications of basic modules, which usually also admit robust and efficient implementations in practice.</text></s>
<s sid="599"><CoreSc1 advantage="None" conceptID="Res245" novelty="None" type="Res"/><text>Any unhandled scenarios are relegated to imperfect implementation coverage.</text></s>
<s sid="600"><CoreSc1 advantage="None" conceptID="Res246" novelty="None" type="Res"/><text>This feature is essential for devising proofs of reasonable complexity that show self-stabilization of all compound modules, implying that the system indeed will recover once transient faults of (basic) modules cease.</text></s>
<s sid="601"><CoreSc1 advantage="None" conceptID="Obj67" novelty="None" type="Obj"/><text>Due to the hierarchical composition of modules, compound modules fully derive their behavior from their submodules and can therefore be analyzed based on the properties of their submodules, while we may switch at will between viewing a module as given (i.e., basic), analyzing it in more detail as a compound implementation, or (for low-level modules) analyzing it in an even more detailed model.</text></s>
<s sid="602"><CoreSc1 advantage="None" conceptID="Res247" novelty="None" type="Res"/><text>This way, our approach also inherently supports tight interaction between algorithmic design and design of the basic building blocks used in the algorithms.</text></s>
<s sid="603"><CoreSc1 advantage="None" conceptID="Res248" novelty="None" type="Res"/><text>The FATAL+ clock synchronization protocol</text></s>
<s sid="604"><CoreSc1 advantage="None" conceptID="Obj68" novelty="None" type="Obj"/><text>In this section, we recast the self-stabilizing clock synchronization algorithm introduced in [13] in the modeling framework of the previous section and summarize its most important properties.</text></s>
<s sid="605"><CoreSc1 advantage="None" conceptID="Goa22" novelty="None" type="Goa"/><text>Since the main focus of our paper is on the implementation of our algorithm in this model, there is no need to provide a detailed description of the stabilization mechanism, let alone formal proofs of the stated claims; the analysis of the correctness and performance of the algorithm in [13] is based on a simpler abstract system model, assuming a globally valid end-to-end delay bound d covering any (local and remote) communication and processing action, which is fully compatible with our modeling framework.</text></s>
<s sid="606"><CoreSc1 advantage="None" conceptID="Goa23" novelty="None" type="Goa"/><text>More specifically, all that is needed in order to reuse the results of the analysis in [13] is to compute the maximum end-to-end delay occurring in the implementation of our algorithm in the modeling framework introduced in Section 2.2.</text></s>
<s sid="607"><CoreSc1 advantage="None" conceptID="Res249" novelty="None" type="Res"/><text>Recall from Section 2.2 that our top-level clock synchronization module is implemented as a compound module consisting of n nodes and their connecting top-level channels (with maximum delay dChan).</text></s>
<s sid="608"><CoreSc1 advantage="None" conceptID="Res250" novelty="None" type="Res"/><text>Every node, in turn, is a compound module made up of a state transition module, watchdog timers, memory flags, and threshold modules interconnected by channels (modeling various delays) as shown in Fig. 5.</text></s>
<s sid="609"><CoreSc1 advantage="None" conceptID="Res251" novelty="None" type="Res"/><text>Finally, a state transition module represents several communicating concurrent asynchronous state machines (with maximum transition time dTrans).</text></s>
<s sid="610"><CoreSc1 advantage="None" conceptID="Res252" novelty="None" type="Res"/><text>It ensures that state transitions of every constituent state machine occur in an orderly fashion, i.e., that every transition happens exactly once and, if need be, memory flags are consistently reset.</text></s>
<s sid="611"><CoreSc1 advantage="None" conceptID="Res253" novelty="None" type="Res"/><text>The state of each state machine is encoded in a few bits and conveyed via the top-level channels to all other modules in the system that need to receive it on some input port.</text></s>
<s sid="612"><CoreSc1 advantage="None" conceptID="Res254" novelty="None" type="Res"/><text>Given this simple internal structure, computing the resulting end-to-end delay bound d (or, for the quick cycle, dmin+ and dmax+, see below) from the constituent delay bounds is straightforward, see Section 6 for details.</text></s>
State machine representation
<s sid="613"><CoreSc1 advantage="None" conceptID="Res255" novelty="None" type="Res"/><text>Obviously, the entire logic of our algorithm is encoded in the state machines of a node.</text></s>
<s sid="614"><CoreSc1 advantage="None" conceptID="Res256" novelty="None" type="Res"/><text>In [13], we use a graphical representation that also reveals the layered structure imposed by their communication.</text></s>
<s sid="615"><CoreSc1 advantage="None" conceptID="Res257" novelty="None" type="Res"/><text>We already employed this description in Fig. 1.</text></s>
<s sid="616"><CoreSc1 advantage="None" conceptID="Con78" novelty="None" type="Con"/><text>With the definitions from the previous section at hand, we can now give our graphical representation a precise formal meaning that will allow us to translate the results from [13] to our modeling framework.</text></s>
<s sid="617"><CoreSc1 advantage="None" conceptID="Con79" novelty="None" type="Con"/><text>Our graphical representation defines the set of possible states S of a state machine (in Fig. 1 ready, propose, and increase) and, by means of the arrows between the states, the set of possible state transitions T⊆S2 (here ready to propose, propose to increase, and increase to ready).</text></s>
<s sid="618"><CoreSc1 advantage="None" conceptID="Con80" novelty="None" type="Con"/><text>If, for a state transition from s to s′, re(s,s′)≠∅, i.e., there are memory flags that need to be reset, re(s,s′) is given in a rectangular box on the arrow.</text></s>
<s sid="619"><CoreSc1 advantage="None" conceptID="Con81" novelty="None" type="Con"/><text>Since for each node i and state s we will always reset all memory flags Memi,j,s for j∈{1,…,n} together, we simply write s1,…,sk in such a box to represent the fact that all flags Memi,j,s, j∈{1,…,n}, s∈{s1,…,sk}, are to be reset.</text></s>
<s sid="620"><CoreSc1 advantage="None" conceptID="Con82" novelty="None" type="Con"/><text>Note that some of these states may be from a different state machine, i.e., the states s1,…,sk need not all be from S.</text></s>
<s sid="621"><CoreSc1 advantage="None" conceptID="Con83" novelty="None" type="Con"/><text>Completing the description, for each (s,s′)∈T, tr(s,s′) is given by the label next to the respective arrow.</text></s>
<s sid="622"><CoreSc1 advantage="None" conceptID="Con84" novelty="None" type="Con"/><text>Again, we make use of a condensed notation.</text></s>
<s sid="623"><CoreSc1 advantage="None" conceptID="Con85" novelty="None" type="Con"/><text>Assume that the state machine in question is part of node i.</text></s>
<s sid="624"><CoreSc1 advantage="None" conceptID="Con86" novelty="None" type="Con"/><text>We will employ threshold conditions like &quot;⩾f+1 s1&quot;, whereby we refer to at least f+1 of i's memory flags Memi,j,s1 being in state 1, or &quot;⩾n-f s1 or s2&quot;, which is true if ∑j∈Nmax{Memi,j,s1,Memi,j,s2}⩾n-f, i.e., for at least n-f nodes j flag Memi,j,s1 or flag Memi,j,s2 is in state 1.</text></s>
<s sid="625"><CoreSc1 advantage="None" conceptID="Con87" novelty="None" type="Con"/><text>An example for such a rule is the transition from propose to increase in Fig. 1.</text></s>
<s sid="626"><CoreSc1 advantage="None" conceptID="Exp3" novelty="None" type="Exp"/><text>Such conditions will be translated to a binary signal by feeding the memory flags' signals (or, in the latter case, the output of n OR-gates with inputs Memi,j,s1 and Memi,j,s2) into a threshold gate (of threshold f+1 or n-f, respectively).</text></s>
<s sid="627"><CoreSc1 advantage="None" conceptID="Exp4" novelty="None" type="Exp"/><text>Further abbreviations we use for timeouts.</text></s>
<s sid="628"><CoreSc1 advantage="None" conceptID="Exp5" novelty="None" type="Exp"/><text>Recall that for a timeout (T,s,C), we omit the clock C from the notation, i.e., write (T,s) instead of (T,s,C).</text></s>
<s sid="629"><CoreSc1 advantage="None" conceptID="Exp6" novelty="None" type="Exp"/><text>Timeout (T,s) switches to 1 after T local time units (i.e., between T/ϑ and T+dTrans reference time) has passed since the last switch to state s was triggered.</text></s>
<s sid="630"><CoreSc1 advantage="None" conceptID="Mod40" novelty="None" type="Mod"/><text>In case it is part of a transition rule, we write (T,s) for the condition TimeT,s,C=1, and if the transition goes from the state s to which the timeout corresponds to some state s′, we simply write T.</text></s>
<s sid="631"><CoreSc1 advantage="None" conceptID="Mod41" novelty="None" type="Mod"/><text>For instance, the condition &quot;3ϑd local time has passed&quot; in Fig. 1 is concisely stated as &quot;3ϑd&quot;.</text></s>
<s sid="632"><CoreSc1 advantage="None" conceptID="Mod42" novelty="None" type="Mod"/><text>Finally, as for memory flag resets, transition rules may also refer to a state s of another state machine.</text></s>
<s sid="633"><CoreSc1 advantage="None" conceptID="Mod43" novelty="None" type="Mod"/><text>In the special case that a predicate solely depends on the current state of another of the node's state machines, we write &quot;in s&quot; or &quot;not in s&quot; to indicate the predicates p=s and ¬(p=s), respectively, where p is the input port connected to the channel communicating the other state machine's state to the state transition module.</text></s>
<s sid="634"><CoreSc1 advantage="None" conceptID="Mod44" novelty="None" type="Mod"/><text>Finally, the above rules can be composed by logical AND or OR, which we display by connecting expressions with and or or, respectively.</text></s>
<s sid="635"><CoreSc1 advantage="None" conceptID="Obs27" novelty="None" type="Obs"/><text>In Fig. 1, such a composition occurs in tr(ready,propose).</text></s>
Overview of the algorithm
<s sid="636"><CoreSc1 advantage="None" conceptID="Res258" novelty="None" type="Res"/><text>Each node is a collection of several state machines that are organized in a layered structure.</text></s>
<s sid="637"><CoreSc1 advantage="None" conceptID="Res259" novelty="None" type="Res"/><text>On each layer, the state machines of the (at least n-f) non-faulty nodes cooperate in order to establish certain synchronization properties of their output signals.</text></s>
<s sid="638"><CoreSc1 advantage="None" conceptID="Res260" novelty="None" type="Res"/><text>The higher is a state machine in the hierarchy, the stronger are these guarantees; the lower it is, the weaker are the synchronization properties its input signals need to satisfy for stabilization.</text></s>
<s sid="639"><CoreSc1 advantage="None" conceptID="Res261" novelty="None" type="Res"/><text>The lowest-layer state machine utilizes randomization to recover from any configuration (provided its basic modules are correct (again), i.e., guarantee feasible executions).</text></s>
<s sid="640"><CoreSc1 advantage="None" conceptID="Res262" novelty="None" type="Res"/><text>Each other layer utilizes auxiliary information from the layer below to stabilize.</text></s>
<s sid="641"><CoreSc1 advantage="None" conceptID="Res263" novelty="None" type="Res"/><text>Finally, the top level state machine outputs the logical clocks Li.</text></s>
<s sid="642"><CoreSc1 advantage="None" conceptID="Res264" novelty="None" type="Res"/><text>More specifically, we have the following state machines.•</text></s>
<s sid="643"><CoreSc1 advantage="None" conceptID="Res265" novelty="None" type="Res"/><text>At the top level, we have the quick cycle state machine (Fig. 6) that outputs Li.</text></s>
<s sid="644"><CoreSc1 advantage="None" conceptID="Res266" novelty="None" type="Res"/><text>The quick cycle is very similar to the algorithm given in Fig. 1, except that it is coupled to the state machine beneath it in order to ensure eventual stabilization.</text></s>
<s sid="645"><CoreSc1 advantage="None" conceptID="Res267" novelty="None" type="Res"/><text>Once the system is stabilized, it consistently and deterministically increases Li at a high frequency while guaranteeing small clock imprecision.</text></s>
•
<s sid="646"><CoreSc1 advantage="None" conceptID="Res268" novelty="None" type="Res"/><text>The main state machine (Fig. 8) is the centerpiece of the stabilization mechanism.</text></s>
<s sid="647"><CoreSc1 advantage="None" conceptID="Res269" novelty="None" type="Res"/><text>Once stabilized, it generates slow, roughly synchronized &quot;pulses&quot; within certain frequency bounds.</text></s>
<s sid="648"><CoreSc1 advantage="None" conceptID="Res270" novelty="None" type="Res"/><text>These pulses can be seen as a &quot;heartbeat&quot; of the system; at each pulse, the quick cycle's clocks are reset to 0 and the quick cycle's state machines are forced into state accept+ (corresponding to the increase state in Fig. 1).</text></s>
<s sid="649"><CoreSc1 advantage="None" conceptID="Con88" novelty="None" type="Con"/><text>This enforces exactly the initial synchrony that we explained to be necessary for the correct operation of the algorithm from Fig. 1.</text></s>
<s sid="650"><CoreSc1 advantage="None" conceptID="Con89" novelty="None" type="Con"/><text>By itself, however, the main state machine is not capable of recovering from every possible initial configuration of the non-faulty nodes.</text></s>
<s sid="651"><CoreSc1 advantage="None" conceptID="Con90" novelty="None" type="Con"/><text>In certain cases, it requires some coarse synchrony to be established first in order to stabilize, which is probabilistically provided by the underlying layer.</text></s>
<s sid="652"><CoreSc1 advantage="None" conceptID="Con91" novelty="None" type="Con"/><text>We remark that, once stabilized, the main state machine operates fully independently of this layer (and thus deterministically).</text></s>
•
<s sid="653"><CoreSc1 advantage="None" conceptID="Con92" novelty="None" type="Con"/><text>The auxiliary information potentially required for stabilization by the main state machine is provided by a simple intermediate layer we refer to as extension of the main state machine (Fig. 9).</text></s>
<s sid="654"><CoreSc1 advantage="None" conceptID="Con93" novelty="None" type="Con"/><text>Essentially, it is supposed to be consistently reset by the underlying layer and then communicate information vital for stabilization to the main state machine.</text></s>
<s sid="655"><CoreSc1 advantage="None" conceptID="Con94" novelty="None" type="Con"/><text>This information depends both on the time of reset and the current states of the n main state machines, which it therefore monitors.</text></s>
•
<s sid="656"><CoreSc1 advantage="None" conceptID="Res271" novelty="None" type="Res"/><text>Finally, the resynchronization routine (Fig. 10) utilizes randomized timeouts to consistently generate events at all non-faulty nodes that could be understood as &quot;randomized pulses&quot;.</text></s>
<s sid="657"><CoreSc1 advantage="None" conceptID="Res272" novelty="None" type="Res"/><text>Such a pulse is correct for our purposes if all non-faulty nodes generate a respective event in coarse synchrony and no non-faulty node generates another such event within a time window of a certain length.</text></s>
<s sid="658"><CoreSc1 advantage="None" conceptID="Res273" novelty="None" type="Res"/><text>The crux of the matter is that a single such pulse suffices to achieve stabilization deterministically.</text></s>
<s sid="659"><CoreSc1 advantage="None" conceptID="Res274" novelty="None" type="Res"/><text>Relying on (pseudo-)randomness on this layer greatly simplifies the task of overcoming the interference by faulty nodes at low costs in both time and communication.</text></s>
<s sid="660"><CoreSc1 advantage="None" conceptID="Con95" novelty="None" type="Con"/><text>We note that the main state machine masks this randomness once stabilization is achieved, facilitating deterministic behavior of the higher levels and, ultimately, the nodes' clocks Li.</text></s>
<s sid="661"><CoreSc1 advantage="None" conceptID="Obj69" novelty="None" type="Obj"/><text>We will now present the individual state machines.</text></s>
<s sid="662"><CoreSc1 advantage="None" conceptID="Obj70" novelty="None" type="Obj"/><text>We refrain from a discussion of choosing appropriate durations for the timers, confining ourselves to stating a feasible family of choices later on.</text></s>
The quick cycle
<s sid="663"><CoreSc1 advantage="None" conceptID="Obs28" novelty="None" type="Obs"/><text>The quick cycle state machine is depicted in Fig. 6.</text></s>
<s sid="664"><CoreSc1 advantage="None" conceptID="Res275" novelty="None" type="Res"/><text>It introduces an additional notation: As the states ready+ and accept+ are not distinguished in any of the transition conditions in the other state machines, the same state none+ can be communicated here.</text></s>
<s sid="665"><CoreSc1 advantage="None" conceptID="Res276" novelty="None" type="Res"/><text>This allows for a very efficient single-bit representation of the communicated states.</text></s>
<s sid="666"><CoreSc1 advantage="None" conceptID="Res277" novelty="None" type="Res"/><text>In Fig. 6, this is expressed by dividing the circles representing states, putting the state names in the upper part and the communicated states in the lower part.</text></s>
<s sid="667"><CoreSc1 advantage="None" conceptID="Res278" novelty="None" type="Res"/><text>Apart from saving a wire, this permits to use trivial encoding and decoding of the signal, a simplification of the logic that minimizes delays and therefore maximizes the clock frequency that can be achieved.</text></s>
<s sid="668"><CoreSc1 advantage="None" conceptID="Res279" novelty="None" type="Res"/><text>Essentially, the quick cycle works as the algorithm given in Fig. 1, where the logical clock is increased whenever the machine switches to state accept+.</text></s>
<s sid="669"><CoreSc1 advantage="None" conceptID="Obs29" novelty="None" type="Obs"/><text>However, the quick cycle differs from the algorithm in Fig. 1 in that there is an interface to the main state machine given in Fig. 8.</text></s>
<s sid="670"><CoreSc1 advantage="None" conceptID="Res280" novelty="None" type="Res"/><text>These state machines communicate by means of two signals only, one for each direction of the communication: (i) The quick cycle state machine of node i generates the nexti signal by which it exerts some limited influence on the time between two successive pulses generated by the main state machine, and (ii) it observes the (T2+,accept) timer.</text></s>
<s sid="671"><CoreSc1 advantage="None" conceptID="Res281" novelty="None" type="Res"/><text>This timer is coupled to the state accept of Fig. 8, in which the pulse synchronization algorithm generates a new pulse.</text></s>
<s sid="672"><CoreSc1 advantage="None" conceptID="Res282" novelty="None" type="Res"/><text>The signal's purpose is to enforce a consistent reset of the quick cycle state machine (once the main state machine has stabilized).</text></s>
<s sid="673"><CoreSc1 advantage="None" conceptID="Res283" novelty="None" type="Res"/><text>The feedback mechanism (i) makes sure that, during regular operation, the reset of the quick cycle does not have any effect on the clocks.</text></s>
<s sid="674"><CoreSc1 advantage="None" conceptID="Res284" novelty="None" type="Res"/><text>This is guaranteed by triggering pulses (by means of the non-faulty nodes briefly changing the nexti signal to 1 and back to 0 again) exactly at the wrap-around of the logical clock Li, i.e., at the time when Li is &quot;increased&quot; from the maximal clock value K-1=2b-1 (of a b-bit clock) to 0=KmodK.</text></s>
<s sid="675"><CoreSc1 advantage="None" conceptID="Res285" novelty="None" type="Res"/><text>Similar to Fig. 1, the transition conditions of the quick cycle ensure that the logical clocks never have a clock imprecision of more than one.</text></s>
<s sid="676"><CoreSc1 advantage="None" conceptID="Res286" novelty="None" type="Res"/><text>To increase the frequency further, each node could increase the number of clock &quot;ticks&quot; generated in each iteration of the quick cycle by means of a high-frequency local clock (essentially, a watchdog timer together with a counter), at the expense of larger clock imprecision (see [13]).</text></s>
Main state machine
<s sid="677"><CoreSc1 advantage="None" conceptID="Res287" novelty="None" type="Res"/><text>Before we show the complete main state machine, consider its basic cycle depicted in Fig. 7.</text></s>
<s sid="678"><CoreSc1 advantage="None" conceptID="Res288" novelty="None" type="Res"/><text>Once the main state machines have stabilized, all non-faulty nodes will undergo the states of the basic cycle in rough synchrony.</text></s>
<s sid="679"><CoreSc1 advantage="None" conceptID="Con96" novelty="None" type="Con"/><text>The states sleep, sleep→waking, and waking serve diagnostic purposes related to the stabilization process.</text></s>
<s sid="680"><CoreSc1 advantage="None" conceptID="Con97" novelty="None" type="Con"/><text>The duration T2 of the timer (T2,accept) triggering the transition from waking to ready is so large that the node will always be in state waking long before the timer expires.</text></s>
<s sid="681"><CoreSc1 advantage="None" conceptID="Res289" novelty="None" type="Res"/><text>Thus, we can see that the basic cycle has an underlying structure that is very similar to the quick cycle.</text></s>
<s sid="682"><CoreSc1 advantage="None" conceptID="Res290" novelty="None" type="Res"/><text>Due to the more complicated logic and conditions on the duration of timers required for the stabilization mechanism, it is however executed at a frequency that is by orders of magnitude smaller than that of the quick cycle.</text></s>
<s sid="683"><CoreSc1 advantage="None" conceptID="Res291" novelty="None" type="Res"/><text>The difference in the rules for switching to propose and accept, respectively, are also mostly related to the stabilization process.</text></s>
<s sid="684"><CoreSc1 advantage="None" conceptID="Res292" novelty="None" type="Res"/><text>An exception is the condition &quot;T3 and nexti=1&quot; that can trigger a transition from ready to propose.</text></s>
<s sid="685"><CoreSc1 advantage="None" conceptID="Res293" novelty="None" type="Res"/><text>Choosing T3 smaller than T4 and taking the signal nexti into account, we permit the quick cycle to adjust the time between pulses (i.e., switches to accept) triggered by the main state machine: Once both state machines are roughly synchronized among all non-faulty nodes, the main state machines will always be in state ready before the logical clocks Li maintained by the quick cycle reach the wrap-around (i.e., become 0 modulo K) and trigger the nexti signals.</text></s>
<s sid="686"><CoreSc1 advantage="None" conceptID="Res294" novelty="None" type="Res"/><text>Moreover, this happens at all nodes at close times and before any timer (T4,ready) expires at one of the non-faulty nodes.</text></s>
<s sid="687"><CoreSc1 advantage="None" conceptID="Res295" novelty="None" type="Res"/><text>Hence, by a reasoning similar as for Fig. 1, all non-faulty nodes will switch to propose and subsequently accept in a well-synchronized fashion, caused by the wrap-around of the logical clocks.</text></s>
<s sid="688"><CoreSc1 advantage="None" conceptID="Res296" novelty="None" type="Res"/><text>An important observation that is proved in [13] is that, once the main state machines stabilized, the nodes execute the basic cycle deterministically and any state transition is certainly completed before one of the conditions for leaving the basic cycle can be satisfied.</text></s>
<s sid="689"><CoreSc1 advantage="None" conceptID="Res297" novelty="None" type="Res"/><text>Apart from small additional slacks in the timer durations, this is a consequence of the fact that none of the transition conditions of the basic cycle refer to the probabilistic lower layers of the protocol; all evaluated timers and memory flags solely involve states of the basic cycle only, and the nexti signal is provided by the quick cycle.</text></s>
<s sid="690"><CoreSc1 advantage="None" conceptID="Res298" novelty="None" type="Res"/><text>As we will discuss in Section 6, this property prevents non-faulty nodes from introducing metastability once stabilization is achieved.</text></s>
<s sid="691"><CoreSc1 advantage="None" conceptID="Res299" novelty="None" type="Res"/><text>We now turn our attention to the full main state machine that is shown in Fig. 8.</text></s>
<s sid="692"><CoreSc1 advantage="None" conceptID="Res300" novelty="None" type="Res"/><text>Compared to the basic cycle, we have two additional states, resync and join, that can be occupied by non-faulty nodes during the stabilization process only, and an additional reset of memory flags on the transition from sleep→waking to waking.</text></s>
<s sid="693"><CoreSc1 advantage="None" conceptID="Res301" novelty="None" type="Res"/><text>The various conditions for leaving the basic cycle and switching to recover are consistency checks.</text></s>
<s sid="694"><CoreSc1 advantage="None" conceptID="Con98" novelty="None" type="Con"/><text>A node will only leave the basic cycle if it is certain that the system is not operating as desired.</text></s>
<s sid="695"><CoreSc1 advantage="None" conceptID="Con99" novelty="None" type="Con"/><text>As the high-level operation of the algorithm is not the subject of this article, we limit our exposition to briefly discussing the two possible ways to re-enter the basic cycle, corresponding to two different stabilization mechanisms.</text></s>
<s sid="696"><CoreSc1 advantage="None" conceptID="Con100" novelty="None" type="Con"/><text>The first stabilization mechanism is very simple, and it is much faster than the second one.</text></s>
<s sid="697"><CoreSc1 advantage="None" conceptID="Res302" novelty="None" type="Res"/><text>Assuming that at least n-f non-faulty nodes are executing the basic cycle (i.e., the main state machines have already stabilized if we consider the remaining nodes faulty), a recovering node just needs to &quot;jump on the train&quot; and start executing the basic cycle as well.</text></s>
<s sid="698"><CoreSc1 advantage="None" conceptID="Res303" novelty="None" type="Res"/><text>This is realized by the condition for switching from recover to accept.</text></s>
<s sid="699"><CoreSc1 advantage="None" conceptID="Res304" novelty="None" type="Res"/><text>It is not hard to see that due to this condition, the node will switch to accept in sufficient synchrony with the majority of n-f synchronized, non-faulty nodes within at most two consecutive pulses and subsequently follow the basic cycle as well.</text></s>
<s sid="700"><CoreSc1 advantage="None" conceptID="Res305" novelty="None" type="Res"/><text>Note that this condition makes direct use of the state signals instead of using memory flags.</text></s>
<s sid="701"><CoreSc1 advantage="None" conceptID="Res306" novelty="None" type="Res"/><text>This potentially induces metastability at the joining node, but we will explain in Section 6 why the risk is low.1717</text></s>
<s sid="702"><CoreSc1 advantage="None" conceptID="Con101" novelty="None" type="Con"/><text>Recall that during stabilization we cannot exclude metastability with certainty even in the absence of any further faults.</text></s>
<s sid="703"><CoreSc1 advantage="None" conceptID="Con102" novelty="None" type="Con"/><text>On the plus side, this simplifies the algorithm, as the node does not need to implement frequent resets of the respective memory flags to ensure consistent observation of others' states; the sending nodes will just do this implicitly by leaving state accept.</text></s>
<s sid="704"><CoreSc1 advantage="None" conceptID="Con103" novelty="None" type="Con"/><text>Clearly, the first stabilization mechanism will fail in certain settings.</text></s>
<s sid="705"><CoreSc1 advantage="None" conceptID="Con104" novelty="None" type="Con"/><text>Most obviously, it cannot &quot;restart&quot; the system if all nodes are in state recover.</text></s>
<s sid="706"><CoreSc1 advantage="None" conceptID="Con105" novelty="None" type="Con"/><text>Hence it may not surprise that the second stabilization mechanism, which deals with such cases, is much more involved.</text></s>
<s sid="707"><CoreSc1 advantage="None" conceptID="Con106" novelty="None" type="Con"/><text>Careful attention has to be paid to avoiding the potential for system-wide dead- or live-locks.</text></s>
<s sid="708"><CoreSc1 advantage="None" conceptID="Con107" novelty="None" type="Con"/><text>In view of our design goals, state-of-the-art deterministic solutions for this problem are not sufficiently efficient.</text></s>
<s sid="709"><CoreSc1 advantage="None" conceptID="Con108" novelty="None" type="Con"/><text>Hence, the main state machine relies on a probabilistic lower layer that provides certain guarantees with a very large probability.</text></s>
<s sid="710"><CoreSc1 advantage="None" conceptID="Con109" novelty="None" type="Con"/><text>Extension of the main state machine</text></s>
<s sid="711"><CoreSc1 advantage="None" conceptID="Con110" novelty="None" type="Con"/><text>The extension of the main state machine, given in Fig. 9, can be seen as a simple control structure for the phases of stabilization.</text></s>
<s sid="712"><CoreSc1 advantage="None" conceptID="Con111" novelty="None" type="Con"/><text>The intricacy lies in designing the interface such that this control does not interfere with the basic cycle if the system is stable.</text></s>
<s sid="713"><CoreSc1 advantage="None" conceptID="Con112" novelty="None" type="Con"/><text>Consequently, the influence of the extension of the main state machine is limited to (i) resetting the join and sleep→waking flags upon &quot;initializing&quot; the stabilization process (by switching from dormant to passive) and (ii) providing the signals of the timers (T6,active) and (T7,passive) the main state machine utilizes in the transition rule from recover to join.</text></s>
<s sid="714"><CoreSc1 advantage="None" conceptID="Con113" novelty="None" type="Con"/><text>Roughly speaking, the main state machines will stabilize deterministically under the condition that their extensions switch at all non-faulty nodes from dormant to passive in rough synchrony and then do not switch back to dormant too quickly, i.e., before the second stabilization mechanism of the main state machine completes its work.</text></s>
<s sid="715"><CoreSc1 advantage="None" conceptID="Obj71" novelty="None" type="Obj"/><text>Putting it simply, we require a single, coarsely synchronized pulse, whose generation is the purpose of the lowest layer we present now.</text></s>
Resynchronization state machine
<s sid="716"><CoreSc1 advantage="None" conceptID="Obs30" novelty="None" type="Obs"/><text>The resynchronization state machine is specified in Fig. 10.</text></s>
<s sid="717"><CoreSc1 advantage="None" conceptID="Res307" novelty="None" type="Res"/><text>Strictly speaking, it actually consists of two separate state machines, one of which is however extremely simple.</text></s>
<s sid="718"><CoreSc1 advantage="None" conceptID="Res308" novelty="None" type="Res"/><text>Every now and then, each node will briefly switch to the init state, seeking to induce the generation of a &quot;pulse&quot; (where the pulse here is locally triggered by switching to resync) that causes a consistent switch of all non-faulty nodes from dormant to passive.</text></s>
<s sid="719"><CoreSc1 advantage="None" conceptID="Con114" novelty="None" type="Con"/><text>Leaving resync will force the extension state machine back into state dormant.</text></s>
<s sid="720"><CoreSc1 advantage="None" conceptID="Con115" novelty="None" type="Con"/><text>This is the only interaction with the above layer, which is sufficient if a pulse is successfully generated once.</text></s>
<s sid="721"><CoreSc1 advantage="None" conceptID="Met52" novelty="None" type="Met"/><text>The generation of a pulse is achieved by all non-faulty nodes following the advice of a single node switching to init, thus establishing the common time base required for a synchronized pulse.</text></s>
<s sid="722"><CoreSc1 advantage="None" conceptID="Res309" novelty="None" type="Res"/><text>Two obstacles are to be overcome: possibly some of the non-faulty nodes already believe that the system is in the middle of an attempt to stabilize (i.e., they are already in state resync and thus not ready to follow the advice given by another node) and possibly inconsistent information by nodes that remain faulty (causing only some of the non-faulty nodes to switch to resync).</text></s>
<s sid="723"><CoreSc1 advantage="None" conceptID="Con116" novelty="None" type="Con"/><text>In contrast to the higher levels, however, we are satisfied if only occasionally a successful pulse is generated.</text></s>
<s sid="724"><CoreSc1 advantage="None" conceptID="Con117" novelty="None" type="Con"/><text>Hence, the above issues can be overcome by randomization.</text></s>
<s sid="725"><CoreSc1 advantage="None" conceptID="Res310" novelty="None" type="Res"/><text>The source of randomness here is the randomized timer (R3,wait).</text></s>
<s sid="726"><CoreSc1 advantage="None" conceptID="Res311" novelty="None" type="Res"/><text>The distribution R3 and the logic of the second, more complicated state machine including the state resync are designed such that there is a large probability that within time O(n) all non-faulty nodes will consistently switch to state resync.</text></s>
<s sid="727"><CoreSc1 advantage="None" conceptID="Res312" novelty="None" type="Res"/><text>This O(n) is essentially the factor by which the second stabilization mechanism of the main state machine is slower than the first one.</text></s>
Timer durations
<s sid="728"><CoreSc1 advantage="None" conceptID="Res313" novelty="None" type="Res"/><text>Clearly, in order for the protocol to operate as desired, the timer durations need to satisfy certain constraints.</text></s>
<s sid="729"><CoreSc1 advantage="None" conceptID="Res314" novelty="None" type="Res"/><text>We state a feasible family of durations here; the minimal constraints that are required by the proofs are given in [13].</text></s>
<s sid="730"><CoreSc1 advantage="None" conceptID="Res315" novelty="None" type="Res"/><text>Recall that ϑ&gt;1 and that d bounds the maximal end-to-end delay incurred between the time when a state transition condition is met and the time when the respective signal transition is observed at all receivers.</text></s>
<s sid="731"><CoreSc1 advantage="None" conceptID="Res316" novelty="None" type="Res"/><text>As the logic of the quick cycle is much simpler than that of the other state machines, it typically permits much tighter upper and lower bounds on this end-to-end delay.</text></s>
<s sid="732"><CoreSc1 advantage="None" conceptID="Res317" novelty="None" type="Res"/><text>As in [13], these bounds are denoted by dmin+ and dmax+⩽d.</text></s>
<s sid="733"><CoreSc1 advantage="None" conceptID="Res318" novelty="None" type="Res"/><text>In Section 6, we will discuss how d, dmin+, and dmax+ can be computed out of the constituent delays incurred in our basic modules.</text></s>
<s sid="734"><CoreSc1 advantage="None" conceptID="Res319" novelty="None" type="Res"/><text>Definingλ:=(25ϑ-9)/(25ϑ)∈(4/5,1)andα:=(T2+T4)/(ϑ(T2+T3+4d)), for any ϑ&gt;1, α⩾1, the following family of timeout durations meets the requirements stated in [13] (see the reference for a proof):T1+:=6ϑ2d+6ϑ2dmax+-ϑdmin+T2+:=3ϑd+3ϑdmax+T3+:=6ϑ3d+6ϑ3dmax+-ϑ2dmin+T1:=4ϑdT2:=46ϑ3d/(1-λ)T3:=(ϑ2-1)46ϑ3d/(1-λ)+31ϑ3dT4:=46ϑ3(αϑ3-1)d/(1-λ)+35αϑ4dT5:=46ϑ4(αϑ3-1)d/(1-λ)+39αϑ5dT6:=46ϑ4d/(1-λ)T7:=92αϑ8d/(1-λ)+78αϑ5d and further,R1:=46ϑ6(3αϑ3-1)d/(1-λ)+109αϑ6dR2:=(92ϑ7(3αϑ3-1)/(1-λ)2+(218αϑ7+108ϑ3)/(1-λ))(n-f)dR3:=uniformly distributed random variable on[3ϑd+(92ϑ8(3αϑ3-1)/(1-λ)2+(218αϑ8+108ϑ4)/(1-λ))(n-f)d,3ϑd+(8(1-λ)+ϑ)(92ϑ7(3αϑ3-1)/(1-λ)2+(218αϑ7+108ϑ3)/(1-λ))(n-f)d].</text></s>
<s sid="735"><CoreSc1 advantage="None" conceptID="Res320" novelty="None" type="Res"/><text>Finally, the maximal logical clock value K-1 is not arbitary, as we require(1)K∈[(46ϑ4/(1-λ)+52ϑ2)/(12+10dmax+/d),α(46ϑ4/(1-λ)+32ϑ2)/(12+12dmax+/d)].</text></s>
<s sid="736"><CoreSc1 advantage="None" conceptID="Res321" novelty="None" type="Res"/><text>Note that, by manipulating α, we can make K arbitrarily large, but this comes at the expense of a proportional increase in the timer durations of the main state machine and its underlying layers, increasing the overall stabilization time.</text></s>
<s sid="737"><CoreSc1 advantage="None" conceptID="Con118" novelty="None" type="Con"/><text>Summary of results from theory</text></s>
<s sid="738"><CoreSc1 advantage="None" conceptID="Con119" novelty="None" type="Con"/><text>We conclude the section with a summary of the most important statements proved in [13], expressed in terms of the model employed in this article.</text></s>
<s sid="739"><CoreSc1 advantage="None" conceptID="Res322" novelty="None" type="Res"/><text>To this end, we need to specify the protocol as a compound implementation about that we will formulate our theorems.</text></s>
Definition 5.1
The FATAL+ Protocol
<s sid="740"><CoreSc1 advantage="None" conceptID="Res323" novelty="None" type="Res"/><text>The FATAL+ protocol is a compound module consisting of nodes i∈{1,…,n}.</text></s>
<s sid="741"><CoreSc1 advantage="None" conceptID="Res324" novelty="None" type="Res"/><text>It has no input ports and an output port Li for each node i.</text></s>
<s sid="742"><CoreSc1 advantage="None" conceptID="Res325" novelty="None" type="Res"/><text>The n input ports of node i are connected to the output ports of the channels Si,j, j∈{1,…,n}.</text></s>
<s sid="743"><CoreSc1 advantage="None" conceptID="Res326" novelty="None" type="Res"/><text>Each node is comprised of one copy of each of the state machines presented in this section, and the implementation of each node is derived from the implementations (given in Section 6) of the basic modules defined in Section 2.2 that are connected as specified in this section.</text></s>
<s sid="744"><CoreSc1 advantage="None" conceptID="Res327" novelty="None" type="Res"/><text>The output port Li of node i is the output port of its quick cycle state machine.</text></s>
<s sid="745"><CoreSc1 advantage="None" conceptID="Res328" novelty="None" type="Res"/><text>The first theorem states a probabilistic stabilization result.</text></s>
<s sid="746"><CoreSc1 advantage="None" conceptID="Res329" novelty="None" type="Res"/><text>Since we did not formally define probabilistically stabilizing implementations, its formulation is somewhat cumbersome.</text></s>
<s sid="747"><CoreSc1 advantage="None" conceptID="Res330" novelty="None" type="Res"/><text>Intuitively (and slightly inaccurately), the statement is to be read as &quot;no matter what the initial state and the execution, the protocol stabilizes almost certainly within Tslow time&quot;.</text></s>
Theorem 5.2
<s sid="748"><CoreSc1 advantage="None" conceptID="Res331" novelty="None" type="Res"/><text>Fix any f′⩽f:=⌊(n-1)/3⌋ and feasible α, setTslow:=(24(1-λ)+3ϑ)R2+R1/ϑ+T1++T3++(9ϑ+8)d+5dmax+-dmax-∈Θ(αn), and pick K∈Θ(αn) in accordance with inequality (1).</text></s>
<s sid="749"><CoreSc1 advantage="None" conceptID="Res332" novelty="None" type="Res"/><text>Consider an execution on [t-,t+] of the FATAL+ protocol where (at least) n-f′ nodes are feasible.</text></s>
<s sid="750"><CoreSc1 advantage="None" conceptID="Con120" novelty="None" type="Con"/><text>Assume that an adversary that knows everything about the system except that it does not learn about the durations of randomized watchdog timers before they expire controls all other aspects of the execution (clock drifts and delays of feasible submodules within the admissible bounds as well as the output ports' signals of faulty modules).</text></s>
<s sid="751"><CoreSc1 advantage="None" conceptID="Res333" novelty="None" type="Res"/><text>Then the execution restricted to [t-+Tslow,t+] is with probability at least 1-2-(n-f) a feasible execution of a clock synchronization module with clock imprecision Σ=1, amortized frequency bounds A-=1/(T1++T3++3dmax+) and A+=1/(ϑ(T1++T3+)), slacks τ-=τ+=2, maximum frequency F+=1/(ϑ(T1++T3+-2dmax++dmin+)), at most f′ faults, and clocks of size K∈Θ(αn).</text></s>
<s sid="752"><CoreSc1 advantage="None" conceptID="Res334" novelty="None" type="Res"/><text>In this sense, for each f′⩽f, the FATAL+ protocol is an f′-tolerant implementation of a clock synchronization module with the respective parameters that stabilizes with probability at least 1-2-(n-f) within time Tslow∈O(αn).</text></s>
<s sid="753"><CoreSc1 advantage="None" conceptID="Res335" novelty="None" type="Res"/><text>The above theorem corresponds to the slow, but robust, second stabilization mechanism.</text></s>
<s sid="754"><CoreSc1 advantage="None" conceptID="Res336" novelty="None" type="Res"/><text>The next theorem, which corresponds to the faster first stabilization mechanism, essentially states that in an execution where n-f nodes already stabilized, any further non-faulty nodes recover quickly and deterministically, within O(α) time.</text></s>
Theorem 5.3
<s sid="755"><CoreSc1 advantage="None" conceptID="Res337" novelty="None" type="Res"/><text>We use the notation of the previous theorem. Moreover,Tfast:=T2+T4+(1+5/(2ϑ))R1+5d∈Θ(α).</text></s>
<s sid="756"><CoreSc1 advantage="None" conceptID="Res338" novelty="None" type="Res"/><text>Suppose an execution of the FATAL+ protocol is feasible on [t-,t+] with respect to the clock synchronization module specified in Theorem 5.2.</text></s>
<s sid="757"><CoreSc1 advantage="None" conceptID="Res339" novelty="None" type="Res"/><text>Consider the set of nodes W⊆N whose restricted executions on [t-,t+] are feasible.</text></s>
<s sid="758"><CoreSc1 advantage="None" conceptID="Res340" novelty="None" type="Res"/><text>Then the execution restricted to [t-+Tfast,t+] is feasible with respect to a clock synchronization module with the same parameters, except that it tolerates n-|W| faults only.</text></s>
<s sid="759"><CoreSc1 advantage="None" conceptID="Res341" novelty="None" type="Res"/><text>We should like to mention that in [13] a number of further results on stabilization are given.</text></s>
<s sid="760"><CoreSc1 advantage="None" conceptID="Res342" novelty="None" type="Res"/><text>In particular, if the faulty nodes exhibit only little coordination among themselves or do not tune their operations to the non-faulty nodes' states, also the &quot;slow&quot; stabilization mechanism will succeed quickly, granted that the resynchronization state machines are not in a &quot;too bad&quot; configuration, i.e., most timers of type R2 are expired and timeouts of type R3 are in (roughly) random states.</text></s>
<s sid="761"><CoreSc1 advantage="None" conceptID="Res343" novelty="None" type="Res"/><text>We will informally discuss some of these scenarios in Section 7.</text></s>
<s sid="762"><CoreSc1 advantage="None" conceptID="Res344" novelty="None" type="Res"/><text>Finally, we emphasize again that the power of the above theorems severely depends on the quality of basic implementations (cf.</text></s>
Section 4.2).
<s sid="763"><CoreSc1 advantage="None" conceptID="Con121" novelty="None" type="Con"/><text>While compound modules' properties can be formally analyzed, e.g.</text></s>
<s sid="764"><CoreSc1 advantage="None" conceptID="Con122" novelty="None" type="Con"/><text>giving rise to the theorems above, these results are meaningless if too many basic implementations are infeasible too frequently.</text></s>
<s sid="765"><CoreSc1 advantage="None" conceptID="Con123" novelty="None" type="Con"/><text>Hence it is vital to come up with robust implementations of the basic modules, which is the subject of the next section.</text></s>
Implementation
<s sid="766"><CoreSc1 advantage="None" conceptID="Con124" novelty="None" type="Con"/><text>In this section, we present the cornerstones of our FPGA prototype implementation of the FATAL+ protocol.</text></s>
<s sid="767"><CoreSc1 advantage="None" conceptID="Con125" novelty="None" type="Con"/><text>The objectives of this implementation are (i) to serve as a proof of concept, (ii) to validate the predictions of the theoretical analysis, and (iii) to form a basis for the future development of protocol variants and engineering improvements.</text></s>
<s sid="768"><CoreSc1 advantage="None" conceptID="Res345" novelty="None" type="Res"/><text>Rather than striving for optimizing performance, area, or power efficiency, our primary goal is hence to essentially provide a direct mapping of the algorithmic description to hardware, and to evaluate its properties in various operating scenarios.</text></s>
<s sid="769"><CoreSc1 advantage="None" conceptID="Res346" novelty="None" type="Res"/><text>Not surprisingly, traditional design principles for digital circuits are not adequate for our purposes.</text></s>
<s sid="770"><CoreSc1 advantage="None" conceptID="Res347" novelty="None" type="Res"/><text>This is true for three major reasons:•</text></s>
<s sid="771"><CoreSc1 advantage="None" conceptID="Res348" novelty="None" type="Res"/><text>Asynchrony: Targeting ultra-reliable clock generation in SoCs, the implementation of FATAL+ itself cannot rely on the availability of a synchronous clock.</text></s>
<s sid="772"><CoreSc1 advantage="None" conceptID="Res349" novelty="None" type="Res"/><text>Moreover, many guards, like the one of the transition from propose to accept in Fig. 8, depend on remote nodes' states and should hence not be synchronized to a local clock in order to maximize performance.</text></s>
<s sid="773"><CoreSc1 advantage="None" conceptID="Res350" novelty="None" type="Res"/><text>Testing for activated guards synchronized to a local clock source also increases the risk of generating metastability, as remote signals originate in different clock domains.</text></s>
<s sid="774"><CoreSc1 advantage="None" conceptID="Res351" novelty="None" type="Res"/><text>On the other hand, conventional asynchronous state machines (ASM) are not well-suited for implementing the state machines from Fig. 6-Fig. 10 due to the possibility of choice w.r.t.</text></s>
<s sid="775"><CoreSc1 advantage="None" conceptID="Res352" novelty="None" type="Res"/><text>successor states and continuously enabled (i.e., non-alternating) guards.</text></s>
<s sid="776"><CoreSc1 advantage="None" conceptID="Res353" novelty="None" type="Res"/><text>Our prototype hence relies on hybrid state machines (HSM) that combine ASM with synchronous transition state machines (TSM) that are started on demand only.</text></s>
•
<s sid="777"><CoreSc1 advantage="None" conceptID="Met53" novelty="None" type="Met"/><text>Fault tolerance: The consideration of Byzantine faulty nodes forced us to abandon the classic &quot;wait for all&quot; paradigm traditionally used for enforcing the indication principle in asynchronous designs: Failures may easily inhibit the completion of the request/acknowledge cycles typically used for transition-based flow control.</text></s>
<s sid="778"><CoreSc1 advantage="None" conceptID="Exp7" novelty="None" type="Exp"/><text>A few timing constraints, established by our theoretical analysis, in conjunction with state-based communication are resorted to in order to establish event ordering and synchronized executions in FATAL+.</text></s>
•
<s sid="779"><CoreSc1 advantage="None" conceptID="Res354" novelty="None" type="Res"/><text>Self-stabilization: In sharp contrast to non-stabilizing algorithms, which can always assume that there is a (substantial) number of non-faulty nodes that run approximately synchronously and hence jointly adhere to certain timing constraints, self-stabilizing algorithms cannot even assume this.</text></s>
<s sid="780"><CoreSc1 advantage="None" conceptID="Res355" novelty="None" type="Res"/><text>Although FATAL+ guarantees that non-faulty nodes will eventually execute synchronously, even when started from an arbitrary state, the violation of timing constraints and hence metastability cannot be avoided during stabilization [44].</text></s>
<s sid="781"><CoreSc1 advantage="None" conceptID="Res356" novelty="None" type="Res"/><text>For example, state accept in Fig. 8 has two successors sleep and recover, the guards of which could become true arbitrarily close to each other in certain stabilization scenarios.</text></s>
<s sid="782"><CoreSc1 advantage="None" conceptID="Res357" novelty="None" type="Res"/><text>This is acceptable, though, as long as such problematic events are neither systematic nor frequent, which is ensured by the design and implementation of FATAL+ (see Section 6.1).</text></s>
 Inspecting Figs.
<s sid="783"><CoreSc1 advantage="None" conceptID="Res358" novelty="None" type="Res"/><text>6-10 reveals that the state transitions of the FATAL+ state machines are triggered by AND/OR combinations of the following different types of conditions:(1)</text></s>
<s sid="784"><CoreSc1 advantage="None" conceptID="Res359" novelty="None" type="Res"/><text>A watchdog timer expires [&quot;(T2,accept)&quot;].</text></s>
(2)
<s sid="785"><CoreSc1 advantage="None" conceptID="Res360" novelty="None" type="Res"/><text>The state machines of a certain number (1, ⩾f+1, or ⩾n-f) of nodes reached a particular (subset of) state(s) at least once since the reset of the corresponding memory flags [&quot;⩾n-f accept&quot;].</text></s>
(3)
<s sid="786"><CoreSc1 advantage="None" conceptID="Res361" novelty="None" type="Res"/><text>The state machines of a certain number (1, ⩾f+1, or ⩾n-f) of nodes are currently in (one of) a particular (subset of) state(s) [&quot;in resync&quot;].</text></s>
(4)
Always [&quot;true&quot;].
<s sid="787"><CoreSc1 advantage="None" conceptID="Res362" novelty="None" type="Res"/><text>These requirements reveal the need for the following major building blocks (cf.</text></s>
Section 4):•
<s sid="788"><CoreSc1 advantage="None" conceptID="Res363" novelty="None" type="Res"/><text>Concurrent HSMs, implementing the states and transitions specified in the protocol.</text></s>
<s sid="789"><CoreSc1 advantage="None" conceptID="Res364" novelty="None" type="Res"/><text>An ideal HSM would always provide feasible executions of its state transition module.</text></s>
•
<s sid="790"><CoreSc1 advantage="None" conceptID="Res365" novelty="None" type="Res"/><text>Communication infrastructure between these state machines, continuously conveying state information.</text></s>
<s sid="791"><CoreSc1 advantage="None" conceptID="Res366" novelty="None" type="Res"/><text>This is simply done by the channels Si,j propagating the signal Si to all receivers.</text></s>
•
<s sid="792"><CoreSc1 advantage="None" conceptID="Res367" novelty="None" type="Res"/><text>Watchdog timers (also with random timeouts) for implementing type (1) guards.</text></s>
•
<s sid="793"><CoreSc1 advantage="None" conceptID="Res368" novelty="None" type="Res"/><text>Threshold modules and memory flags for implementing type (2) and type (3) guards.</text></s>
<s sid="794"><CoreSc1 advantage="None" conceptID="Con126" novelty="None" type="Con"/><text>If we could provide implementations of all these building blocks that match the specifications of the formal model in Section 2.2 under all circumstances, in the sense that all executions at non-faulty nodes are always feasible, the theoretical guarantees derived in [13] would apply without restriction.</text></s>
<s sid="795"><CoreSc1 advantage="None" conceptID="Res369" novelty="None" type="Res"/><text>As already noted, however, this is impossible to guarantee, since there is no way to rule out metastable upsets with complete certainty, and there are no elements available for our purpose whose behavior is specified for metastable inputs.</text></s>
<s sid="796"><CoreSc1 advantage="None" conceptID="Res370" novelty="None" type="Res"/><text>Nevertheless, it is possible to design our basic modules in a way that keeps the probability of such events acceptably low.</text></s>
<s sid="797"><CoreSc1 advantage="None" conceptID="Res371" novelty="None" type="Res"/><text>Moreover, all stateful components must be implemented in a self-stabilizing way: They must be able to eventually recover from an arbitrary erroneous internal state, including metastability, when facing sufficiently long executions on their input ports that do not induce metastability.</text></s>
<s sid="798"><CoreSc1 advantage="None" conceptID="Res372" novelty="None" type="Res"/><text>Before we proceed with a description of the implementations of the required basic modules, we discuss how FATAL+ deals with the threat of metastability arising from our extreme fault scenarios.</text></s>
Metastability issues
<s sid="799"><CoreSc1 advantage="None" conceptID="Con127" novelty="None" type="Con"/><text>Reducing the potential for both metastability generation and metastability propagation are important goals in the design and implementation of FATAL+.</text></s>
<s sid="800"><CoreSc1 advantage="None" conceptID="Con128" novelty="None" type="Con"/><text>Although it is impossible to completely rule out metastability generation in the presence of Byzantine faulty nodes (which may issue signal transitions at arbitrary times anyway) and during self-stabilization (where all nodes may be completely unsynchronized), we nevertheless achieve the following properties.</text></s>
<s sid="801"><CoreSc1 advantage="None" conceptID="Res373" novelty="None" type="Res"/><text>Robustness against metastable upsets and their propagation:(I)</text></s>
<s sid="802"><CoreSc1 advantage="None" conceptID="Res374" novelty="None" type="Res"/><text>Guaranteed metastability-freedom in fault-free executions after stabilization.</text></s>
(II)
<s sid="803"><CoreSc1 advantage="None" conceptID="Res375" novelty="None" type="Res"/><text>Low probability of metastable upsets: We have taken care to keep the windows of vulnerability of our implementations of basic modules as small as possible.</text></s>
<s sid="804"><CoreSc1 advantage="None" conceptID="Con129" novelty="None" type="Con"/><text>Thus, desynchronized or faulty nodes must be very lucky to actually trigger a metastable upset.</text></s>
<s sid="805"><CoreSc1 advantage="None" conceptID="Con130" novelty="None" type="Con"/><text>In addition, mechanisms for decreasing the upset probability even further could be incorporated, if required in particularly critical applications.</text></s>
(III)
<s sid="806"><CoreSc1 advantage="None" conceptID="Res376" novelty="None" type="Res"/><text>Metastability containment: Non-faulty nodes are very robust against propagation of metastable upsets due to the algorithm's control flow.</text></s>
<s sid="807"><CoreSc1 advantage="None" conceptID="Res377" novelty="None" type="Res"/><text>Limited consequences of metastable upsets:(IV)</text></s>
<s sid="808"><CoreSc1 advantage="None" conceptID="Res378" novelty="None" type="Res"/><text>Limited impact of metastable upsets during stabilization: Metastable upsets that occur at non-faulty nodes during the stabilization phase can only delay stabilization.</text></s>
<s sid="809"><CoreSc1 advantage="None" conceptID="Res379" novelty="None" type="Res"/><text>Since these are rare events even then, the respective effect on the (average) stabilization time is very small.</text></s>
(V)
<s sid="810"><CoreSc1 advantage="None" conceptID="Res380" novelty="None" type="Res"/><text>Fast recovery from metastability after stabilization: As long as n-f non-faulty nodes remain synchronized, a metastable upset at a node may disrupt its synchrony towards the other nodes only for a short time.</text></s>
<s sid="811"><CoreSc1 advantage="None" conceptID="Res381" novelty="None" type="Res"/><text>Due to the fast stabilization mechanism the node will fully recover within O(1) time once metastability ceases.</text></s>
(VI)
<s sid="812"><CoreSc1 advantage="None" conceptID="Res382" novelty="None" type="Res"/><text>Masking of metastable upsets as faults: Provided that the measures ensuring (II) and (III) are effective (i.e., metastability does not spread) and the system-level fault-tolerance of f nodes operating outside their specification is not exhausted, metastable upsets at some nodes do not affect the correctness of other nodes.</text></s>
<s sid="813"><CoreSc1 advantage="None" conceptID="Bac42" novelty="None" type="Bac"/><text>The following approaches have been used in FATAL+ to accomplish these goals (additional details will be given in the subsequent sections):(I)</text></s>
<s sid="814"><CoreSc1 advantage="None" conceptID="Con131" novelty="None" type="Con"/><text>If all nodes are synchronized and fault-free, we can satisfy timing constraints on the modules' input ports' signals that ensure that even our (necessarily imperfect) implementations of the abstract modules maintain feasibility at all times.</text></s>
<s sid="815"><CoreSc1 advantage="None" conceptID="Con132" novelty="None" type="Con"/><text>Essentially, the argument is that since there is no initial violation of the constraints and no faults are imposed by external events, we can conclude that the constraints will be satisfied at later points in time as well.</text></s>
<s sid="816"><CoreSc1 advantage="None" conceptID="Res383" novelty="None" type="Res"/><text>This property is formally proved in [13].</text></s>
(II)
<s sid="817"><CoreSc1 advantage="None" conceptID="Res384" novelty="None" type="Res"/><text>All building blocks that are susceptible to metastable upsets, like memory flags, are implemented in a way that minimizes the time span during which they are vulnerable.</text></s>
<s sid="818"><CoreSc1 advantage="None" conceptID="Res385" novelty="None" type="Res"/><text>Moreover, elastic pipelines acting as metastability filters [40] or synchronizers could be added easily to our design to further protect such elements.</text></s>
(III)
<s sid="819"><CoreSc1 advantage="None" conceptID="Res386" novelty="None" type="Res"/><text>We enforce (standard) error containment by avoiding any explicit control flow between ASMs: Since the communication is exclusively performed by virtue of states, a faulty receiver cannot impact a non-faulty sender, and a faulty sender, in turn, cannot directly interfere with the operation of a non-faulty receiver (apart from conveying an incorrect state, of course).</text></s>
<s sid="820"><CoreSc1 advantage="None" conceptID="Res387" novelty="None" type="Res"/><text>To extend error containment to also cover metastability to the best possible extent, several forms of logical masking are employed.</text></s>
<s sid="821"><CoreSc1 advantage="None" conceptID="Res388" novelty="None" type="Res"/><text>One example is the combination of memory flags and threshold gates, which ensure that possibly upset memory flags are always overruled quickly by correct ones at the threshold output.1818</text></s>
<s sid="822"><CoreSc1 advantage="None" conceptID="Res389" novelty="None" type="Res"/><text>It is not self-evident that this type of masking is very effective for metastability as well.</text></s>
<s sid="823"><CoreSc1 advantage="None" conceptID="Res390" novelty="None" type="Res"/><text>Later on we will discuss why this is indeed the case.</text></s>
<s sid="824"><CoreSc1 advantage="None" conceptID="Res391" novelty="None" type="Res"/><text>A higher-level form of logical masking occurs due to the fact that, after stabilization, all non-faulty nodes execute the outer cycle of the main state machine (Fig. 8) only.</text></s>
<s sid="825"><CoreSc1 advantage="None" conceptID="Obs31" novelty="None" type="Obs"/><text>The outer cycle's guards do not involve any of the timeouts, states, or flags accessed by the resynchronization routine (Fig. 10) or the extension of the main state machine (Fig. 9); hence any metastability of the corresponding signals does not affect the logic of the main state machine and the layers on top of it (including the logical clocks).</text></s>
(IV)
<s sid="826"><CoreSc1 advantage="None" conceptID="Res392" novelty="None" type="Res"/><text>The measures outlined in (II) and (III) are complemented by adding time masking using randomization.</text></s>
<s sid="827"><CoreSc1 advantage="None" conceptID="Res393" novelty="None" type="Res"/><text>The resynchronization routine (Fig. 10) tries to initialize recovery from arbitrary states at random, sufficiently sparse points in time.</text></s>
<s sid="828"><CoreSc1 advantage="None" conceptID="Res394" novelty="None" type="Res"/><text>Hence non-faulty nodes cannot be systematically kept from stabilizing.</text></s>
<s sid="829"><CoreSc1 advantage="None" conceptID="Con133" novelty="None" type="Con"/><text>The proofs in [13] reveal that within O(n) time in fact it is likely that there are multiple events that will imply subsequent stabilization.</text></s>
<s sid="830"><CoreSc1 advantage="None" conceptID="Con134" novelty="None" type="Con"/><text>Considering that metastable upsets are rare events in our setting, their impact thus becomes negligible.</text></s>
(V)
<s sid="831"><CoreSc1 advantage="None" conceptID="Res395" novelty="None" type="Res"/><text>This property directly follows from the results shown in [13]: If n-f nodes faithfully execute the basic cycle, any non-faulty node will (re)synchronize within O(1) time, irrespectively of its current state.</text></s>
(VI)
<s sid="832"><CoreSc1 advantage="None" conceptID="Res396" novelty="None" type="Res"/><text>If metastability does not spread to a given receiver, the latter will observe for each sender some execution, even if the sender does not send a valid signal in terms of our system model.</text></s>
<s sid="833"><CoreSc1 advantage="None" conceptID="Res397" novelty="None" type="Res"/><text>Since we assume that faulty nodes may output arbitrary signals at their output ports, our model thus makes no distinction between a &quot;conventionally&quot; faulty node and one that behaves erratically due to metastable upsets.1919</text></s>
<s sid="834"><CoreSc1 advantage="None" conceptID="Res398" novelty="None" type="Res"/><text>To match our model, invalid signal states are simply mapped to some default state, e.g.</text></s>
<s sid="835"><CoreSc1 advantage="None" conceptID="Res399" novelty="None" type="Res"/><text>resync for the main state machine.</text></s>
<s sid="836"><CoreSc1 advantage="None" conceptID="Res400" novelty="None" type="Res"/><text>As the algorithm is resilient to up to f faults, such upsets are masked as long as the total number of nodes operating outside their module specification is at most f.</text></s>
State machine communication
<s sid="837"><CoreSc1 advantage="None" conceptID="Con135" novelty="None" type="Con"/><text>According to our system model, an HSM of node i must continuously communicate its current state system-wide via the channels Sj,i.</text></s>
<s sid="838"><CoreSc1 advantage="None" conceptID="Con136" novelty="None" type="Con"/><text>For simplicity, we use parallel communication, by means of a suitably sized data bus, in our implementation.2020</text></s>
<s sid="839"><CoreSc1 advantage="None" conceptID="Res401" novelty="None" type="Res"/><text>It is, however, possible to replace parallel communication by serial communication, e.g., by extending the (synchronous) TSM (see Section 6.3) appropriately.</text></s>
<s sid="840"><CoreSc1 advantage="None" conceptID="Res402" novelty="None" type="Res"/><text>A complete receiver as described below is employed for every state machine in the system.</text></s>
<s sid="841"><CoreSc1 advantage="None" conceptID="Res403" novelty="None" type="Res"/><text>Since a node treats itself like any other node in type (2) and type (3) guards with thresholds, every node receives its own state as well.</text></s>
Channels
<s sid="842"><CoreSc1 advantage="None" conceptID="Obs32" novelty="None" type="Obs"/><text>Fig. 11 shows the circuitry used for communicating the current state of the main algorithm in Fig. 8.</text></s>
<s sid="843"><CoreSc1 advantage="None" conceptID="Con137" novelty="None" type="Con"/><text>The sender consists of a simple array of flip-flops, which drive the parallel data bus that thus continuously reflects the current state of the sender's HSM.</text></s>
<s sid="844"><CoreSc1 advantage="None" conceptID="Con138" novelty="None" type="Con"/><text>Technically speaking, the flip-flops are not part of the channel but rather the sender's HSM; they are the &quot;physical location&quot; of the HSM's state in the sense of our model.</text></s>
<s sid="845"><CoreSc1 advantage="None" conceptID="Res404" novelty="None" type="Res"/><text>The channel thus &quot;begins&quot; with the wires conveying the stored values.2121</text></s>
<s sid="846"><CoreSc1 advantage="None" conceptID="Res405" novelty="None" type="Res"/><text>Note that there is some freedom with respect to the mapping of module ports to the physical system, which also affects which module(s) become(s) infeasible due to a (physical) fault.</text></s>
<s sid="847"><CoreSc1 advantage="None" conceptID="Res406" novelty="None" type="Res"/><text>However, no matter what the precise mapping, care has to be taken to avoid correlated failures.</text></s>
<s sid="848"><CoreSc1 advantage="None" conceptID="Res407" novelty="None" type="Res"/><text>For instance, if all channels meet in a single spot due to bad routing, manufacturing defects or electromigration could connect several channels, therefore rendering our system-level fault-tolerance (i.e., the resilience to f node failures) ineffective.</text></s>
<s sid="849"><CoreSc1 advantage="None" conceptID="Res408" novelty="None" type="Res"/><text>In sharp contrast to handshake-based communication, reading at the receiver occurs without any direct coordination with the sender.</text></s>
<s sid="850"><CoreSc1 advantage="None" conceptID="Con139" novelty="None" type="Con"/><text>To avoid the unacceptable risk of reading and capturing false intermediate sender states, which might be perceived by the receiver upon a sender state transition in case of different delays on the data bus wires, delay-insensitive state coding [45] must be used.</text></s>
<s sid="851"><CoreSc1 advantage="None" conceptID="Res409" novelty="None" type="Res"/><text>We have chosen the following encoding for the main state machine in Fig. 8:propose0000accept1001sleep1011sleep→waking0011waking0101ready0110recover1100join1010</text></s>
<s sid="852"><CoreSc1 advantage="None" conceptID="Res410" novelty="None" type="Res"/><text>The receiver comprises a simple combinational decoder consisting of AND gates, which generate a 1-out-of-m encoding of the binary representation of the state communicated via the data bus.</text></s>
<s sid="853"><CoreSc1 advantage="None" conceptID="Res411" novelty="None" type="Res"/><text>The decoded signals correspond to a single sender state each.</text></s>
<s sid="854"><CoreSc1 advantage="None" conceptID="Res412" novelty="None" type="Res"/><text>This information is directly used for type (3) guards, and fed into memory flags for type (2) guards.</text></s>
<s sid="855"><CoreSc1 advantage="None" conceptID="Res413" novelty="None" type="Res"/><text>For the other state machines making up FATAL+, it suffices to communicate only a single bit of state information (supp or none in Fig. 9, init or wait in Fig. 10, and propose+ or none+ in Fig. 6).</text></s>
<s sid="856"><CoreSc1 advantage="None" conceptID="Res414" novelty="None" type="Res"/><text>Hence, every bus consists of a single sender flip-flop plus a wire here, and the decoder in the receiver becomes trivial.</text></s>
<s sid="857"><CoreSc1 advantage="None" conceptID="Res415" novelty="None" type="Res"/><text>In the sequel, we restrict our discussion to the main state machine's channel, as the simpler single-bit channels clearly meet the specification of a channel.</text></s>
<s sid="858"><CoreSc1 advantage="None" conceptID="Res416" novelty="None" type="Res"/><text>Note that in both cases the (physical) channels used in our implementation trivially recover from any inputs and transient faults, as they are obviously forgetful.</text></s>
<s sid="859"><CoreSc1 advantage="None" conceptID="Res417" novelty="None" type="Res"/><text>The memory flags at the receiver's side contain feedback-loops, however, which do not allow us to apply Theorem 3.7 and Lemma 3.8.</text></s>
Correctness.
We now argue informally2222
<s sid="860"><CoreSc1 advantage="None" conceptID="Res418" novelty="None" type="Res"/><text>Our basic modules appear simple enough to be amenable to formal verification.</text></s>
<s sid="861"><CoreSc1 advantage="None" conceptID="Res419" novelty="None" type="Res"/><text>Still, there are complications: Besides the fact that we assume not only continuous time but also continuous computations, which rules out using standard verification approaches, there is the challenge of finding and expressing suitable input port execution constraints required for implementation correctness.</text></s>
<s sid="862"><CoreSc1 advantage="None" conceptID="Res420" novelty="None" type="Res"/><text>Exploring this avenue is part of our future work.</text></s>
<s sid="863"><CoreSc1 advantage="None" conceptID="Res421" novelty="None" type="Res"/><text>why and when the above implementation matches the specifications given in Section 4.</text></s>
<s sid="864"><CoreSc1 advantage="None" conceptID="Res422" novelty="None" type="Res"/><text>Note that when affected by faults or provided with illegal inputs, modules may of course exhibit arbitrary behavior.</text></s>
<s sid="865"><CoreSc1 advantage="None" conceptID="Res423" novelty="None" type="Res"/><text>In that case we rely on (a) the system-level fault tolerance properties (for fault masking), (b) the self-stabilization properties of the affected modules (for recovery), and (c) the rare occurrence of these situations (in order to not exhaust the system-level fault tolerance limits).</text></s>
<s sid="866"><CoreSc1 advantage="None" conceptID="Res424" novelty="None" type="Res"/><text>In addition to considering the fault-free behavior, it hence suffices to restrict our attention to (b) and (c) here.</text></s>
<s sid="867"><CoreSc1 advantage="None" conceptID="Res425" novelty="None" type="Res"/><text>For fault-free operation, the described implementation essentially realizes a channel as specified in Section 4 with some maximum delay dChan, granted that changes of the input provided by the sender are separated in time sufficiently well.</text></s>
<s sid="868"><CoreSc1 advantage="None" conceptID="Res426" novelty="None" type="Res"/><text>To see this, consider an input switch from state s to s′ (note that not all flip-flops will switch their output signals at exactly the same instant), where initially the signal is stable also on the receiver's side.</text></s>
<s sid="869"><CoreSc1 advantage="None" conceptID="Res427" novelty="None" type="Res"/><text>Once the signal change propagated through the wires and the AND gates, the decoder output signal corresponding to state s′ will be 1, while all other signals will be 0.</text></s>
<s sid="870"><CoreSc1 advantage="None" conceptID="Res428" novelty="None" type="Res"/><text>Due to the use of delay-insensitive state encoding, there are no glitches and the signals for all other states s″∉{s,s′} will continuously be 0.</text></s>
<s sid="871"><CoreSc1 advantage="None" conceptID="Res429" novelty="None" type="Res"/><text>Nevertheless, formally, this behavior does not yet fully match the definition of our communication channels in Section 4: It is possible that temporarily both s and s′ are 1.</text></s>
<s sid="872"><CoreSc1 advantage="None" conceptID="Con140" novelty="None" type="Con"/><text>Since our algorithms are completely oblivious to the exact point in time when the perceived Si,j changes after the sender's state Sj changed (the analysis in [13] only requires that this happens within d time), however, this problem can easily be abstracted away.2323</text></s>
<s sid="873"><CoreSc1 advantage="None" conceptID="Con141" novelty="None" type="Con"/><text>Formally, this abstraction builds upon a weakened definition of lower-level channels, which attain values from S∪(S2).</text></s>
<s sid="874"><CoreSc1 advantage="None" conceptID="Con142" novelty="None" type="Con"/><text>Alternatively, it would also be possible to use an explicit transition state ⊥ (encoded by any bit sequence not corresponding to a state), and force the sender to always perform state transition via ⊥.</text></s>
<s sid="875"><CoreSc1 advantage="None" conceptID="Res430" novelty="None" type="Res"/><text>All that is needed here is to interpret, in a static way, the situation where both s and s′ are valid as, say, s.</text></s>
<s sid="876"><CoreSc1 advantage="None" conceptID="Res431" novelty="None" type="Res"/><text>The attentive reader will have noticed that the 1-out-of-m decoder outputs (i.e., the state signals at the inputs of the memory flags) may temporarily be all 0 during the reception of a sender state transition as well.</text></s>
<s sid="877"><CoreSc1 advantage="None" conceptID="Res432" novelty="None" type="Res"/><text>Fortunately, this behavior is completely masked from becoming visible to our algorithms: The memory flags prohibit this from becoming visible in type (2) guards at all, and all state transition conditions involving type (3) guards refer to a single state only.</text></s>
<s sid="878"><CoreSc1 advantage="None" conceptID="Con143" novelty="None" type="Con"/><text>Hence, in terms of the transition condition, a similar abstraction as above is valid (i.e., for a remote state transition from s to s′ with a &quot;gap&quot; we can define an equivalent execution without gap in which the node in question behaves identically).</text></s>
<s sid="879"><CoreSc1 advantage="None" conceptID="Con144" novelty="None" type="Con"/><text>The above arguments critically rely on the assumption that states change not too rapidly.</text></s>
<s sid="880"><CoreSc1 advantage="None" conceptID="Con145" novelty="None" type="Con"/><text>Otherwise, the receiver could e.g.</text></s>
<s sid="881"><CoreSc1 advantage="None" conceptID="Con146" novelty="None" type="Con"/><text>fail to observe states that the sender assumed for a too short period of time only, or even decode a state that has not been attained.</text></s>
<s sid="882"><CoreSc1 advantage="None" conceptID="Res433" novelty="None" type="Res"/><text>For non-faulty nodes, this is guaranteed in our implementation because the minimal amount of time an HSM needs to complete a state transition is greater than the maximum end-to-end delay variation of the signals employed in the communication channel.</text></s>
<s sid="883"><CoreSc1 advantage="None" conceptID="Res434" novelty="None" type="Res"/><text>This constraint is easy to ensure by proper circuit design rules.</text></s>
Metastability.
<s sid="884"><CoreSc1 advantage="None" conceptID="Res435" novelty="None" type="Res"/><text>Within the communication channels themselves, metastable upsets could only occur in the senders' flip-flops and in the receivers' memory flags; everything else is stateless combinational logic.</text></s>
<s sid="885"><CoreSc1 advantage="None" conceptID="Res436" novelty="None" type="Res"/><text>The flip-flops are clocked by the sender's own clock, hence could become metastable only in case of a faulty sender.</text></s>
<s sid="886"><CoreSc1 advantage="None" conceptID="Res437" novelty="None" type="Res"/><text>The issue of upsets of the memory flags is discussed in Section 6.2.2.</text></s>
<s sid="887"><CoreSc1 advantage="None" conceptID="Res438" novelty="None" type="Res"/><text>Viewed at the node level, it is obvious that if the sender's state signal becomes metastable or changes too quickly (which can only happen if the sender is faulty), this can also induce metastability at the receiver side by propagation over the channel.</text></s>
<s sid="888"><CoreSc1 advantage="None" conceptID="Con147" novelty="None" type="Con"/><text>During the stabilization phase, the receiver could also experience a channel-induced metastable upset in memory flags and/or in its HSMs due to the arbitrary desynchronization between sender and receiver; since the windows of vulnerability are very small, the upset probability is very low, though.</text></s>
<s sid="889"><CoreSc1 advantage="None" conceptID="Res439" novelty="None" type="Res"/><text>Eventually, after stabilization, the synchrony between non-faulty nodes guaranteed by the FATAL+ protocol ensures that the received state data will always be stable when read in a transition condition in the main algorithm's outer cycle, recall item (I) in Section 6.1.</text></s>
Memory flags
<s sid="890"><CoreSc1 advantage="None" conceptID="Res440" novelty="None" type="Res"/><text>Every memory flag is just an SR-latch with dominant reset, whose functional equivalents are also depicted in Fig. 11.</text></s>
<s sid="891"><CoreSc1 advantage="None" conceptID="Res441" novelty="None" type="Res"/><text>Note that a memory flag is set depending on the state communicated by the sender, but (dominantly) cleared under the receiver's control.</text></s>
Metastability.
<s sid="892"><CoreSc1 advantage="None" conceptID="Res442" novelty="None" type="Res"/><text>A memory flag may become metastable when the inputs change during stabilization of its feedback loop, which can occur due to (a) input glitches and/or (b) simultaneous falling transitions on both inputs.</text></s>
<s sid="893"><CoreSc1 advantage="None" conceptID="Res443" novelty="None" type="Res"/><text>However, for correct receivers, (a) can only occur in case of a faulty sender, and (b) is again only possible during stabilization: Once non-faulty nodes execute the outer cycle of Fig. 8, it is guaranteed that e.g.</text></s>
<s sid="894"><CoreSc1 advantage="None" conceptID="Res444" novelty="None" type="Res"/><text>all non-faulty nodes enter accept before the first one leaves.</text></s>
<s sid="895"><CoreSc1 advantage="None" conceptID="Res445" novelty="None" type="Res"/><text>Overall, the upset probability is thus very small.</text></s>
<s sid="896"><CoreSc1 advantage="None" conceptID="Res446" novelty="None" type="Res"/><text>It could be further reduced by diverse known means for metastability filtering, like using an elastic pipeline or Schmitt-trigger stages (which must be accounted for in the delay bounds, though).</text></s>
<s sid="897"><CoreSc1 advantage="None" conceptID="Res447" novelty="None" type="Res"/><text>Finally, it is well-known that a metastable flip-flop will recover in finite time with probability one [44].</text></s>
<s sid="898"><CoreSc1 advantage="None" conceptID="Res448" novelty="None" type="Res"/><text>Any SR latch matches the specification of a memory flag according to Section 4 followed by a channel with some maximum delay dMem, provided that it starts from a clean initial state and the set/reset signals avoid (a) and (b) above.</text></s>
<s sid="899"><CoreSc1 advantage="None" conceptID="Res449" novelty="None" type="Res"/><text>As argued above, the latter is guaranteed by our algorithm except in case of a metastable upset.</text></s>
<s sid="900"><CoreSc1 advantage="None" conceptID="Res450" novelty="None" type="Res"/><text>In case of the memory flag implementation shown in Fig. 11, dMem is primarily determined by the end-to-end settling time of the feedback loop.</text></s>
<s sid="901"><CoreSc1 advantage="None" conceptID="Res451" novelty="None" type="Res"/><text>This delay also determines the vulnerability window with respect to metastability (i.e. critical glitch length, and &quot;simultaneity&quot; of transitions).</text></s>
<s sid="902"><CoreSc1 advantage="None" conceptID="Res452" novelty="None" type="Res"/><text>Hence, making dMem small, which is easy to achieve by design, contributes to both speed and robustness.</text></s>
<s sid="903"><CoreSc1 advantage="None" conceptID="Res453" novelty="None" type="Res"/><text>Except in case of metastability, discussed before, our memory flag implementation is self-stabilizing since it is dMem-forgetful in the presence of input executions that avoid (a) and (b).</text></s>
Threshold modules
<s sid="904"><CoreSc1 advantage="None" conceptID="Res454" novelty="None" type="Res"/><text>The most straightforward implementation of the threshold modules used for generating the ⩾f+1 and ⩾n-f thresholds in type (2) and type (3) guards is a simple sum-of-product network, which just builds the OR of all AND combinations of f+1 respectively n-f inputs.</text></s>
<s sid="905"><CoreSc1 advantage="None" conceptID="Res455" novelty="None" type="Res"/><text>This implementation however quickly becomes highly expensive, as it requires Θ((nf)) gates.</text></s>
<s sid="906"><CoreSc1 advantage="None" conceptID="Res456" novelty="None" type="Res"/><text>A more efficient alternative is a sorting network, where the kth output indicates whether a threshold of k is reached.</text></s>
<s sid="907"><CoreSc1 advantage="None" conceptID="Res457" novelty="None" type="Res"/><text>For simplicity, in our FPGA implementation, threshold modules are built by means of lookup-tables (LUT).</text></s>
Correctness.
<s sid="908"><CoreSc1 advantage="None" conceptID="Res458" novelty="None" type="Res"/><text>Similar to our memory flag implementation, it is impossible to implement the properties of a threshold module as stated in Section 4, followed by a channel with some maximum delay dTh, in case of arbitrary inputs: Finding out whether a certain number of inputs is 1 exactly at the same time cannot be implemented with real circuits.</text></s>
<s sid="909"><CoreSc1 advantage="None" conceptID="Res459" novelty="None" type="Res"/><text>All implementations proposed above are forgetful and their outputs will stabilize quickly if their inputs do not change.</text></s>
<s sid="910"><CoreSc1 advantage="None" conceptID="Res460" novelty="None" type="Res"/><text>Moreover, after stabilization type (2) guards are irrelevant, since neither the basic cycle of the main state machine nor the quick cycle evaluate such guards.</text></s>
<s sid="911"><CoreSc1 advantage="None" conceptID="Con148" novelty="None" type="Con"/><text>Hence, in this case we can restrict our attention to input executions where inputs may change from 0 to 1 only, not back.</text></s>
<s sid="912"><CoreSc1 advantage="None" conceptID="Res461" novelty="None" type="Res"/><text>The reset of the memory flags to 0 is performed during state transitions (when the guards' signals are suppressed by the locked signal) and therefore safe.</text></s>
<s sid="913"><CoreSc1 advantage="None" conceptID="Con149" novelty="None" type="Con"/><text>As any of the proposed threshold module implementations involve combinational logic only, they are trivially self-stabilizing: According to Theorem 3.7, they are forgetful and hence, by Lemma 3.8, self-stabilizing.</text></s>
<s sid="914"><CoreSc1 advantage="None" conceptID="Con150" novelty="None" type="Con"/><text>Therefore, provided that the longest path delay does not exceed dTh, the properties stated in Section 4 are satisfied for monotonic inputs.2424</text></s>
<s sid="915"><CoreSc1 advantage="None" conceptID="Con151" novelty="None" type="Con"/><text>Some dedicated experiments confirmed that even our LUT implementation on an FPGA, for which we have no control over the placement, operates glitch-free on monotonic inputs.</text></s>
Metastability.
<s sid="916"><CoreSc1 advantage="None" conceptID="Con152" novelty="None" type="Con"/><text>As discussed above, type (2) guards cannot be safely evaluated by threshold gates and may cause glitches or metastable upsets.</text></s>
<s sid="917"><CoreSc1 advantage="None" conceptID="Con153" novelty="None" type="Con"/><text>Since this is of relevance before stabilization only, this risk is considered acceptable.</text></s>
<s sid="918"><CoreSc1 advantage="None" conceptID="Con154" novelty="None" type="Con"/><text>Like our channel implementations, threshold modules can propagate metastability: A metastable input could be propagated to the output when there are exactly k-1 non-faulty inputs in state 1 and the metastable input therefore makes the difference between output 0 and 1.</text></s>
<s sid="919"><CoreSc1 advantage="None" conceptID="Res462" novelty="None" type="Res"/><text>In all other cases, however, the metastable input will simply be masked.</text></s>
<s sid="920"><CoreSc1 advantage="None" conceptID="Res463" novelty="None" type="Res"/><text>Thus, albeit not perfect, threshold gates are an efficient means for metastability containment.</text></s>
Hybrid state machines
<s sid="921"><CoreSc1 advantage="None" conceptID="Res464" novelty="None" type="Res"/><text>Our prototype implementation of FATAL+ relies on hybrid state machines (HSM): An asynchronous state machine (ASM) is used for determining, by asynchronously evaluating the guards, the points in time when a state transition shall occur.</text></s>
<s sid="922"><CoreSc1 advantage="None" conceptID="Res465" novelty="None" type="Res"/><text>Our ASMs have been built by deriving a state transition graph (STG) specification directly from Figs.</text></s>
<s sid="923"><CoreSc1 advantage="None" conceptID="Res466" novelty="None" type="Res"/><text>6-10 and generating the delay-insensitive implementation via Petrify [46].</text></s>
<s sid="924"><CoreSc1 advantage="None" conceptID="Res467" novelty="None" type="Res"/><text>The actual state transition of an HSM is governed by an underlying synchronous transition state machine (TSM).</text></s>
<s sid="925"><CoreSc1 advantage="None" conceptID="Res468" novelty="None" type="Res"/><text>The TSM resolves a possibly non-deterministic choice of the successor state and then sequentially performs the required transition actions:1.</text></s>
<s sid="926"><CoreSc1 advantage="None" conceptID="Res469" novelty="None" type="Res"/><text>&quot;Locking&quot; the transition, i.e., disabling any other transitions of the ASM (despite possibly satisfied guards); this happens at the start of the TSM and is thus asynchronously triggered.</text></s>
2.
<s sid="927"><CoreSc1 advantage="None" conceptID="Res470" novelty="None" type="Res"/><text>Reset of memory flags and watchdog timers.</text></s>
3.
<s sid="928"><CoreSc1 advantage="None" conceptID="Res471" novelty="None" type="Res"/><text>Communication of the new state, i.e., writing its representation into the flip-flops whose output is fed into the channels Sj,i.</text></s>
4.
<s sid="929"><CoreSc1 advantage="None" conceptID="Res472" novelty="None" type="Res"/><text>Completing the transition to the new state by enabling further transitions of the ASM.</text></s>
<s sid="930"><CoreSc1 advantage="None" conceptID="Res473" novelty="None" type="Res"/><text>The TSM is driven by a pausable clock (see Section 6.4), which is started dynamically by the ASM upon triggering the transition.</text></s>
<s sid="931"><CoreSc1 advantage="None" conceptID="Res474" novelty="None" type="Res"/><text>Note that this avoids the need for synchronization with a free-running clock and hence preserves the ASM's continuous time scale.</text></s>
<s sid="932"><CoreSc1 advantage="None" conceptID="Res475" novelty="None" type="Res"/><text>The TSM works as follows (see Fig. 12): Assume that the ASM is in state A, and that the guard G for the transition from A to B becomes true.</text></s>
<s sid="933"><CoreSc1 advantage="None" conceptID="Res476" novelty="None" type="Res"/><text>If no other transition is currently being taken (indicated by the locked signal being 0), the TSM clock is started and the TSM sequence counter is released.</text></s>
<s sid="934"><CoreSc1 advantage="None" conceptID="Res477" novelty="None" type="Res"/><text>With every rising edge of TSMClock, the TSM moves through a sequence of three states: synchronize (Syn), commit (Cmt), and terminate (Trm) shown in the rectangular box in Fig. 12.</text></s>
<s sid="935"><CoreSc1 advantage="None" conceptID="Res478" novelty="None" type="Res"/><text>In Syn, the locked signal is activated to prevent other choices from being executed in case of more than one guard becoming true.</text></s>
<s sid="936"><CoreSc1 advantage="None" conceptID="Res479" novelty="None" type="Res"/><text>Once the TSM has reached Syn, it has decided to actually take the transition to B and hence moves on to state Cmt.</text></s>
<s sid="937"><CoreSc1 advantage="None" conceptID="Res480" novelty="None" type="Res"/><text>Here the watchdog timer associated with B and possibly some memory flags are cleared according to the FATAL+ state machine, and the new state B is captured by the output flip-flops driving the state communication data bus (recall Section 6.2).</text></s>
<s sid="938"><CoreSc1 advantage="None" conceptID="Res481" novelty="None" type="Res"/><text>Note that the resulting delay must be accounted for in the communication delay bounds d, dmax+ and dmin+.</text></s>
<s sid="939"><CoreSc1 advantage="None" conceptID="Res482" novelty="None" type="Res"/><text>Finally, the TSM moves on to state Trm, in which the reset signals are inactivated again and the TSM clock is halted (and the TSM sequence counter forced to the reset state).</text></s>
<s sid="940"><CoreSc1 advantage="None" conceptID="Res483" novelty="None" type="Res"/><text>The locked signal is also cleared here, which effectively moves the ASM to state B.</text></s>
<s sid="941"><CoreSc1 advantage="None" conceptID="Res484" novelty="None" type="Res"/><text>It is only now that guards pertaining to state B may become true.</text></s>
Metastability.
<s sid="942"><CoreSc1 advantage="None" conceptID="Res485" novelty="None" type="Res"/><text>Whereas any ambiguity of state transitions due to multiple activated guards can easily be resolved via some priority rule, metastability due to (a) enabled guards that become immediately disabled again or (b) new guards that are enabled close to &quot;locking&quot; time cannot be ruled out in general.</text></s>
<s sid="943"><CoreSc1 advantage="None" conceptID="Res486" novelty="None" type="Res"/><text>However, as argued in Section 6.1, in FATAL+ (a) could only occur during stabilization, due to type (3) guards, or due to faulty nodes successfully inducing metastability of memory flags; recall that otherwise type (1) and type (2) guards are always monotonic, with the reset (of watchdog timers and memory flags) being under the control of the local state machine.</text></s>
<s sid="944"><CoreSc1 advantage="None" conceptID="Res487" novelty="None" type="Res"/><text>Similarly, our proofs in [13] reveal that upsets due to (b) do not occur after stabilization in the main state machine and the quick cycle (Figs.</text></s>
6 and 8).
<s sid="945"><CoreSc1 advantage="None" conceptID="Res488" novelty="None" type="Res"/><text>As the main state machine is logically independent of the lower layers (Figs.</text></s>
<s sid="946"><CoreSc1 advantage="None" conceptID="Res489" novelty="None" type="Res"/><text>9 and 10) after stabilization, any metastability in these layers is fully masked.</text></s>
<s sid="947"><CoreSc1 advantage="None" conceptID="Res490" novelty="None" type="Res"/><text>Thus, after stabilization, metastability of the TSMs we care about can only occur due to unstable inputs, i.e., upsets in memory flags, that are in addition filtered through threshold gates (type (1) guards use local timeouts and are thus considered non-faulty, and all type (2) guards employed by the main state machine and the quick cycle use thresholds).</text></s>
<s sid="948"><CoreSc1 advantage="None" conceptID="Res491" novelty="None" type="Res"/><text>Note that due to the logical masking of metastability provided by the threshold gates (cf.</text></s>
<s sid="949"><CoreSc1 advantage="None" conceptID="Res492" novelty="None" type="Res"/><text>Section 6.2.3) any memory flag acts as an implicit synchronizer: If a faulty node successfully induces metastability in the flag, this does not matter until the threshold can actually be reached.</text></s>
<s sid="950"><CoreSc1 advantage="None" conceptID="Con155" novelty="None" type="Con"/><text>If the respective time span is large, the memory flag is likely to have stabilized again already.</text></s>
<s sid="951"><CoreSc1 advantage="None" conceptID="Con156" novelty="None" type="Con"/><text>Therefore, in addition to succeeding in creating metastability, faulty nodes must do so within a specific window of time.</text></s>
<s sid="952"><CoreSc1 advantage="None" conceptID="Con157" novelty="None" type="Con"/><text>Due to the asynchronously triggered transitions, this window of vulnerability of the synchronizing stage for Syn is very small.</text></s>
<s sid="953"><CoreSc1 advantage="None" conceptID="Con158" novelty="None" type="Con"/><text>The resulting very low probability of a metastable upset due to a fault is considered acceptable.</text></s>
<s sid="954"><CoreSc1 advantage="None" conceptID="Con159" novelty="None" type="Con"/><text>The residual probability of metastable upsets could be further reduced by introducing synchronizer stages.</text></s>
<s sid="955"><CoreSc1 advantage="None" conceptID="Res493" novelty="None" type="Res"/><text>Considering their performance penalty of one extra clock cycle on the one hand and the low initial risk of metastable upsets (that are handled by the system level fault tolerance with much lower average performance penalty) on the other hand, however, the introduction of synchronizers does not seem beneficial in general.</text></s>
Correctness.
<s sid="956"><CoreSc1 advantage="None" conceptID="Res494" novelty="None" type="Res"/><text>Thanks to the synchronous TSM described above, the maximum state transition time dTrans can easily be expressed in terms of the frequency of the pausable clock.</text></s>
<s sid="957"><CoreSc1 advantage="None" conceptID="Res495" novelty="None" type="Res"/><text>Hence, it is reasonably easy to see that the HSM satisfies the specification given in Section 4, when it starts from a proper initial state and avoids the above scenarios (a) and (b) of unstable guards.</text></s>
<s sid="958"><CoreSc1 advantage="None" conceptID="Res496" novelty="None" type="Res"/><text>A careful simulation analysis of the overall HSM design confirms that it can in fact recover from arbitrary initial states, except metastable ones.</text></s>
<s sid="959"><CoreSc1 advantage="None" conceptID="Res497" novelty="None" type="Res"/><text>With respect to metastable initial states, we conjecture that eventual recovery occurs with probability 1 due to the fact that the only devices used in the implementation that are not forgetful are flip-flops with dominant reset (in the TSM sequence counter) and Muller C-gates (in the control logic of the ASM), for both of which it is known that metastability eventually resolves.</text></s>
Clocks and watchdog timers
Pausable oscillator
<s sid="960"><CoreSc1 advantage="None" conceptID="Res498" novelty="None" type="Res"/><text>The TSM clock is an asynchronously startable and synchronously stoppable ring oscillator, which provides a clock signal TSMClock that is 0 when the clock is stopped via an active 1 input signal TSMCStop.</text></s>
<s sid="961"><CoreSc1 advantage="None" conceptID="Res499" novelty="None" type="Res"/><text>A variant that is also asynchronously stoppable (under certain timing constraints) is used for driving the watchdog timers (see below).</text></s>
<s sid="962"><CoreSc1 advantage="None" conceptID="Con160" novelty="None" type="Con"/><text>The frequency of the ring oscillator is primarily determined by the (odd) number of inverters in the feedback loop.2525</text></s>
<s sid="963"><CoreSc1 advantage="None" conceptID="Con161" novelty="None" type="Con"/><text>In our FPGA implementation, the oscillator frequency is so high that, to reduce the hardware overhead for this proof-of-concept implementation, we also employ a frequency divider at the output.</text></s>
<s sid="964"><CoreSc1 advantage="None" conceptID="Res500" novelty="None" type="Res"/><text>It varies heavily with the operating conditions, in particular with supply voltage and temperature: The resulting (two-sided) clock drift ξ is typically in the range of 7 to 9% for uncompensated ring oscillators like ours; in ASICs, it could be lowered down of 1 to 2% by special compensation techniques [15].</text></s>
<s sid="965"><CoreSc1 advantage="None" conceptID="Res501" novelty="None" type="Res"/><text>Note that the two-sided clock drifts map to ϑ=(1+ξ)/(1-ξ) bounds roughly between 1.15 and 1.19 or 1.02 and 1.04, respectively.</text></s>
Correctness.
<s sid="966"><CoreSc1 advantage="None" conceptID="Res502" novelty="None" type="Res"/><text>The operation of the TSM clock circuit shown in Fig. 13 is straightforward: In its initial state, TSMCStop=1 and the Muller C-gate has 1 at its output, so TSMClock=0.</text></s>
<s sid="967"><CoreSc1 advantage="None" conceptID="Res503" novelty="None" type="Res"/><text>Note that the circuit also stabilizes to the initial state if the Muller C-gate was erroneously initialized to 0, as the ring oscillator would eventually generate TSMClock=1, enforcing the correct initial value 1 of the C-gate.</text></s>
<s sid="968"><CoreSc1 advantage="None" conceptID="Obs33" novelty="None" type="Obs"/><text>When the ASM requests a state transition, at some arbitrary time when a transition guard becomes true, it just sets TSMCStop=LOW.</text></s>
<s sid="969"><CoreSc1 advantage="None" conceptID="Res504" novelty="None" type="Res"/><text>This starts the TSM clock and produces the first rising edge of TSMClock half a clock cycle time later.</text></s>
<s sid="970"><CoreSc1 advantage="None" conceptID="Res505" novelty="None" type="Res"/><text>As long as TSMCStop remains 0, the ring oscillator runs freely.</text></s>
<s sid="971"><CoreSc1 advantage="None" conceptID="Res506" novelty="None" type="Res"/><text>The stopping of TSMClock is regularly initiated by the TSM itself: With the rising edge of TSMClock that moves the TSM into Trm, TSMCStop is set to 1.</text></s>
<s sid="972"><CoreSc1 advantage="None" conceptID="Res507" novelty="None" type="Res"/><text>Since TSMClock is also 1 after the rising edge,2626</text></s>
<s sid="973"><CoreSc1 advantage="None" conceptID="Con162" novelty="None" type="Con"/><text>Obviously, we only have to take care in the timing analysis that setting TSMCStop=1 occurs well within the first half period.</text></s>
<s sid="974"><CoreSc1 advantage="None" conceptID="Con163" novelty="None" type="Con"/><text>the output of the C-gate is forced to 1 as well.</text></s>
<s sid="975"><CoreSc1 advantage="None" conceptID="Con164" novelty="None" type="Con"/><text>Hence, after having finished the half period of this final clock cycle, the feedback loop is frozen and TSMClock remains 0.</text></s>
Metastability.
<s sid="976"><CoreSc1 advantage="None" conceptID="Con165" novelty="None" type="Con"/><text>The problem of devising a proof that the pausable clock will eventually recover when it starts from a metastable initial state is intricate (and outside the scope of this paper); this is not obvious due to the quite complex feedback loop involved in this circuit.</text></s>
<s sid="977"><CoreSc1 advantage="None" conceptID="Con166" novelty="None" type="Con"/><text>We conjecture that similar arguments as in [44] can be used to show that this will happen with probability 1; with this result established one could hope to infer that the HSM as a whole recovers from arbitrary metastable states with probability 1.</text></s>
<s sid="978"><CoreSc1 advantage="None" conceptID="Res508" novelty="None" type="Res"/><text>For metastability-free operation of the C-gate in Fig. 13, (a) the falling transition of TSMCStop must not occur simultaneously with a rising edge of TSMClock, and (b) the rising transition of TSMCStop must not occur simultaneously with the falling edge of TSMClock.</text></s>
<s sid="979"><CoreSc1 advantage="None" conceptID="Res509" novelty="None" type="Res"/><text>(a) is guaranteed by stopping the clock in state Trm of the TSM, since the output of the C-gate is permanently forced to 1 on this occasion; TSMClock cannot hence generate a rising transition before TSMCStop goes to 0 again.</text></s>
<s sid="980"><CoreSc1 advantage="None" conceptID="Res510" novelty="None" type="Res"/><text>Whereas this synchronous stopping normally also ensures (b), we cannot always rule out the possibility of getting TSMCStop=1 close to the first rising edge of TSMClock: (b) could thus occur due to prematurely disabled type (3) guards, which we discussed already with respect to their potential to create metastability in the TSM, recall Section 6.3.</text></s>
<s sid="981"><CoreSc1 advantage="None" conceptID="Res511" novelty="None" type="Res"/><text>Besides being a rare event, this can only do harm during stabilization, however.</text></s>
Watchdog timer design
<s sid="982"><CoreSc1 advantage="None" conceptID="Res512" novelty="None" type="Res"/><text>Every ASM state, except for accept in Fig. 8, is associated with at most one watchdog timer required for type (1) guards; accept is associated with three timers (for T1 and T2 as well as for T2+ in Fig. 6).</text></s>
<s sid="983"><CoreSc1 advantage="None" conceptID="Res513" novelty="None" type="Res"/><text>Recall that a timer is reset by the TSM when its associated state is entered, which does not necessarily happen synchronously with its counting clock.</text></s>
<s sid="984"><CoreSc1 advantage="None" conceptID="Res514" novelty="None" type="Res"/><text>According to Fig. 14, every watchdog timer consists of a synchronous, dominantly resettable up-counter that is clocked by its own pausable oscillator (as shown in Fig. 13) and a timeout register that holds the timeout value TO.2727</text></s>
<s sid="985"><CoreSc1 advantage="None" conceptID="Con167" novelty="None" type="Con"/><text>Note that these values must be hard-coded in order to avoid that a fault that intuitively should be transient (e.g. a bit flip in volatile memory) becomes permanent by &quot;altering the algorithm&quot;.</text></s>
<s sid="986"><CoreSc1 advantage="None" conceptID="Res515" novelty="None" type="Res"/><text>A comparator raises an output signal if the counter value is equal to the TO register value.</text></s>
<s sid="987"><CoreSc1 advantage="None" conceptID="Res516" novelty="None" type="Res"/><text>A &quot;capture flip-flop&quot; with dominant reset memorizes the expired condition until the timer is re-triggered.</text></s>
<s sid="988"><CoreSc1 advantage="None" conceptID="Res517" novelty="None" type="Res"/><text>Note that using a (synchronous) flip-flop instead of an SR latch here allows us to completely mask glitches at the comparator output, which may originate from intermediate inconsistent bit patterns at the counter output.</text></s>
<s sid="989"><CoreSc1 advantage="None" conceptID="Res518" novelty="None" type="Res"/><text>The reset signal TSMresWD, supplied by the TSM, (re-)triggers the watchdog as follows: The counter is reset to zero, the capture flip-flop is cleared, and the oscillator is temporarily stopped.</text></s>
<s sid="990"><CoreSc1 advantage="None" conceptID="Res519" novelty="None" type="Res"/><text>Stopping the oscillator is necessary to avoid metastability effects due to the unsynchronized release of the reset signal (recall that this signal originates from the clock domain of the TSM!) and the watchdog's local oscillator.</text></s>
<s sid="991"><CoreSc1 advantage="None" conceptID="Res520" novelty="None" type="Res"/><text>Note carefully, however, that the Muller C-gate in Fig. 13 must be extended by a dominant reset input connected to its stop input (TSMCStop) to prevent metastable upsets.</text></s>
<s sid="992"><CoreSc1 advantage="None" conceptID="Res521" novelty="None" type="Res"/><text>Moreover, to ensure a proper reset, one has to make sure that the reset duration is sufficiently large.</text></s>
<s sid="993"><CoreSc1 advantage="None" conceptID="Res522" novelty="None" type="Res"/><text>To guarantee this, TSMresWD is fed into a pulse shaping circuitry (bottom left part of Fig. 14) that makes sure that the reset pulse is longer than one period of the local clock.2828</text></s>
<s sid="994"><CoreSc1 advantage="None" conceptID="Res523" novelty="None" type="Res"/><text>This is why seven inverters are shown in Fig. 14, indicating that the output pulse needs to remain active for more than two half periods of the local clock, assuming three inverters in its oscillator's loop.</text></s>
<s sid="995"><CoreSc1 advantage="None" conceptID="Res524" novelty="None" type="Res"/><text>At the end of this shaped reset pulse, counter and flip-flop have attained a clean reset state, and the local oscillator has safely been brought to a stable stopped state (with its output at 0).</text></s>
<s sid="996"><CoreSc1 advantage="None" conceptID="Res525" novelty="None" type="Res"/><text>When reset is finally released (to 0), the oscillator starts running.</text></s>
<s sid="997"><CoreSc1 advantage="None" conceptID="Res526" novelty="None" type="Res"/><text>As soon as the comparator detects a match between the current count and the timeout register, it will set match to 1.</text></s>
<s sid="998"><CoreSc1 advantage="None" conceptID="Res527" novelty="None" type="Res"/><text>This rising edge is captured by the flip-flop, thus keeping the watchdog timeout signal WDexpired at 1 even when the comparator reverts its output to 0 later on again (note that the counter keeps on running).</text></s>
<s sid="999"><CoreSc1 advantage="None" conceptID="Con168" novelty="None" type="Con"/><text>This construction ensures that the oscillator continues to operate also after the timeout expires, which is crucial for self-stabilization; in a system where the clocks driving the timeouts can be permanently halted, there is no way to avoid deadlocks for all possible states.</text></s>
<s sid="1000"><CoreSc1 advantage="None" conceptID="Res528" novelty="None" type="Res"/><text>As for the watchdog timer with random timeout R3 in Fig. 10, our implementation uses a linear feedback shift register (LFSR) that is continuously clocked by the watchdog's oscillator: A uniformly distributed random value from the specified range, sampled from the LFSR, is loaded into the timeout register whenever the watchdog timer is re-triggered.2929</text></s>
<s sid="1001"><CoreSc1 advantage="None" conceptID="Con169" novelty="None" type="Con"/><text>Note that for many settings, it is reasonable to assume that the new random value remains a secret until the timeout expires, as it is not read or in any other way considered by the node until then.</text></s>
<s sid="1002"><CoreSc1 advantage="None" conceptID="Con170" novelty="None" type="Con"/><text>Under this condition, FATAL+ is resilient against the strong adversary specified in [13].</text></s>
<s sid="1003"><CoreSc1 advantage="None" conceptID="Con171" novelty="None" type="Con"/><text>As our prototype implementation is not meant for studying security issues, however, the simple LFSR implementation is sufficient here.</text></s>
<s sid="1004"><CoreSc1 advantage="None" conceptID="Con172" novelty="None" type="Con"/><text>If both the watchdog timer and the LFSR are clocked by the same oscillator, this can be done in a synchronous way.</text></s>
<s sid="1005"><CoreSc1 advantage="None" conceptID="Res529" novelty="None" type="Res"/><text>In order to avoid metastable upsets of the LFSR, which might occur when stopping the clock upon retriggering the watchdog as described above, we use a standard pausable oscillator for R3: Since R3 is guaranteed to timeout before it is re-triggered (see Fig. 10), we can stop the oscillator synchronously (as in the TSM) when the timeout occurs, i.e., tie its stop input to the OR of WDexpired and the pulse-shaped reset signal.</text></s>
<s sid="1006"><CoreSc1 advantage="None" conceptID="Con173" novelty="None" type="Con"/><text>Another add-on is needed for the random timer R3 in order to guarantee that the LFSR recovers from an arbitrary state after a fault: Since an LFSR has a forbidden internal state (all-0 in our case), we use an additional comparator that detects an all-0 LFSR output and forces a (synchronized) reset of the LFSR to a proper initial state.</text></s>
Metastability.
<s sid="1007"><CoreSc1 advantage="None" conceptID="Con174" novelty="None" type="Con"/><text>Using a dominant reset in conjunction with a reset pulse of sufficient length guarantees that pausable oscillator, counter, and flip-flop cannot become metastable when a watchdog timer is re-triggered.</text></s>
<s sid="1008"><CoreSc1 advantage="None" conceptID="Con175" novelty="None" type="Con"/><text>Since all other activities are driven by the local oscillator and hence trivially metastability-free, this leaves the pulse-shaping unit as the only component that could possibly suffer from a metastable upset.</text></s>
<s sid="1009"><CoreSc1 advantage="None" conceptID="Con176" novelty="None" type="Con"/><text>However, TSMresWD is generated by the TSM, which is guaranteed to generate a clean pulse at every correct node.</text></s>
<s sid="1010"><CoreSc1 advantage="None" conceptID="Con177" novelty="None" type="Con"/><text>Hence, the pulse shaping unit could become metastable only at a faulty node.</text></s>
<s sid="1011"><CoreSc1 advantage="None" conceptID="Con178" novelty="None" type="Con"/><text>With respect to the recovery from a metastable state, similar considerations as for the memory flags in Section 6.2.2 suggest that the pulse shaping unit will stabilize to an initial state with reset set to 0 eventually with probability 1.</text></s>
Correctness.
<s sid="1012"><CoreSc1 advantage="None" conceptID="Con179" novelty="None" type="Con"/><text>Combining the implementations of the pausable oscillator (with additional reset) and the watchdog timer, it is not too difficult to verify that the specification given in Section 4 is met, provided all circuits start from a non-metastable initial state.</text></s>
<s sid="1013"><CoreSc1 advantage="None" conceptID="Con180" novelty="None" type="Con"/><text>With respect to self-stabilization, the pulse shaping unit can be guaranteed to stabilize to an initial state with its reset output 0 from an arbitrary internal state.</text></s>
<s sid="1014"><CoreSc1 advantage="None" conceptID="Con181" novelty="None" type="Con"/><text>Hence, the pausable oscillator and hence the counter will eventually run.</text></s>
<s sid="1015"><CoreSc1 advantage="None" conceptID="Con182" novelty="None" type="Con"/><text>Provided that the counter implementation guarantees that it cycles through the full (finite) sequence of possible states (unless reset earlier), i.e., there are no deadlock states or alternative cyclic sequences that might be entered in case of a fault, our implementation ensures that WDexpired will eventually be set to 1, even if started from an arbitrary initial state.</text></s>
<s sid="1016"><CoreSc1 advantage="None" conceptID="Con183" novelty="None" type="Con"/><text>One should bear in mind, though, that the time to recover a watchdog timer contributes to the overall stabilization time of the system.</text></s>
<s sid="1017"><CoreSc1 advantage="None" conceptID="Res530" novelty="None" type="Res"/><text>It is hence advisable to make sure that recovering a watchdog timer does not take much longer than the largest timeout value in the system, e.g.</text></s>
<s sid="1018"><CoreSc1 advantage="None" conceptID="Res531" novelty="None" type="Res"/><text>by avoiding oversized counter registers.</text></s>
<s sid="1019"><CoreSc1 advantage="None" conceptID="Res532" novelty="None" type="Res"/><text>Computing the end-to-end delay bounds</text></s>
<s sid="1020"><CoreSc1 advantage="None" conceptID="Res533" novelty="None" type="Res"/><text>From the implementations of the individual components, it is straightforward to compute the delays d, dmin+, and dmax+.</text></s>
<s sid="1021"><CoreSc1 advantage="None" conceptID="Res534" novelty="None" type="Res"/><text>Recall that d bounds, for any node i, the maximal time that passes between a state transition of a remote node and a possibly triggered corresponding state change, i.e., the transition of Si.</text></s>
<s sid="1022"><CoreSc1 advantage="None" conceptID="Res535" novelty="None" type="Res"/><text>This is done by computing the maximal sum of delays of any possible computing path, ranging over all possible state transitions (cf.</text></s>
<s sid="1023"><CoreSc1 advantage="None" conceptID="Res536" novelty="None" type="Res"/><text>Fig. 5), taking into account the delay of the channels Sj,i.</text></s>
<s sid="1024"><CoreSc1 advantage="None" conceptID="Res537" novelty="None" type="Res"/><text>Clearly, the channel delay for the remote channels Sj,i exceeds the delays of the local channels; hence, the longest path to the input ports of the state transition module is bounded by dChan+dMem+dTh&gt;dTime.</text></s>
<s sid="1025"><CoreSc1 advantage="None" conceptID="Res538" novelty="None" type="Res"/><text>Subsequently, the HSM locks the state transition and the TSM executes, which takes about two and a half clock cycles C of the pausable oscillator.</text></s>
<s sid="1026"><CoreSc1 advantage="None" conceptID="Res539" novelty="None" type="Res"/><text>Note, however, that the new state is written into the flip-flops holding the state already during the commit cycle, i.e., after at most 1.5C.</text></s>
<s sid="1027"><CoreSc1 advantage="None" conceptID="Res540" novelty="None" type="Res"/><text>A more accurate bound on d than dChan+dMem+dTh+2.5C is thusd⩽1.5C+max{C,dChan+dMem+dTh}.</text></s>
<s sid="1028"><CoreSc1 advantage="None" conceptID="Con184" novelty="None" type="Con"/><text>For our approach, dmax+≈d, since the only difference to d is that dChan is replaced by dChan+, the delay of the simpler 1-bit channels (cf.</text></s>
Section 6.2.1).
<s sid="1029"><CoreSc1 advantage="None" conceptID="Con185" novelty="None" type="Con"/><text>If the main state machine's channels would utilize serial encoding, though, one might well have that dChan≫max{dChan+,C}.</text></s>
<s sid="1030"><CoreSc1 advantage="None" conceptID="Con186" novelty="None" type="Con"/><text>Finally, dmin+&gt;C/ϑ, since this is the minimal time the HSM allows between locking a state transition and actually performing the transition at the port Si.3030</text></s>
<s sid="1031"><CoreSc1 advantage="None" conceptID="Con187" novelty="None" type="Con"/><text>Clearly, a more precise analysis would yield tighter bounds.</text></s>
Experimental evaluation
<s sid="1032"><CoreSc1 advantage="None" conceptID="Con188" novelty="None" type="Con"/><text>Our prototype implementation has been written in VHDL and compiled for an Altera Cyclone IV FPGA using the Quartus tool, see [47].</text></s>
<s sid="1033"><CoreSc1 advantage="None" conceptID="Con189" novelty="None" type="Con"/><text>Since FPGAs neither natively provide the basic elements required for asynchronous designs nor allow the designer to exercise control over the actual mapping of functions to the available LUTs (we implemented threshold modules via LUTs rather than via combinational AND-OR networks for complexity reasons), we had to make sure that properties that hold naturally in &quot;real&quot; asynchronous implementations also hold here.</text></s>
<s sid="1034"><CoreSc1 advantage="None" conceptID="Res541" novelty="None" type="Res"/><text>Apart from standard functional and timing verification via Modelsim, we therefore conducted some preliminary experiments for verifying the assumed properties (glitch-freeness, monotonicity, etc.) of the synthesized implementations of our core building blocks.</text></s>
<s sid="1035"><CoreSc1 advantage="None" conceptID="Res542" novelty="None" type="Res"/><text>Backed up by the (positive) results of these experiments, complete systems consisting of n=4 respectively n=8 nodes (tolerating at most f=1 respectively f=2 Byzantine faulty nodes) have been built and verified to work as expected; overall, they consume 23000 respectively 55000 logic blocks.</text></s>
<s sid="1036"><CoreSc1 advantage="None" conceptID="Res543" novelty="None" type="Res"/><text>Note however, that both designs also include the test environment, which makes up a significant part of the setup.</text></s>
<s sid="1037"><CoreSc1 advantage="None" conceptID="Obj72" novelty="None" type="Obj"/><text>To facilitate systematic experiments, we developed a custom test bench that provides the following functionality:(1)</text></s>
<s sid="1038"><CoreSc1 advantage="None" conceptID="Obs34" novelty="None" type="Obs"/><text>Measurement of pulse frequency and skew at different nodes.</text></s>
(2)
<s sid="1039"><CoreSc1 advantage="None" conceptID="Obs35" novelty="None" type="Obs"/><text>Continuous monitoring of the potential for generating metastability in HSM state transitions.</text></s>
(3)
<s sid="1040"><CoreSc1 advantage="None" conceptID="Met54" novelty="None" type="Met"/><text>Starting the entire system from an arbitrary state (including memory flags and timers), either specified deterministically or chosen at random.</text></s>
(4)
<s sid="1041"><CoreSc1 advantage="None" conceptID="Res544" novelty="None" type="Res"/><text>Resetting a single node to some initial state, at arbitrary times.</text></s>
(5)
<s sid="1042"><CoreSc1 advantage="None" conceptID="Res545" novelty="None" type="Res"/><text>Varying the clock frequency of any oscillator, at arbitrary times.</text></s>
(6)
<s sid="1043"><CoreSc1 advantage="None" conceptID="Res546" novelty="None" type="Res"/><text>Choosing the communication delay between each pair of sender and receiver.</text></s>
<s sid="1044"><CoreSc1 advantage="None" conceptID="Res547" novelty="None" type="Res"/><text>All these experiments can be performed with Byzantine nodes.</text></s>
<s sid="1045"><CoreSc1 advantage="None" conceptID="Res548" novelty="None" type="Res"/><text>To this end, the HSMs of the Byzantine nodes can be replaced by special devices that allow to (possibly inconsistently) communicate, via the communication data buses, any HSM state to any receiver HSM at any time.</text></s>
<s sid="1046"><CoreSc1 advantage="None" conceptID="Res549" novelty="None" type="Res"/><text>Points (1) to (6) are achieved as follows:</text></s>
<s sid="1047"><CoreSc1 advantage="None" conceptID="Res550" novelty="None" type="Res"/><text>(1) is accomplished using standard measurement equipment (logic analyzer, oscilloscope, frequency counter) attached to the appropriate signals routed via output pins.</text></s>
<s sid="1048"><CoreSc1 advantage="None" conceptID="Res551" novelty="None" type="Res"/><text>(2) is implemented by memorizing any event where more than one guard is enabled (at the time when the TSM locks a state transition) in a flag that can be externally monitored.</text></s>
<s sid="1049"><CoreSc1 advantage="None" conceptID="Res552" novelty="None" type="Res"/><text>(3) is realized by adding a scan-chain to the FPGA implementation, which allows us to serially shift-in arbitrary initial system states at run-time.</text></s>
<s sid="1050"><CoreSc1 advantage="None" conceptID="Res553" novelty="None" type="Res"/><text>Repeated random experiments are controlled via a Python script executed at a PC workstation, which is connected via USB to an ATMega 16 microcontroller (μC) that acts as a scan-controller towards the FPGA.</text></s>
<s sid="1051"><CoreSc1 advantage="None" conceptID="Res554" novelty="None" type="Res"/><text>For each experiment, the Python script generates a bit-stream representing an initial configuration.</text></s>
<s sid="1052"><CoreSc1 advantage="None" conceptID="Res555" novelty="None" type="Res"/><text>The μC takes this stream, sends it to the FPGA via the serial scan-chain interface, and finally signals the FPGA to start execution of FATAL+.</text></s>
<s sid="1053"><CoreSc1 advantage="None" conceptID="Res556" novelty="None" type="Res"/><text>Simultaneously, it starts a timer.</text></s>
<s sid="1054"><CoreSc1 advantage="None" conceptID="Res557" novelty="None" type="Res"/><text>When a timeout occurs or the FPGA signals completion of the experiment, the μC informs the Python script which records the time until completion together with the outcome of the experiment and proceeds with sending the next initial configuration.</text></s>
<s sid="1055"><CoreSc1 advantage="None" conceptID="Con190" novelty="None" type="Con"/><text>To enable (4) to (6), the testbench provides a global high-resolution clock that can be used for triggering mode changes.</text></s>
<s sid="1056"><CoreSc1 advantage="None" conceptID="Con191" novelty="None" type="Con"/><text>To ensure its synchrony w.r.t.</text></s>
<s sid="1057"><CoreSc1 advantage="None" conceptID="Con192" novelty="None" type="Con"/><text>the various nodes' clocks, we replaced all start/stoppable ring oscillators by start/stoppable oscillators that derive their output from the global clock signal.</text></s>
<s sid="1058"><CoreSc1 advantage="None" conceptID="Con193" novelty="None" type="Con"/><text>Point (4) is achieved by just forcing a node to reset to its initial state for this run at any time during the current execution.</text></s>
<s sid="1059"><CoreSc1 advantage="None" conceptID="Con194" novelty="None" type="Con"/><text>In order to facilitate (5), dividers combined with clock multipliers (PLLs) are used: For any oscillator, it is possible to choose one of five different frequencies (0, excessively slow, slow, fast, excessively fast) at any time.</text></s>
<s sid="1060"><CoreSc1 advantage="None" conceptID="Res558" novelty="None" type="Res"/><text>For (6), a variable delay line implemented as a synchronous shift register of length X∈[0,15], driven by the global clock, can be inserted in any data bus connecting different HSMs individually.</text></s>
<s sid="1061"><CoreSc1 advantage="None" conceptID="Res559" novelty="None" type="Res"/><text>In order to exercise also complex test scenarios in a reproducible way, a dedicated testbed execution state machine (TESM), driven by the global clock, is used to control the times and nodes when and where clock speeds, transmission delays, and communicated fault states are changed and when a single node is reset throughout an execution of the system.</text></s>
<s sid="1062"><CoreSc1 advantage="None" conceptID="Res560" novelty="None" type="Res"/><text>Transition guards may involve global time and any combinatorial expression of signals used in the implementation of FATAL+, i.e., any predicate on the current system state.</text></s>
<s sid="1063"><CoreSc1 advantage="None" conceptID="Res561" novelty="None" type="Res"/><text>Using our testbench, it was not too difficult to get our FATAL+ implementation up and running.</text></s>
<s sid="1064"><CoreSc1 advantage="None" conceptID="Res562" novelty="None" type="Res"/><text>With the implementation parameters ϑ=1.3, d=13T, dmin+=dmax+=3T, where T=400ns (2.5 MHz) is the experimental clock period, and minimal timeouts according to the constraints listed in [13] (cf.</text></s>
<s sid="1065"><CoreSc1 advantage="None" conceptID="Res563" novelty="None" type="Res"/><text>Section 5.7), pulses of an 8 node FATAL respectively FATAL+ system (including the quick cycle) occur at a frequency of about 62 Hz respectively 10 kHz.</text></s>
<s sid="1066"><CoreSc1 advantage="None" conceptID="Obs36" novelty="None" type="Obs"/><text>A logic analyzer screenshot is depicted in Fig. 15.</text></s>
<s sid="1067"><CoreSc1 advantage="None" conceptID="Obs37" novelty="None" type="Obs"/><text>Note that the quite low values for the frequency stem from the fact that we were intentionally slowing down the system, enabling better control of the execution.</text></s>
<s sid="1068"><CoreSc1 advantage="None" conceptID="Res564" novelty="None" type="Res"/><text>As to be expected from such a fairly complex setup, we spotted several hidden design errors that showed up during our experiments, but also some minor, yet problematic errors in our theoretical analysis (like a missing factor of ϑ in one of our timeouts due to a typo).</text></s>
<s sid="1069"><CoreSc1 advantage="None" conceptID="Res565" novelty="None" type="Res"/><text>In the original setup, these issues manifested in deviations of the measured w.r.t.</text></s>
the predicted performance.
<s sid="1070"><CoreSc1 advantage="None" conceptID="Res566" novelty="None" type="Res"/><text>After resolving them, we conducted the following experiments, observing the behavior of both the overall FATAL+ and the underlying FATAL pulse generation protocol.</text></s>
Worst-case skew experiment
<s sid="1071"><CoreSc1 advantage="None" conceptID="Res567" novelty="None" type="Res"/><text>To drive an 8-node FATAL system into a worst-case skew scenario,3131</text></s>
<s sid="1072"><CoreSc1 advantage="None" conceptID="Res568" novelty="None" type="Res"/><text>The maximal imprecision is meaningful in connection with the system's frequency only.</text></s>
<s sid="1073"><CoreSc1 advantage="None" conceptID="Res569" novelty="None" type="Res"/><text>In contrast, the skew captures the maximal time difference between corresponding signal transitions at the nodes, which reflects the quality of synchronization without requiring additional context.</text></s>
<s sid="1074"><CoreSc1 advantage="None" conceptID="Res570" novelty="None" type="Res"/><text>the set of nodes was split into four sets:•</text></s>
<s sid="1075"><CoreSc1 advantage="None" conceptID="Res571" novelty="None" type="Res"/><text>A set A of two nodes with slow clock sources.</text></s>
<s sid="1076"><CoreSc1 advantage="None" conceptID="Res572" novelty="None" type="Res"/><text>All communication to these nodes is maximally delayed.</text></s>
•
<s sid="1077"><CoreSc1 advantage="None" conceptID="Res573" novelty="None" type="Res"/><text>A set B of two nodes with fast clock sources.</text></s>
<s sid="1078"><CoreSc1 advantage="None" conceptID="Res574" novelty="None" type="Res"/><text>All communication to these nodes is minimally delayed.</text></s>
•
<s sid="1079"><CoreSc1 advantage="None" conceptID="Res575" novelty="None" type="Res"/><text>Another set C of two nodes with fast clock sources.</text></s>
<s sid="1080"><CoreSc1 advantage="None" conceptID="Res576" novelty="None" type="Res"/><text>All communication to these nodes is maximally delayed.</text></s>
•
<s sid="1081"><CoreSc1 advantage="None" conceptID="Res577" novelty="None" type="Res"/><text>A set D of two faulty nodes.</text></s>
<s sid="1082"><CoreSc1 advantage="None" conceptID="Con195" novelty="None" type="Con"/><text>These nodes always send propose to the nodes in B and do not send any other signals.3232</text></s>
<s sid="1083"><CoreSc1 advantage="None" conceptID="Con196" novelty="None" type="Con"/><text>In our model, this behavior is mapped to a default signal at the receivers, e.g.</text></s>
<s sid="1084"><CoreSc1 advantage="None" conceptID="Con197" novelty="None" type="Con"/><text>resync in the main state machine.</text></s>
<s sid="1085"><CoreSc1 advantage="None" conceptID="Con198" novelty="None" type="Con"/><text>This setup leads to the following behavior of the main state machine (Fig. 8) once the system is stabilized.</text></s>
<s sid="1086"><CoreSc1 advantage="None" conceptID="Res578" novelty="None" type="Res"/><text>The nodes in B∪C will always switch to propose first because their timeouts T3 expire (it is shown in [13] that at this time nexti=1 at all non-faulty nodes), and due to the &quot;help&quot; of the faulty nodes, the threshold of n-f=6 for switching to accept is reached at the nodes in B after the minimal delay.</text></s>
<s sid="1087"><CoreSc1 advantage="None" conceptID="Res579" novelty="None" type="Res"/><text>It takes the maximal delay until the nodes in A realize that 4⩾f+1 nodes reached state propose and switch to this state.</text></s>
<s sid="1088"><CoreSc1 advantage="None" conceptID="Res580" novelty="None" type="Res"/><text>Since 4&lt;n-f, the nodes in A∪C require the support of the nodes in A to follow to state accept.</text></s>
<s sid="1089"><CoreSc1 advantage="None" conceptID="Con199" novelty="None" type="Con"/><text>Hence, this happens another maximal delay later.</text></s>
<s sid="1090"><CoreSc1 advantage="None" conceptID="Con200" novelty="None" type="Con"/><text>The resulting scenario is depicted in Fig. 16.</text></s>
<s sid="1091"><CoreSc1 advantage="None" conceptID="Res581" novelty="None" type="Res"/><text>Assuming that the communication delay is at most d and at least dmin⩾0, we predict a skew of at most 2d-dmin between the nodes in B switching to accept and the nodes in A∪C catching up.</text></s>
<s sid="1092"><CoreSc1 advantage="None" conceptID="Res582" novelty="None" type="Res"/><text>The experimental results confirmed the analytic predictions as being essentially tight: Letting the fast nodes run at a speed of 3 MHz and the slow nodes at 2.5 MHz, and setting the maximum delay d to about 3.6 μs (9 clock cycles), we observed a skew of about 6 μs.</text></s>
<s sid="1093"><CoreSc1 advantage="None" conceptID="Con201" novelty="None" type="Con"/><text>This is consistent with the relatively large minimum delay dmin arising in our testbed.</text></s>
<s sid="1094"><CoreSc1 advantage="None" conceptID="Con202" novelty="None" type="Con"/><text>A logic analyzer screenshot is depicted in Fig. 17.</text></s>
<s sid="1095"><CoreSc1 advantage="None" conceptID="Con203" novelty="None" type="Con"/><text>The figure also demonstrates the capability of FATAL+ to generate pulses with significantly less skew (1 μs) on top of the FATAL pulses with worst-case skew.</text></s>
Metastability experiments
<s sid="1096"><CoreSc1 advantage="None" conceptID="Res583" novelty="None" type="Res"/><text>We run a series of experiments dedicated to finding situations that potentially lead to metastable upsets.</text></s>
<s sid="1097"><CoreSc1 advantage="None" conceptID="Res584" novelty="None" type="Res"/><text>We repeatedly set up 8-node systems with randomly chosen clock speeds between 2.5 MHz and 3.25 MHz and communication delays of at most 16 clock cycles.</text></s>
<s sid="1098"><CoreSc1 advantage="None" conceptID="Res585" novelty="None" type="Res"/><text>While the system stabilized from these random initial states, we monitored the nodes' HSM state transitions after stabilization for multiple active conflicting state transitions during a period of over 60 h in total.</text></s>
<s sid="1099"><CoreSc1 advantage="None" conceptID="Res586" novelty="None" type="Res"/><text>As predicted by our theoretical findings, in none of the experiments two conflicting guards were ever active at the same (global testbench) time after stabilization.</text></s>
Stabilization time experiments
<s sid="1100"><CoreSc1 advantage="None" conceptID="Res587" novelty="None" type="Res"/><text>We evaluated stabilization time both in the absence and in the presence of faulty nodes.</text></s>
<s sid="1101"><CoreSc1 advantage="None" conceptID="Con204" novelty="None" type="Con"/><text>In the latter case, we demonstrated the influence of the choice of the random timeout R3 on the stabilization time.</text></s>
<s sid="1102"><CoreSc1 advantage="None" conceptID="Con205" novelty="None" type="Con"/><text>Stabilization in the absence of faulty nodes</text></s>
<s sid="1103"><CoreSc1 advantage="None" conceptID="Res588" novelty="None" type="Res"/><text>To evaluate stabilization times in the absence of faulty nodes, we set up an 8 node system and run over 250000 experiments in each of which the nodes booted from random initial states, with randomly chosen clock speeds between 2.5 MHz and 3.25MHz=2.5ϑMHz, and message delays of up to d=16 clock cycles.</text></s>
<s sid="1104"><CoreSc1 advantage="None" conceptID="Res589" novelty="None" type="Res"/><text>As soon as all nodes switched to state accept within 2d time, the FPGA signaled the μC to record the elapsed time and start the next experiment.</text></s>
<s sid="1105"><CoreSc1 advantage="None" conceptID="Res590" novelty="None" type="Res"/><text>A considerable fraction of the scenarios (over 45%) stabilizes within less than 0.035 s (less than 5500d), which can be credited to the fast stabilization mechanism intended for individual nodes resynchronizing to a running system (see Fig. 18).</text></s>
<s sid="1106"><CoreSc1 advantage="None" conceptID="Res591" novelty="None" type="Res"/><text>The remaining runs (see Fig. 19; please mind the different y-axis scale) stabilize, supported by the resynchronization routine, in less than 12 s (about 1.9⋅106d), which is less than the system's upper bound on R3 of approximately 14.9 s (about 2.3⋅106d) and significantly less than the system's upper bound on Tslow given in Theorem 5.2, which is no more than 44.5 s (about 7⋅106d) in this scenario.</text></s>
<s sid="1107"><CoreSc1 advantage="None" conceptID="Obs38" novelty="None" type="Obs"/><text>Note that the stabilization time is inversely proportional to the frequency, i.e., in a system that is not artificially slowed down, stabilization is orders of magnitude faster.</text></s>
<s sid="1108"><CoreSc1 advantage="None" conceptID="Obs39" novelty="None" type="Obs"/><text>For example, assuming d=1ns, we obtain that over 45% of the experiments stabilize within 5.5 μs, and all experiments stabilize within 1.9 ms.</text></s>
<s sid="1109"><CoreSc1 advantage="None" conceptID="Obs40" novelty="None" type="Obs"/><text>Experimental results carried out for a 4-node system were analogous.</text></s>
<s sid="1110"><CoreSc1 advantage="None" conceptID="Met55" novelty="None" type="Met"/><text>Either the main algorithm was capable to stabilize by itself (as for a large fraction the experiments in the head of the distribution), or once the resynchronization algorithm provided support after R3 expired at some node and n-f nodes switched to resync in approximate synchrony (the experiments in the tail of the distribution).</text></s>
<s sid="1111"><CoreSc1 advantage="None" conceptID="Obs41" novelty="None" type="Obs"/><text>Fig. 20 shows stabilization by the resynchronization algorithm in a 4-node system: Eventually, all nodes switch to state none.</text></s>
<s sid="1112"><CoreSc1 advantage="None" conceptID="Res592" novelty="None" type="Res"/><text>A node whose timeout R3 expires at a time when all timeouts (R2,supp) are expired, say node 1, forces all nodes from none into supp1.</text></s>
<s sid="1113"><CoreSc1 advantage="None" conceptID="Con206" novelty="None" type="Con"/><text>Additional R3 timers, expiring at other nodes, may only force nodes into suppj, with j≠1, but do not prevent nodes from eventually communicating supp to all other nodes.</text></s>
<s sid="1114"><CoreSc1 advantage="None" conceptID="Con207" novelty="None" type="Con"/><text>Thus nodes finally switch to supp→resync and from there to resync, in synchrony.</text></s>
<s sid="1115"><CoreSc1 advantage="None" conceptID="Con208" novelty="None" type="Con"/><text>This again suffices to deterministically stabilize the nodes' main algorithm (as shown in [13]).</text></s>
<s sid="1116"><CoreSc1 advantage="None" conceptID="Con209" novelty="None" type="Con"/><text>Note that the condition that all corresponding R2 timeouts are expired when a timeout R3 expires (actually, n-f suffice) will eventually be satisfied.</text></s>
<s sid="1117"><CoreSc1 advantage="None" conceptID="Res593" novelty="None" type="Res"/><text>This happens at the latest when R3 expires for the second time at some node, simply because the distribution of the randomized timeout R3 guarantees that the picked duration is always larger than (roughly) ϑR2.</text></s>
<s sid="1118"><CoreSc1 advantage="None" conceptID="Con210" novelty="None" type="Con"/><text>Stabilization with Byzantine nodes and deterministic timeouts</text></s>
<s sid="1119"><CoreSc1 advantage="None" conceptID="Con211" novelty="None" type="Con"/><text>The importance of timeout R3 being randomly distributed is demonstrated in the following experiment.</text></s>
<s sid="1120"><CoreSc1 advantage="None" conceptID="Res594" novelty="None" type="Res"/><text>We set up a 4-node FATAL+ system with one Byzantine faulty node, say node 4, and chose all R3 to be equal and initially synchronized, i.e., all R3 timeouts expire at about the same time at all correct nodes.</text></s>
<s sid="1121"><CoreSc1 advantage="None" conceptID="Res595" novelty="None" type="Res"/><text>If the Byzantine node knows when R3 is going to expire, it can prohibit correct nodes from simultaneously switching to resync, thereby preventing synchronization of the Main Algorithm and thus stabilization: Shortly before R3 expires at the correct nodes, it sends init to two nodes, say 1 and 3, making them switch to supp4.</text></s>
<s sid="1122"><CoreSc1 advantage="None" conceptID="Res596" novelty="None" type="Res"/><text>Subsequently, however, it only supports node 1 by sending supp to it.</text></s>
<s sid="1123"><CoreSc1 advantage="None" conceptID="Res597" novelty="None" type="Res"/><text>This forces node 1 to switch to supp→resync and then resync alone.</text></s>
<s sid="1124"><CoreSc1 advantage="None" conceptID="Res598" novelty="None" type="Res"/><text>While node 1 is in resync (i.e., while R1 is running), it does not support other nodes by sending supp.</text></s>
<s sid="1125"><CoreSc1 advantage="None" conceptID="Res599" novelty="None" type="Res"/><text>Specifically, it does not support nodes 2 and 3 when they switch to supp1.</text></s>
<s sid="1126"><CoreSc1 advantage="None" conceptID="Res600" novelty="None" type="Res"/><text>Eventually all nodes switch back to none, and the scenario can be repeated.</text></s>
<s sid="1127"><CoreSc1 advantage="None" conceptID="Obs42" novelty="None" type="Obs"/><text>Fig. 21 depicts the scenario and Fig. 22 shows a logic analyzer screenshot of this experiment.</text></s>
<s sid="1128"><CoreSc1 advantage="None" conceptID="Res601" novelty="None" type="Res"/><text>Note, however, that by definition of the probability distribution of R3, executions where R3 expires in synchrony at all correct nodes forever occur with probability 0.</text></s>
<s sid="1129"><CoreSc1 advantage="None" conceptID="Con212" novelty="None" type="Con"/><text>We remark that there is always a nonzero probability that the randomly chosen durations of the timeouts R3 at non-faulty nodes align in a fortunate manner, so that stabilization could not be prevented even by an omniscient adversary orchestrating clock drifts, message delays, and faulty nodes.</text></s>
<s sid="1130"><CoreSc1 advantage="None" conceptID="Con213" novelty="None" type="Con"/><text>While the probability of such a convenient event occurring in O(n) time decreases exponentially in the number of nodes n, it is reasonably likely for n=7 and in particular n=4 (i.e., systems that tolerate f=2 or f=1 faults, respectively).</text></s>
<s sid="1131"><CoreSc1 advantage="None" conceptID="Con214" novelty="None" type="Con"/><text>This observation has been verified for n=4 by the first of the two experiments below.</text></s>
<s sid="1132"><CoreSc1 advantage="None" conceptID="Con215" novelty="None" type="Con"/><text>Stabilization with Byzantine nodes and probabilistic timeouts</text></s>
<s sid="1133"><CoreSc1 advantage="None" conceptID="Obj73" novelty="None" type="Obj"/><text>Two experimental setups were chosen to test stabilization in the presence of Byzantine nodes, using probabilistic timeouts R3 for correct nodes.</text></s>
<s sid="1134"><CoreSc1 advantage="None" conceptID="Obs43" novelty="None" type="Obs"/><text>In the first experiment, a Byzantine node has access to the timeout values of all nodes as soon as they start their R3 timers.</text></s>
<s sid="1135"><CoreSc1 advantage="None" conceptID="Con216" novelty="None" type="Con"/><text>In this case, the Byzantine node followed the strategy from before, obstructing any stabilization attempt that would otherwise be successful.</text></s>
<s sid="1136"><CoreSc1 advantage="None" conceptID="Res602" novelty="None" type="Res"/><text>We observed that the Byzantine node was able to block at most one stabilization attempt of each non-faulty node.</text></s>
<s sid="1137"><CoreSc1 advantage="None" conceptID="Res603" novelty="None" type="Res"/><text>Then it failed to prevent stabilization because the R2 timeouts corresponding to the Byzantine node did not expire on time before some non-faulty node successfully initialized stabilization.</text></s>
<s sid="1138"><CoreSc1 advantage="None" conceptID="Res604" novelty="None" type="Res"/><text>In the second experiment, the Byzantine node has no access to timeout values, and therefore simply sends inconsistent init and supp signals as often as allowed by the timeouts R2 corresponding to it.</text></s>
<s sid="1139"><CoreSc1 advantage="None" conceptID="Res605" novelty="None" type="Res"/><text>We did not observe any inhibited synchronized switch to resync when R3 expired at a correct node, however.</text></s>
<s sid="1140"><CoreSc1 advantage="None" conceptID="Res606" novelty="None" type="Res"/><text>It should be noted that weaker adversaries and &quot;better&quot; initial configurations result in a constant stabilization time, irrespectively of the number of nodes n (see [13] for details).</text></s>
<s sid="1141"><CoreSc1 advantage="None" conceptID="Res607" novelty="None" type="Res"/><text>The second experiment above demonstrates such a case; Theorem 5.3 states another.</text></s>
<s sid="1142"><CoreSc1 advantage="None" conceptID="Res608" novelty="None" type="Res"/><text>The common-case stabilization time will therefore be considerably smaller than the (probabilistic) worst-case bound that is linear in n.</text></s>
Conclusions
<s sid="1143"><CoreSc1 advantage="None" conceptID="Con217" novelty="None" type="Con"/><text>In this work, we introduced a novel modeling framework for self-stabilizing, fault-tolerant asynchronous digital circuits and demonstrated its applicability to our recently introduced FATAL+ clock generation scheme for multi-synchronous GALS architectures.</text></s>
<s sid="1144"><CoreSc1 advantage="None" conceptID="Res609" novelty="None" type="Res"/><text>Our framework enables to reason about high-level properties of the system based on the behavior of basic building blocks, at arbitrary granularity and in a seamless manner.</text></s>
<s sid="1145"><CoreSc1 advantage="None" conceptID="Con218" novelty="None" type="Con"/><text>At the same time, the hierarchical structure of the model permits to do this in a fashion amenable to formal analysis.</text></s>
<s sid="1146"><CoreSc1 advantage="None" conceptID="Con219" novelty="None" type="Con"/><text>We believe this to be the first approach concurrently providing all these features, and therefore consider it as a promising foundation for future research in the area of fault-tolerant digital circuits.</text></s>
<s sid="1147"><CoreSc1 advantage="None" conceptID="Goa24" novelty="None" type="Goa"/><text>As the conclusion of our paper, we now assess to which extent the properties of our implementation of the FATAL+ algorithm, which have been expressed and verified within our modeling framework and tested experimentally, meet our design goals.</text></s>
<s sid="1148"><CoreSc1 advantage="None" conceptID="Goa25" novelty="None" type="Goa"/><text>Furthermore, we will discuss a number of potential improvements and future research avenues.</text></s>
<s sid="1149"><CoreSc1 advantage="None" conceptID="Res610" novelty="None" type="Res"/><text>Our exposition will follow the optimization criteria listed in Section 2.1.7.•</text></s>
<s sid="1150"><CoreSc1 advantage="None" conceptID="Res611" novelty="None" type="Res"/><text>Area consumption: For a suitable implementation, the total number of gates is O(nlogn) per node.</text></s>
<s sid="1151"><CoreSc1 advantage="None" conceptID="Res612" novelty="None" type="Res"/><text>This can be seen by observing that the complexity of the threshold gates is dominating the asymptotic number of gates, since the O(n) remaining components of a node have a constant number of gates each; using sorting networks to implement threshold gates, the stated complexity bound follows [48].</text></s>
<s sid="1152"><CoreSc1 advantage="None" conceptID="Res613" novelty="None" type="Res"/><text>Trivially, this number of gates is a factor of O(logn) from optimal.</text></s>
<s sid="1153"><CoreSc1 advantage="None" conceptID="Con220" novelty="None" type="Con"/><text>We conjecture that in fact this complexity is asymptotically optimal, unless one is willing to sacrifice other desirable properties of the algorithm (e.g. optimal resilience).</text></s>
<s sid="1154"><CoreSc1 advantage="None" conceptID="Con221" novelty="None" type="Con"/><text>Assuming that the gate complexity of the nodes adequately represents the area consumption of our algorithm, we conclude that our solution is satisfactory in that regard.</text></s>
•
<s sid="1155"><CoreSc1 advantage="None" conceptID="Con222" novelty="None" type="Con"/><text>Communication complexity: Our implementation uses 7 (1-bit) wires per channel, and sequential encoding of the states of the main state machine would reduce this number to 5.</text></s>
All communication are broadcasts.
<s sid="1156"><CoreSc1 advantage="None" conceptID="Con223" novelty="None" type="Con"/><text>Considering the complexity of the task, there seems to be very limited room for improvement.</text></s>
•
<s sid="1157"><CoreSc1 advantage="None" conceptID="Con224" novelty="None" type="Con"/><text>Stabilization time: Our algorithm has a stabilization time of O(n) in the worst case.</text></s>
<s sid="1158"><CoreSc1 advantage="None" conceptID="Con225" novelty="None" type="Con"/><text>Recent findings [49] show that a polylogarithmic stabilization time can be achieved at a low communication complexity; however, this comes at the expense of suboptimal resilience, a weaker adversarial model, and, most importantly, constants in the complexity bounds that make the resulting algorithm inferior to our solution for any practical range of parameters.</text></s>
<s sid="1159"><CoreSc1 advantage="None" conceptID="Con226" novelty="None" type="Con"/><text>Moreover, as formalized in [13] and demonstrated in Section 7, for a wide range of scenarios our algorithm achieves constant stabilization time.</text></s>
<s sid="1160"><CoreSc1 advantage="None" conceptID="Con227" novelty="None" type="Con"/><text>Considering the severe fault model, we conclude that despite not being optimal, our algorithm performs satisfactory with respect to this quality measure.</text></s>
•
<s sid="1161"><CoreSc1 advantage="None" conceptID="Con228" novelty="None" type="Con"/><text>Resilience: It is known that 3f+1 nodes are necessary to tolerate f faults [25,14] unless cryptographic tools are available.</text></s>
<s sid="1162"><CoreSc1 advantage="None" conceptID="Con229" novelty="None" type="Con"/><text>Since the complexity incurred by cryptographic tools is prohibitive in our setting, our algorithm features optimal resilience.</text></s>
•
<s sid="1163"><CoreSc1 advantage="None" conceptID="Res614" novelty="None" type="Res"/><text>Delays: As mentioned, the delay of wires is outside our control.</text></s>
<s sid="1164"><CoreSc1 advantage="None" conceptID="Obs44" novelty="None" type="Obs"/><text>Taking dmin+ and dmax+ into account in the quick cycle machine, we make best use of the available bounds in terms of the final frequency/synchrony trade-off.</text></s>
<s sid="1165"><CoreSc1 advantage="None" conceptID="Res615" novelty="None" type="Res"/><text>The delays incurred by the computations performed at nodes are proportional to the depths of the involved circuits.</text></s>
<s sid="1166"><CoreSc1 advantage="None" conceptID="Res616" novelty="None" type="Res"/><text>Again, the implementation of the threshold gates is the dominant cost factor here.</text></s>
<s sid="1167"><CoreSc1 advantage="None" conceptID="Res617" novelty="None" type="Res"/><text>The sorting network by Ajtai et al. [48] exhibits depth O(logn).</text></s>
<s sid="1168"><CoreSc1 advantage="None" conceptID="Con230" novelty="None" type="Con"/><text>Assuming constant fan-in of gates, this is clearly asymptotically optimal if the decision when to increase the logical clock Lv next indeed depends on all n-1 input signals of v from remote nodes.</text></s>
<s sid="1169"><CoreSc1 advantage="None" conceptID="Con231" novelty="None" type="Con"/><text>We conclude that, so far as within our control, the design goal of minimizing the incurred delays is met by our algorithm.</text></s>
•
<s sid="1170"><CoreSc1 advantage="None" conceptID="Con232" novelty="None" type="Con"/><text>Metastability: We discussed several effective measures to prevent metastability in Section 6.</text></s>
<s sid="1171"><CoreSc1 advantage="None" conceptID="Con233" novelty="None" type="Con"/><text>Our experiments support our theoretical finding that, after stabilization, metastability may not occur in absence of further faults.</text></s>
<s sid="1172"><CoreSc1 advantage="None" conceptID="Con234" novelty="None" type="Con"/><text>However, since metastability is an elusive problem for which it is difficult to transfer insights and observations to other modes of operation of a given system-let alone to different implementation technology-a mathematical treatment of metastability is highly desirable.</text></s>
<s sid="1173"><CoreSc1 advantage="None" conceptID="Con235" novelty="None" type="Con"/><text>Our model opens up various possible approaches to this issue.</text></s>
<s sid="1174"><CoreSc1 advantage="None" conceptID="Con236" novelty="None" type="Con"/><text>For one, it is feasible to switch to a more accurate description of signals in terms of signals' voltages as continuous functions of time.</text></s>
<s sid="1175"><CoreSc1 advantage="None" conceptID="Con237" novelty="None" type="Con"/><text>Another option choosing an intermediate level of complexity would be to add an additional signal state (e.g. ⊥) for &quot;invalid&quot; signals, representing e.g.</text></s>
creeping or oscillating signals.
<s sid="1176"><CoreSc1 advantage="None" conceptID="Con238" novelty="None" type="Con"/><text>Assigning appropriate probabilities of metastability propagation and decay to modules, this would enable a unified probabilistic analysis of metastability generation, propagation, and decay within a modeling framework using discrete state representations.</text></s>
<s sid="1177"><CoreSc1 advantage="None" conceptID="Con239" novelty="None" type="Con"/><text>Such an approach could yield entirely unconditional guarantees on system recovery; in contrast, our current description requires an a priori guarantee that metastability is sufficiently contained during the stabilization process.</text></s>
•
<s sid="1178"><CoreSc1 advantage="None" conceptID="Con240" novelty="None" type="Con"/><text>Connectivity: The algorithm presented in this work requires to connect all pairs of nodes and is therefore not scalable.</text></s>
<s sid="1179"><CoreSc1 advantage="None" conceptID="Con241" novelty="None" type="Con"/><text>Unfortunately, it is known that Ω(n2) links are required for tolerating f∈Ω(n) faults in the worst case [26,27].</text></s>
<s sid="1180"><CoreSc1 advantage="None" conceptID="Con242" novelty="None" type="Con"/><text>We argued for the assumption of worst-case behavior of faulty nodes; however, it appears reasonable that typical systems will not exhibit a worst-case distribution of faults within the system.</text></s>
<s sid="1181"><CoreSc1 advantage="None" conceptID="Con243" novelty="None" type="Con"/><text>Indeed, many interesting scenarios justify to assume a much more benign distribution of faults.</text></s>
<s sid="1182"><CoreSc1 advantage="None" conceptID="Con244" novelty="None" type="Con"/><text>In the extreme case where faults are distributed uniformly and independently at random with a constant probability, say, 10%, of a node being faulty, node degrees of Δ∈O(clogn) would suffice to guarantee (at a given point in time) that the probability that more than Δ/9 neighbors of any node are faulty, is at most 1-1/nc.</text></s>
<s sid="1183"><CoreSc1 advantage="None" conceptID="Con245" novelty="None" type="Con"/><text>Note that this implies that the mean time until this property is violated polynomially grows with system size.</text></s>
<s sid="1184"><CoreSc1 advantage="None" conceptID="Con246" novelty="None" type="Con"/><text>Using the FATAL+ protocol in small subsystems (of less than Δ nodes), system-wide synchronization will be much easier to achieve than if one would start from scratch.</text></s>
<s sid="1185"><CoreSc1 advantage="None" conceptID="Con247" novelty="None" type="Con"/><text>In this setting, Δ∈O(logn) would replace n in all complexity bounds of the FATAL+ algorithm, resulting in particular in gate complexity O(lognloglogn) per node, computational delay O(loglogn), and stabilization time O(clogn) with probability 1-1/nc.</text></s>
<s sid="1186"><CoreSc1 advantage="None" conceptID="Con248" novelty="None" type="Con"/><text>Thus, this approach promises &quot;local&quot; fault-tolerance of Ω(Δ) faults in each neighborhood in combination with excellent scalability in all complexity measures, and realizing this is a major goal of our future work.</text></s>
•
<s sid="1187"><CoreSc1 advantage="None" conceptID="Con249" novelty="None" type="Con"/><text>Clock size: The constraint (1) entails that either clock size is bounded or large clocks result in larger stabilization time.</text></s>
<s sid="1188"><CoreSc1 advantage="None" conceptID="Con250" novelty="None" type="Con"/><text>This restriction can be overcome if we use the clocks of bounded size generated by FATAL+ as input to another layer that runs a synchronous consensus algorithm in order to agree on exponentially larger clocks [41].</text></s>
<s sid="1189"><CoreSc1 advantage="None" conceptID="Con251" novelty="None" type="Con"/><text>Finally, we would like to mention two more prospective extensions of our work.</text></s>
<s sid="1190"><CoreSc1 advantage="None" conceptID="Con252" novelty="None" type="Con"/><text>First, building on our modeling framework, it seems feasible to tackle an even more strict verification of the algorithm's properties than &quot;standard&quot; mathematical analysis.</text></s>
<s sid="1191"><CoreSc1 advantage="None" conceptID="Con253" novelty="None" type="Con"/><text>The hierarchical structure and formal specifications of modules seem amenable to formal verification methods.</text></s>
<s sid="1192"><CoreSc1 advantage="None" conceptID="Con254" novelty="None" type="Con"/><text>Such an approach should benefit from the possibilities to adjust the granularity of the model by the distinction between basic and compound modules as well as the restrictions imposed by the module specifications; more restrictive modules may be simpler to analyze, yet will guarantee the same properties as the stated variants.</text></s>
<s sid="1193"><CoreSc1 advantage="None" conceptID="Con255" novelty="None" type="Con"/><text>Second, it should be noted that it is straightforward to derive clocks of even higher frequency from the FATAL+ clocks.</text></s>
<s sid="1194"><CoreSc1 advantage="None" conceptID="Con256" novelty="None" type="Con"/><text>This is essentially done by frequency multiplication, at the expense of increasing the clock skew.</text></s>
<s sid="1195"><CoreSc1 advantage="None" conceptID="Con257" novelty="None" type="Con"/><text>We refer to Dolev et al. [13] for details.</text></s>
<s sid="1196"><CoreSc1 advantage="None" conceptID="Con258" novelty="None" type="Con"/><text>Overall, we consider the present work an important step towards a practical, ultra-robust clocking scheme for SoC.</text></s>
<s sid="1197"><CoreSc1 advantage="None" conceptID="Con259" novelty="None" type="Con"/><text>We plan to address the open problems discussed above in the future, and hope that this will ultimately lead to dependable real-world systems clocked by variants of the scheme proposed in this article.</text></s>
</BODY>
<OTHER>
Acknowledgments
We would like to thank the anonymous reviewers for their valuable comments.

</OTHER>
</PAPER>